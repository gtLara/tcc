\chapter{Univariate Spectral Theory}

\section*{Introduction}

The following chapter discusses the spectral representation of univariate time
series, stationary and non-stationary, linear and non-linear. Some important
aspects of multivariate spectral analysis are developed in
chapter~\ref{chap:multi}.

The development of a spectral representation for the theoretical infinite
realization of a stationary stochastic process begins at the recognition of the
impossibility of a Fourier for this type of signal followed by the introduction
of the Wiener Khinchin theorem. After an interpretation of the resulting
spectral representation (the power spectral density) some deterministic linear
system theory leads to the idea of a shaping filter, which finally allows us to
deduce a general expression for the power spectral density of ARMA processes.

We then proceed to linear but non-stationary analysis, in which the short time
Fourier transform and its shortcomings are presented as motivation for the
S-transform. Wavelet analysis is then introduced motivated by the demand for a
mathematically cleaner version of the S-transform, leading to the concepts of
continuous and discrete wavelet transforms. On the topic of resolution the
possibility of an optimized time-frequency resolution is suggested.

Non-linear analysis is constructed by the introduction of the instantaneous
auto-correlation function and subsequent Cohen's class of distributions. Some
main distributions from Cohens class are discussed along with their
limitations, concluding our univariate spectral exploration.

\section{Stationary Analysis}

Wide sense stationary signals are, by definition (section
~\ref{sec:stationarity}), power signals. Since the Fourier transform of a
signal is well defined only if is has finite energy stationary signals do not
have a Fourier representation in the traditional sense, the exception being
quasi-periodic signals which can be represented by a Fourier series. In order
to develop a spectral representation of stationary time series we must define
the concept of a power spectral density and conclude that this function is
proportional to the square magnitude of a hypothetical Fourier transform.

\subsubsection{Power Spectral Density}

We begin by stating Parseval's theorem, in which $F\{\}$ represents the Fourier
transform.

$$ E = \int^{\infty}_{-\infty} |x(t)|^2 dt = \frac{1}{2\pi} \int^{\infty}_{-\infty} |F\{x(t)\}(\omega)|^2 d\omega $$

Extending this definition to signal power gives us

$$ P = \lim_{T \to \infty} \frac{1}{2T}\frac{1}{2\pi} \int^{T}_{-T}|F\{x(t)\}(\omega)|^2 d\omega$$

Note that even though $F\{x(t)\}$ is not well defined here the above relations
still hold if $|F\{x(t)\}|^2$ is defined in a different manner, which will be
done shortly.

The signal power can be rewritten denoting the Fourier transform of $x(t)$ by
$X(\omega)$ as

$$ P = \lim_{T \to \infty} \frac{1}{2T}\frac{1}{2\pi} \int^{T}_{-T}|X(\omega)|^2 d\omega $$

Where $\lim_{T \to \infty}\frac{1}{2\pi} \frac{1}{2T} |X(\omega)|² $
is recognized as a density function. The power spectral density function is
finally defined as

$$ S_{x}(\omega) = \lim_{T \to \infty}\frac{1}{2\pi} \frac{1}{2T} |X(\omega)|² $$

This function's name is pretty explanatory of its interpretation:
$S_{x}(\omega)$ represents the contribution of $x(t)$s frequency components in
$\omega + d\omega$ to the overall signal power. As mentioned, for this function
to make any sense we must define $|X(\omega)|^2$, which will be done
presently

\subsection{Wiener-Khinchin Theorem}

The Wiener-Khinchin theorem can be developed as follows.

$$ |X(\omega)^2| = X(\omega)X^*(\omega) = F(F^{-1}(X(\omega))*(F^{-1}(X^*(\omega))) = F(x(t) * x^*(-t)) = F(x(t) * x(-t))$$

Examining the right-most part of this equality we observe that the function
which is being Fourier-transformed corresponds to the convolution of $x(t)$
with a mirrored version of itself. This is precisely the definition of
autocorrelation. Assuming ergodicity we can now express the squared magnitude
of the Fourier transform of $x(t)$ as the Fourier transform of is autocorrelation
function.

$$|X(\omega)_T|^2 = \frac{1}{2\pi}\int_{-T}^{T} \rho(t)e^{-j \omega t}dt$$

This results is known as the Wiener-Khinchin theorem and it allows for a
well-defined power density spectrum for stochastic signals.

Note that since the autocorrelation of a signal is even its Fourier transform
is real-valued, which is consistent with our notion of a squared magnitude.

\subsection{Spectrum of ARMA processes}

By taking the square magnitude Z transform of the general ARMA recurrence
relationship (~\ref{ssec:arma_l}) we obtain the transfer function

$$ H(z) = \frac{1 + \sum_{i}^{q} b_k z^{-k}}{1 + \sum_{i}^{q} a_k z^{-k}} $$

which is excited by white noise to generate a realization of and ARMA
process. We can now express the power spectral density of an ARMA process as
follows

$$ S_{ARMA}(\omega) = |H(z)|^2 S_{\varepsilon} $$
$$ S_{ARMA}(\omega) = \frac{\sigma^2 |1 + \sum_{k=1}^{q} b_k e^{-j\omega k}|^2}{2\pi|1 + \sum_{k=1}^{p} a_k e^{-j\omega k}|^2} $$

This definition can be used as a means of parametric spectral estimation: the
parameters are estimated in the time domain and used by the relationship
above to estimate the spectrum.

We will now visualize the spectra of some ARMA processes alongside their
autocorrelation functions. Note how the notion that the spectra corresponds to
the Fourier transform of the autocorrelation function is intuitive.

\subsubsection{ARMA(0, 1)}

\subsubsection{ARMA(2, 0)}

\subsubsection{ARMA(2, 3)}

\section{Linear Non-Stationary Analysis}

From the previous section we can conclude that if a time series is stationary
its spectral representation via power spectral density is uniquely determined
by the Fourier transform of its autocorrelation function. The natural extension
for a spectral representation of non-stationary processes is a time-varying
power spectral density since its autocorrelation function is also time-varying.

The idea of a time-varying spectral representation gives rise to the so called
time-frequency analysis methods. We initially explore methods that maintain
linearity.

\subsection{Short Time Fourier Transform}

The short time Fourier transform is the most intuitive approach to a proposal
of time frequency representation.

\subsection{Wavelet Transform}

\subsubsection{Continuous}

\subsubsection{Discrete}

\section{Non-Linear Representations}

There is, indeed, a way to maximize the time-frequency resolution trade-off
inherent to time-frequency representations (TFR)\cite{HERE BRO}. This is done by the
introduction of non linearity through the Fourier transform of the
instantaneous autocorrelation function. As will be seen presence of non
linearity results in cross terms that limit the quality of the representation.
Attempts to dampen these cross terms lead to the more general Cohen's class of
distributions.

\subsection{Instantaneous autocorrelation function}

The instantaneous autocorrelation function is actually just the autocorrelation
function of a non stationary signal written in a specific format.
Interestingly the term autocorrelation function has become strongly understood
as a function of sample lag $\tau$, which is the case for stationary signals,
instead of a function of $t_1$ and $t_2$. We initially rewrite the general
autocorrelation function $R_{xx}$ of a signal $x(t)$

$$ R_{xx}(t_1, t_2) = E[x(t_1)x(t_2)] $$

We can also write $R_xx$ as a function of a single moment $t$ and a lag $\tau$

$$ R_{x}(t, \tau) = E[x(t)x(t-\tau)] $$

Which is slightly more natural for computations. Note that if $x(t)$ is
stationary the dependence on time is removed because $R_{xx}$ has the same
value for all $t$.

A small adjustment in notation leads to

$$ \mathcal{R}_{x}(t, \tau) = E\left[x\left(t - \frac{\tau}{2}\right)x\left(t + \frac{\tau}{2}\right)\right] $$

With a new symbol to indicate that we have finally arrived at the instantaneous
autocorrelation function $\mathcal{R}_{xx}$.

A non stationary TFR is now natural. Since the
Wiener-Khinchin theorem states that the spectrum of a stationary signal is the
Fourier transform of its autocorrelation function we can in an analogous manner
assume that a spectral representation of a non stationary process will be given
by the Fourier transform along the $\tau$ axis of the instantaneous
autocorrelation function. This leads to the Wigner-Ville distribution.

\subsection{Wigner-Ville Distribution}

We define the Wigner-Ville distribution (WVD) as

$$ \mathcal{W}_{x}(t, f) =  \int_{-\infty}^{\infty} \mathcal{R}_{x}(t, \tau) e^{-j\omega \tau}d\tau$$

$$ \mathcal{W}_{x}(t, f) =  \int_{-\infty}^{\infty} x\left(t - \frac{\tau}{2}\right)x\left(t + \frac{\tau}{2}\right) e^{-j\omega \tau}d\tau$$

This natural representation can be thought of as an instantaneous power density
spectrum. It is known that the WVD optimizes the time-frequency resolution
trade-off\cite{tfr_comparison} which is exactly our goal in the development of
more elaborated TFRs. We will now see that this is not without its problems.

\subsubsection{Cross-terms}

By construction the Wigner-Ville distribution is a quadratic representation. By
the quadratic superposition principle~\cite{quadratic_freq_representation} we
know that if $x(t) = \mu x_1(t) + \lambda x_2(t)$ the WVD representation of
$x(t)$ will be given by

$$ \mathcal{W}_{x} = \mu^2\mathcal{W}_{x_1} + \lambda^2\mathcal{W}_{x_2} + 2(\lambda\mu)^2(\mathcal{W}_{x_1 , x_2})$$

Where $\mathcal{W}_{z, y}$ represents the cross-WVD from $z$ to $y$. Since any
real signal of relevant complexity is a linear combination of the $cos(t)$
$sin(t)$ basis we can expect a considerable introduction of unwanted
information from the cross-WVD components, referred to as cross-terms. This is
the infamous cross-terms problem attributed to the WVD and is one of the
reasons that despite its precision in time and frequency resolution it is not
an ideal choice for the TFR for most signals.

The cross-terms are known to exhibit high frequency patterns~\cite{martin_lol},
leading to the idea that the WVD could be filtered in order to be more
representative of its auto-terms. The different ways in which it is possible
and useful to filter the WVD generates what is known as the Cohen's class of
distributions.

\subsection{Smoothed Pseudo Wigner Ville Distributions}

Most members of Cohen's class of distributions are essentially filtered
versions of the WVD~\cite{}. A particularly useful case is known as the
Smoothed Pseudo Wigner Ville Distribution (SPWVD)

$$ SPWVD(t, f) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} h_t (t - \tau) h_f(f - \phi)d\tau d\phi$$

in which $h_t$ denotes the filter applied along time and $h_f$ along frequency.

If the filters are well designed, which is a data-driven process in some
cases~\cite{}, cross-term suppression is sufficiently successful such that
the use of the SPWVD over simpler non-stationary TFR is justified.
