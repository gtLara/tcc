\chapter{Teoria Espectral Univariada}\label{chap:spectral_analysis}

\section*{Introduction}

O seguinte capítulo discute a representação espectral de séries temporais
univariadas. São exploradas transformações lineares e não lineares.

O desenvolvimento de uma representação espectral para uma realização
teoricamente infinita de um processo estocástico se inicia no reconhecimento da
impossibilidade de uma transformada de Fourier desse tipo de sinal, seguido de
uma apresentação do teorema de Wiener Khinchin. O resultado da representação
desenvolvida, conhecido como densidade de potência espectral, é interpretado.
Em seguida, restringindo a classe de sinais estacionários para realizações de
processos ARMA conseguimos derivar expressões fechadas para a densidade
de potência espectral, resultado importante em teoria de
estimação\cite{estimation_theory}.

Em seguida inicia-se uma discussão sobre representações não lineares por meio
de uma generalização natural da função de autocorrelação e subsequentemente a
classe de distribuições de Cohen. O problema geral dessa classe de
representações e a principal proposta para sua resolução nos leva às
transformadas conhecidas como \emph{Smoothed Pseudo Wigner Ville
Distributions} (SPWVD).

\section{Stationary Analysis}

Wide sense stationary signals are, by definition (section
~\ref{sec:stationarity}), power signals. Since the Fourier transform of a
signal is well defined only if is has finite energy stationary signals do not
have a Fourier representation in the traditional sense, the exception being
quasi-periodic signals which can be represented by a Fourier series. In order
to develop a spectral representation of stationary time series we must define
the concept of a power spectral density and conclude that this function is
proportional to the square magnitude of a hypothetical Fourier transform.

\subsubsection{Power Spectral Density}

We begin by stating Parseval's theorem, in which $F\{\}$ represents the Fourier
transform.

$$ E = \int^{\infty}_{-\infty} |x(t)|^2 dt = \frac{1}{2\pi} \int^{\infty}_{-\infty} |F\{x(t)\}(\omega)|^2 d\omega $$

Extending this definition to signal power gives us

$$ P = \lim_{T \to \infty} \frac{1}{2T}\frac{1}{2\pi} \int^{T}_{-T}|F\{x(t)\}(\omega)|^2 d\omega$$

Note that even though $F\{x(t)\}$ is not well defined here the above relations
still hold if $|F\{x(t)\}|^2$ is defined in a different manner, which will be
done shortly.

The signal power can be rewritten denoting the Fourier transform of $x(t)$ by
$X(\omega)$ as

$$ P = \lim_{T \to \infty} \frac{1}{2T}\frac{1}{2\pi} \int^{T}_{-T}|X(\omega)|^2 d\omega $$

Where $\lim_{T \to \infty}\frac{1}{2\pi} \frac{1}{2T} |X(\omega)|² $
is recognized as a density function. The power spectral denUma interpretaçãosity function is
finally defined as

$$ S_{x}(\omega) = \lim_{T \to \infty}\frac{1}{2\pi} \frac{1}{2T} |X(\omega)|² $$

This function's name is pretty explanatory of its interpretation:
$S_{x}(\omega)$ represents the contribution of $x(t)$s frequency components in
$\omega + d\omega$ to the overall signal power. As mentioned, for this function
to make any sense we must define $|X(\omega)|^2$, which will be done
presently

\subsection{Wiener-Khinchin Theorem}

The Wiener-Khinchin theorem can be developed as follows.

$$ |X(\omega)^2| = X(\omega)X^*(\omega) = F(F^{-1}(X(\omega))*(F^{-1}(X^*(\omega))) = F(x(t) * x^*(-t)) = F(x(t) * x(-t))$$

Examining the right-most part of this equality we observe that the function
which is being Fourier-transformed corresponds to the convolution of $x(t)$
with a mirrored version of itself. This is precisely the definition of
autocorrelation. Assuming ergodicity we can now express the squared magnitude
of the Fourier transform of $x(t)$ as the Fourier transform of is autocorrelation
function.

$$|X(\omega)_T|^2 = \frac{1}{2\pi}\int_{-T}^{T} \rho(t)e^{-j \omega t}dt$$

This results is known as the Wiener-Khinchin theorem and it allows for a
well-defined power density spectrum for stochastic signals.

Note that since the autocorrelation of a signal is even its Fourier transform
is real-valued, which is consistent with our notion of a squared magnitude.

\subsection{Spectrum of ARMA processes}

By taking the square magnitude Z transform of the general ARMA recurrence
relationship (~\ref{ssec:arma_l}) we obtain the transfer function

$$ H(z) = \frac{1 + \sum_{i}^{q} b_k z^{-k}}{1 + \sum_{i}^{q} a_k z^{-k}} $$

which is excited by white noise to generate a realization of and ARMA
process. We can now express the power spectral density of an ARMA process as
follows

% $$ S_{ARMA}(\omega) = |H(z)|^2 S_{\varepsilon} $$
% \begin{equation}\label{eq:arma_spectrum}
%      S_{ARMA}(\omega) = \frac{\sigma^2 |1 + \sum_{k=1}^{q} b_k e^{-j\omega k}|^2}{2\pi|1 + \sum_{k=1}^{p} a_k e^{-j\omega k}|^2} $$
% \end{equation}

This definition can be used as a means of parametric spectral estimation: the
parameters are estimated in the time domain and used by the relationship
above to estimate the spectrum.

We will now visualize the spectra of some ARMA processes.

\subsubsection{MA(1)}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/ma_1_spectrum.png}
    \caption{Spectrum of an MA(1) process with
    $\protect \beta_1 = -0.5$}
    \label{fig:ma_1_spectrum}
\end{figure}

\subsubsection{AR(1)}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/ar_1_spectrum_1.png}
    \caption{Spectrum of an AR(1) process with
    $\protect \alpha_1 = 0.8$}
    \label{fig:ar_1_spectrum_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/ar_1_spectrum_2.png}
    \caption{Spectrum of an AR(1) process with
    $\protect \alpha_1 = -0.8$}
    \label{fig:ar_1_spectrum_2}
\end{figure}

\subsubsection{AR(2)}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/ar_2_spectrum.png}
    \caption{Spectrum of an AR(1) process with
    $\protect \alpha_1 = 0.5$ and $\protect \alpha_2 = -0.25$}
    \label{fig:ar_2_spectrum}
\end{figure}

\subsubsection{ARMA(4, 3)}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/arma_4_3_spectrum.png}
    \caption{Spectrum of an ARMA(4, 3) process}
    \label{fig:ar_4_3_spectrum}
\end{figure}


\subsection{Effect of Unit Roots on ARMA Spectra}

As mentioned in subsection~\ref{ssec:ma_roots} the introduction of unit roots
to the moving average or auto regressive polynomials of an ARMA process has
clear effects on its spectral content. The spectral effects of these operations
can be understood by a pole zero plot, sample time domain or closed form
spectral analysis.

By analyzing the pole zero plot of an ARMA system it is evident how integration
affects the system's transfer function: since a pole is introduced at
$e^{j\omega}=0$ energy is supplied to low-frequency signal components.
Conversely we can understand differencing, which is the introduction of a
zero at $e^{j\omega}=0$, as a suppression of low-frequency components,
resulting in a signal with higher overall frequency. This is precisely the
idea behind integrating filters.

In the time domain the spectral effect of differencing can be seen by comparing
figures~\ref{fig:ARMA2-1} and~\ref{fig:ARMA2-1-diff}. The differenced series
clearly has more energy distributed towards high frequency components. This is
in fact true for all signals: the act of time domain differencing biases the
spectral content towards higher frequencies. The reverse is also true: by
integrating a signal we see that it becomes smoother.

TODO: complete this !!!

Finally the most rigorous understanding is achieved by inspection of equation
~\ref{eq:arma_spectrum}. From this equation it is evident that an additional
root on the numerator increases the high frequency content and an additional
root on the denominator increases the low frequency content.

We now present the differenced version of the MA(1) spectrum from figure
~\ref{fig:ma_1_spectrum}. Note that the signal is essentially high pass
filtered, as mentioned.

\subsubsection{Differenced MA(1)}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/diff_ma_1_spectrum.png}
    \caption{Spectrum of an differenced MA(1) process with
    $\protect \beta_1 = -0.5$}
    \label{fig:diff_ma_1_spectrum}
\end{figure}


\section{Linear Non-Stationary Analysis}

From the previous section we can conclude that if a time series is stationary
its spectral representation via power spectral density is uniquely determined
by the Fourier transform of its autocorrelation function. The natural extension
for a spectral representation of non-stationary processes is a time-varying
power spectral density since its autocorrelation function is also time-varying.

The idea of a time-varying spectral representation gives rise to the so called
time-frequency analysis methods. We initially explore methods that maintain
linearity.

\subsection{Short Time Fourier Transform}

The short time Fourier transform is the most intuitive approach to a proposal
of time frequency representation.

\subsection{Wavelet Transform}

\subsubsection{Continuous}

\subsubsection{Discrete}

\section{Non-Linear Representations}

There is, indeed, a way to maximize the time-frequency resolution trade-off
inherent to time-frequency representations (TFR). This is done by the
introduction of non linearity through the Fourier transform of the
instantaneous autocorrelation function. As will be seen presence of non
linearity results in cross terms that limit the quality of the representation.
Attempts to dampen these cross terms lead to the more general Cohen's class of
distributions.

\subsection{Instantaneous autocorrelation function}

The instantaneous autocorrelation function is actually just the autocorrelation
function of a non stationary signal written in a specific format.
Interestingly the term autocorrelation function has become strongly understood
as a function of sample lag $\tau$, which is the case for stationary signals,
instead of a function of $t_1$ and $t_2$. We initially rewrite the general
autocorrelation function $R_{xx}$ of a signal $x(t)$

$$ R_{xx}(t_1, t_2) = E[x(t_1)x(t_2)] $$

We can also write $R_xx$ as a function of a single moment $t$ and a lag $\tau$

$$ R_{x}(t, \tau) = E[x(t)x(t-\tau)] $$

Which is slightly more natural for computations. Note that if $x(t)$ is
stationary the dependence on time is removed because $R_{xx}$ has the same
value for all $t$.

A small adjustment in notation leads to

$$ \mathcal{R}_{x}(t, \tau) = E\left[x\left(t - \frac{\tau}{2}\right)x\left(t + \frac{\tau}{2}\right)\right] $$

With a new symbol to indicate that we have finally arrived at the instantaneous
autocorrelation function $\mathcal{R}_{xx}$.

A non stationary TFR is now natural. Since the
Wiener-Khinchin theorem states that the spectrum of a stationary signal is the
Fourier transform of its autocorrelation function we can in an analogous manner
assume that a spectral representation of a non stationary process will be given
by the Fourier transform along the $\tau$ axis of the instantaneous
autocorrelation function. This leads to the Wigner-Ville distribution.

\subsection{Wigner-Ville Distribution}

We define the Wigner-Ville distribution (WVD) as

$$ \mathcal{W}_{x}(t, f) =  \int_{-\infty}^{\infty} \mathcal{R}_{x}(t, \tau) e^{-j\omega \tau}d\tau$$

$$ \mathcal{W}_{x}(t, f) =  \int_{-\infty}^{\infty} x\left(t - \frac{\tau}{2}\right)x\left(t + \frac{\tau}{2}\right) e^{-j\omega \tau}d\tau$$

This natural representation can be thought of as an instantaneous power density
spectrum. It is known that the WVD optimizes the time-frequency resolution
trade-off\cite{tfr_comparison} which is exactly our goal in the development of
more elaborated TFRs. We will now see that this is not without its problems.

\subsubsection{Cross-terms}

By construction the Wigner-Ville distribution is a quadratic representation. By
the quadratic superposition principle~\cite{quadratic_freq_representation} we
know that if $x(t) = \mu x_1(t) + \lambda x_2(t)$ the WVD representation of
$x(t)$ will be given by

$$ \mathcal{W}_{x} = \mu^2\mathcal{W}_{x_1} + \lambda^2\mathcal{W}_{x_2} + 2(\lambda\mu)^2(\mathcal{W}_{x_1 , x_2})$$

Where $\mathcal{W}_{z, y}$ represents the cross-WVD from $z$ to $y$. Since any
real signal of relevant complexity is a linear combination of the $cos(t)$
$sin(t)$ basis we can expect a considerable introduction of unwanted
information from the cross-WVD components, referred to as cross-terms. This is
the infamous cross-terms problem attributed to the WVD and is one of the
reasons that despite its precision in time and frequency resolution it is not
an ideal choice for the TFR for most signals.

The cross-terms are known to exhibit high frequency patterns~\cite{martin_lol},
leading to the idea that the WVD could be filtered in order to be more
representative of its auto-terms. The different ways in which it is possible
and useful to filter the WVD generates what is known as the Cohen's class of
distributions.

\subsection{Smoothed Pseudo Wigner Ville Distributions}

Most members of Cohen's class of distributions are essentially filtered
versions of the WVD~\cite{}. A particularly useful case is known as the
Smoothed Pseudo Wigner Ville Distribution (SPWVD)

$$ SPWVD(t, f) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} h_t (t - \tau) h_f(f - \phi)d\tau d\phi$$

in which $h_t$ denotes the filter applied along time and $h_f$ along frequency.

If the filters are well designed, which is a data-driven process in some
cases~\cite{}, cross-term suppression is sufficiently successful such that
the use of the SPWVD over simpler non-stationary TFR is justified.
