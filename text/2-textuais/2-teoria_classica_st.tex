\chapter*{Introdução}
\label{sec:teorica_classica_series_temporais_introducao}

\chapter{Definições e Propriedades}
\label{sec:teorica_classica_series_temporais_definicoes}

\section{Processo Estocástico}

Dado um conjunto arbitrário $\mathcal{T}$ um processo estocástico é uma família
${X(t,\omega)}$, $ t\in \mathcal{T}$ e $\omega \in \Omega$ de forma que para
cada $t \in T$, $\omega \in \Omega$ $X(t, \Omega)$ é uma variável aleatória. As
variáveis aleatórias podem ser reais ou complexas. Esse trabalho aborda apenas
processos estocásticos reais exceto quando explicitamente mencionado.

Supõe-se que a família de variáveis aleatórias seja definida em um mesmo
espaço de probabilidades $(\Omega, \mathcal{A}, P)$ com $\Omega$ representando
um espaço amostral, $\mathcal{A}$ uma $\sigma$-álgebra e $P$ uma medida de
probabilidade. Para propósitos desse trabalho podemos tomar o conjunto
$\mathcal{T}$ como $\mathbb{R}$, resultando em processos de tempo contínuo, e
$\mathbb{Z}$, resultando em processos de tempo discreto.

Para cada $t \in \mathcal{T}$ temos uma função de densidade de probabilidade
associada à variável aleatória $X(t_k, \omega)$ (assumindo que essa função exista).
Na prática um processo aleatório no mundo real é observado ao longo de $t$, tal que
$\omega$ seja fixado ao universo em que a observação ocorre. Sob essa condição
$X(t, \omega_k)$ é considerada uma realização do processo estocástico. Realizações também
são chamadas de \emph{sample record} em alguns livros de engenharia
e séries temporais na literatura estatística.

Para ilustrar os conceitos acima podemos pensar em um exemplo proposto por
\cite{random_data} em que um gerador de ruído térmico é construído e sua tensão
ao longo de um intervalo do tempo é medida. Se um outro gerador fosse
construído sob condições e com propriedades idênticas sua tensão medida no
mesmo intervalo de tempo não seria idêntica, assim como a tensão medida sob
qualquer outro gerador idêntico. De fato cada registro de tensão é um exemplo
de infinitos registros que poderiam ter ocorrido. Nessa situação os registros
ou séries temporais de tensão são as realizações de um processo estocástico
representativo de todas as possíveis realizações.

O adequado estudo de séries temporais é consequência de um primeiro adequado
estudo sobre processos estocásticos, geradores dessas séries temporais. Essa
não é a intenção desse trabalho. Como em grande parte da teoria de séries
temporais estamos preocupados com o que podemos compreender ou inferir sobre
o processo estocástico gerador de uma realização a partir apenas de seu único
registro. Essa abordagem é essencial e de fato mais aplicável que um estudo
que se preocupa excessivamente com os processos geradores devido ao fato de dados
do mundo real frequentemente representarem realizações únicas. Não é possível
realizar novamente o índice Ibovespa entre 1970 e 2020 e muito menos ter acesso
à realização desses índices em universos paralelos.

Ao longo desse trabalho a distinção e referência ao processo gerador de uma série
temporal será feita quando necessário.


\section{Série Temporal}\label{ssec:definition}

Uma série temporal é um conjunto de observações realizadas sequencialmente no
tempo, indexadas de acordo com o momento em que foram observadas. As
observações representam a realização de um processo estocástico. Em alguns
contextos, como análise de processos industriai, a natureza do processo
subjacente é relevante para análise e modelagem de qualquer série temporal. Em
outros, como análise de séries financeiras, o sistema gerador das séries é tão
complexo que dificilmente conhecimento sobre sua dinâmica seja útil.

Assume-me, na linguagem de~\cite{hamilton}, que um conjunto de amostras
$\mathbf{y}_t = (y_1, y_2, y_3 \dots y_T)$ pode ser interpretado como um
segmento finito de uma sequência duplamente infinita:

$${\mathbf{y}}_{t=-\infty}^{\infty} = ({\dots, y_{-1},y_0, \overbrace{y_1, y_2, y_3, \dots, y_T}^{\text{Série Observada}}, y_{T+1}, y_{T+2}}\dots)$$

\vspace{1cm}

Apesar de parecer pouco tangível, de fato qualquer série observada é
satisfatoriamente descrita dessa forma. Em um contexto industrial, por exemplo,
o valor de uma variável de processo é zero até a planta ser construída e entrar
em operação, assume valores representativos ou não da dinâmica de interesse
(assumindo valores irrelevantes quando a planta não está em operação plena), e
tendendo ao infinito retorna a zero quando a planta for desativada.

Essa interpretação de uma série temporal é importante ao implicitamente
insinuar que o processo existe em um intervalo temporal mais abrangente do que
o observado. É portanto necessário se questionar sobre quanto os
dados representam o processo analisado e em quais intervalos de tempo.

Séries temporais são inerentemente diferentes de dados tabulares por
representarem amostras de um mesmo processo estocástico ao invés de amostras
aleatórias de uma população. Não faz sequer sentido discutir uma população no
contexto de séries temporais uma vez que estamos restritos à realizações
observadas de um processo estocástico. Essa relevante diferença impede
propriedades estatísticas agradáveis consequentes da independência amostral
associada à dados tabulares apropriadamente amostrados como a lei do grandes
números e o teorema do limite central.

A forma mais natural de analisar uma série temporal é visualizar seus valores
no tempo, como ilustra a imagem~\ref{fig:example}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/white_noise.png}
    \caption{Visualização no tempo de ruído branco}
    \label{fig:example}
\end{figure}


\section{Operador de Atraso(\emph{Lag})}

É importante introduzir o operador de atraso ou \emph{lag}.

Dadas as séries $\mathbf{y}_t = (y_1, y_2, y_3 \dots y_T)$ e
$\mathbf{x}_t = (x_0, x_1, x_2 \dots x_{T-1})$ tal que

$$ \mathbf{y}_t = \mathbf{x}_{t-1}$$

isso é,

$$ y_1 = x_0 $$
$$ y_2 = x_1 $$
$$ \vdots $$
$$ y_T = x_{T-1} $$

Podemos definir $\mathbf{x}_t$ em função de $\mathbf{y}_t$ como:

$$ \mathbf{x}_t = L\mathbf{y}_t $$

tal que

$$\mathbf{y}_{t-1} = L\mathbf{y}_t$$

Observamos que o operador de atraso atrasa uma série temporal em uma unidade de
tempo. Uma breve divagação matemática~\cite{hamilton} permite definir o
operador com propriedades muito semelhantes às de multiplicação dos números
reais, como associatividade, comutatividade e distribuição. Para atrasar
múltiplas unidades de tempo temos que:

\vspace{1cm}

$$L(L(\mathbf{y_t})) = L(\mathbf{y}_{t-1}) = \mathbf{y}_{t-2} = L^2 \mathbf{y}_{t}$$

\vspace{1cm}

de forma que

$$ L^n {\mathbf{y}} =  \mathbf{y}_{t-n}$$

\vspace{1cm}

Um uso importante do operador, decorrente de suas propriedades algébricas, é
exemplificado na seguinte expansão

$$ (aL^2 + bL^3) \mathbf{y}_{t} =  a\mathbf{y}_{t-2} + b\mathbf{y}_{t-3}$$

Conhecimento do operador de atraso é importante para compreender a literatura
de séries temporais e facilita comunicação objetiva de análises cotidianas.
O presente trabalho usa do operador para descrever uma série de modelos.

Como nota final é importante mencionar que alguns livros~\cite{chatfield}
~\cite{stoffer} usam a letra $B$ para denotar o operador de atraso e que na
maior parte dos recursos \emph{online} o operador é referido por seu nome em
inglês, \emph{lag}.

A analogia entre o operador de atraso e a variável complexa $e^{-j\omega} = z^{-1}$
é clara, com a relevante diferença que $z^{-1}$ atrasa um sinal em uma unidade
de tempo se a operação for realizada no domínio $z$ enquanto o operador de
atraso atua diretamente no domínio do tempo. Essa característica permite que
filtros com equações de recorrência complexas sejam representados de forma
compacta no domínio do tempo por meio de polinômios de atraso.

Uma propriedade interessante decorrente da equivalência entre $z^{-1}$ e $L$ é
que pode se pensar em um plano $L$ cuja análise é igualmente informativa à do
plano z, notando que o espaço é de certa forma invertido. Uma análise da
posição dos polos de um sistema representado por meio de um polinômio em $L$
conclui que o sistema é instável se tais polos estiverem fora do círculo unitário,
contrário do que conhecemos do plano z.

Por fim vale mencionar que alguns autores como\cite{aguirre} usam a notação $q^{-1}$
para esse operador.

\section{Operador de Diferença}\label{sec:diff}

O operador de diferenças $\nabla$ ou  $\Delta$ é o equivalente discreto da
operação contínua de diferenciação e opera sob uma série temporal
$\mathbf{y}_t$ da seguinte forma:

$$ \nabla \mathbf{y}_t = (1 - L)\mathbf{y}_t = \mathbf{y}_t - L\mathbf{y}_t = \mathbf{y}_t - \mathbf{y}_{t-1} $$

O operador possui propriedades de associatividade e distribuição, tal que

$$ \nabla^2 \mathbf{y}_t = \nabla(\nabla(\mathbf{y}_t)) = \nabla(\mathbf{y}_t - \mathbf{y}_{t-1}) = \nabla \mathbf{y}_t - \nabla \mathbf{y}_{t-1} = \mathbf{y}_t - 2 \mathbf{y}_{t-1} + \mathbf{y}_{t-2} $$

\section{Tendência(\emph{Trend})}

A variação do valor esperado de um processo estocástico é denominado tendência.
A partir de uma série temporal definimos tendência como a variação de sua média
amostral. A imagem ~\ref{fig:trend} ilustra uma série com tendência linear.
Observa-se que ao longo do tempo a média das observações cresce linearmente.
Tendências de séries reais frequentemente seguem um perfil
logarítmico~\cite{chatfield}, como na figura~\ref{fig:log_trend}. Nesse caso
uma transformação exponencial da série, isso é, a aplicação de uma função
exponencial a cada observação, tornaria a tendência linear.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/trend.png}
    \caption{Série temporal com tendência linear}
    \label{fig:trend}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/log_trend.png}
    \caption{Série temporal com tendência logarítmica}
    \label{fig:log_trend}
\end{figure}

\section{Sazonalidade}\label{sec:seasonality}

A variação periódica de média móvel das observações de uma série temporal é
denominada sazonalidade. Em séries no contexto de finanças sazonalidade
frequentemente segue ciclos de calendário como anual, mensal, semestral, etc.
No contexto mais amplo de séries temporais sazonalidade apresenta período
arbitrário, apesar da linguagem em torno dessa propriedade estar muito
associada aos períodos anteriormente mencionados.

Um exemplo de série temporal com sazonalidade é ilustrado na
figura~\ref{fig:seasonality}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/seasonality.png}
    \caption{Série temporal com sazonalidade senoidal de período arbitrário}
    \label{fig:seasonality}
\end{figure}

Na presença de tendência sazonalidade pode ser considerada aditiva, se sua
variação for constante em torno da tendência, e multiplicativa, se sua variação
depender o valor da tendência. Exemplos de sazonalidade aditiva e
multiplicativa são dados pelas figures~\ref{fig:add_seasonality} e
~\ref{fig:mult_seasonality}, respectivamente. Discernir entre os dois tipos de
sazonalidade é importante para modelagem.

% \begin{figure}[H]
%     \centering
%     \includegraphics[scale=0.6]{figures/add_seasonality.png}
%     \caption{Série temporal com sazonalidade aditiva}
%     \label{fig:add_seasonality}
% \end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/mult_seasonality.png}
    \caption{Série temporal com sazonalidade multiplicativa}
    \label{fig:mult_seasonality}
\end{figure}


\section{Autocorrelação}

TODO: later analogy with convolution would be cool

A função de autocorrelação é definida para processos estocásticos como a
correlação de Pearson entre valores do processo em instantes de tempo
diferentes. A função de autocovariância entre os instantes de tempo $t_1$ e
$t_2$ é dada pela seguinte equação

\begin{equation}\label{eq:raw_autocorr}
    K_{xx}(t_1, t_2) = E[(X_{t_1} - \mu_{t_1})(X_{t_2} -\mu_{t_2})]
\end{equation}

Normalizando a autocovariância obtemos a autocorrelação

$$\rho_{xx}(t_1, t_2) =\frac{K_{xx}(t_1, t_2)}{\sigma_{t_1}\sigma_{t_2}}$$

Para processos estacionários (seção~\ref{sec:stationarity}) a autocovariância,
e consequentemente a autocorrelação, é função apenas do atraso $\tau = |t_1 -
t_2|$. Temos então que

$$\rho_{xx}(\tau) =\frac{K_{xx}(\tau)}{\sigma_{t_1}\sigma_{t_2}}$$

Para uma série temporal, isso é, uma única  realização de um processo
estocástico, a função de autocorrelação estacionária (tipicamente chamada
apenas de função de autocorrelação) pode ser definida diretamente a partir da
definição de correlação amostral sob as seguintes premissas

\begin{enumerate}
    \item O processo estocástico gerador da série temporal é estacionário
    \item O número de observações  $N$ de $\mathbf{y}_t$ é suficientemente
    grande ($N \approx 100$)
\end{enumerate}

resultando na equação ~\ref{eq:autocorr}, onde o subscrito duplo é omitido.
Note que o atraso é discreto, indicado por $k$.

\vspace{1cm}

\begin{equation}\label{eq:autocorr}
    \rho_y(k) = \frac{\sum_{t=1}^{N-k}(y_t - \bar{y})(y_{t+k}-\bar{y})}{\sum_{t=1}^{N}(y_t - \bar{y})^2}  , \hspace{1cm} k = 0, 1, 2, \dots
\end{equation}

Alguns comentários sobre a nomenclatura da função são apropriados. A literatura
de engenharia tende a usar os termos autocovariância e autocorrelação de forma
intercambiável para designar a definição de autocovariância apresentada. A
literatura estatística assume as definições abordadas acima, que serão usada no
restante desse trabalho.

Além disso, a função de autocorrelação como apresentada pela
equação~\ref{eq:raw_autocorr} é definida para um processo estocástico não
necessariamente estacionário, apesar do termo ser usado para descrever a
equação~\ref{eq:autocorr}. A aplicação da equação que assume estacionariedade
em uma série não estacionária resulta em correlações informativas praticamente
apenas disso. Uma modificação estratégica da função dada
por~\ref{eq:raw_autocorr} resulta na chamada função de autocorrelação
instantânea (seção~\ref{ssec:inst_autocorr}), que é usada para representar
séries temporais não estacionárias.

\vspace{1cm}

\subsection{Autocorrelação Parcial}

É interessante mencionar a existência de autocorrelação parcial nessa seção
juntamente de uma descrição em alto nível do que esse valor representa. Sua
definição formal será apresentada na seção~\ref{ssec:AR(p)}.

Autocorrelação parcial foi introduzida por Box e Jenkins em\cite{box} como uma
ferramenta auxiliar na identificação de modelos. O valor $\phi_{kk}$ é
definido como a correlação parcial entre $\mathbf{y}_t$ e $\mathbf{y}_{t - k}$,
isso é, a correlação restante entre $\mathbf{y}_t$ e $\mathbf{y}_{t - k}$ após
levar em consideração a contribuição de $\mathbf{y}_t$, $\mathbf{y}_{t - 1}$
$...$ $\mathbf{y}_{t - k + 1}$.

\subsection{Correlalograma}\label{ssec:correlalogram}

Um correlalograma é um gráfico de barras representativo da autocorrelação ou
autocovariância em $k$ amostras de uma série temporal ($k=0, 1, 2, \dots$), de
forma que a primeira barra represente a autocorrelação entre $y_t$ e si mesmo
(sempre igual à 1), a segunda entre $y_t$ e $y_{t-1}$, a terceira entre $y_t$ e
$y_{t-2}$, e assim por diante. O correlalograma da série visualizada pela
figura~\ref{fig:trend} é ilustrado na figura~\ref{fig:correlalogram}.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/corr_trend.png}
    \caption{Visualização de correlalograma de série com tendência linear.
    Observe que as autocorrelações decaem lentamente ao decorrer dos atrasos,
    comportamento típico de tendências determinísticas.}
    \label{fig:correlalogram}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/corr_seasonality.png}
    \caption{Visualização de correlalograma de série com sazonalidade.
    Observe que o padrão senoidal da série é reproduzido nas autocorrelações.}
    \label{fig:corr_season}
\end{figure}

A figura~\ref{fig:correlalogram} informa um intervalo de relevância dado por um
sombreamento vermelho. Qualquer valor de autocorrelação dentro desse intervalo
é estatisticamente insignificante e pode ser considerado igual a zero.

O correlalograma é uma ferramenta indispensável em análise de séries temporais
para tarefas como detecção de estacionariedade, identificação de sazonalidade,
análise de resíduo, engenharia de características, escolha de modelo e
identificação de ruído branco(seção~\ref{sec:white_noise}), entre outros. No
contexto de identificação de sistemas autocovariância e autocorrelação e
portanto o correlalograma desempenha um importante papel na identificação de
propriedades de sinais e sistemas imersos em ruído devido à robustez ao ruído
da operação de correlação cruzada\cite{aguirre}. A figura~\ref{fig:noisy_sine}
apresenta um sinal imerso em ruído cuja natureza periódica subjacente se torna
mais visível por meio de seu correlalograma.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/noisy_seasonal_signal.png}
    \caption{Sinal periódico imerso em ruído e sua correspondente autocorrelação.}
    \label{fig:noisy_sine}
\end{figure}

É importante mencionar que o correlalograma de uma série com tendência
determinística, como a da figura~\ref{fig:correlalogram}, apresenta o
comportamento observado de autocorrelações altas com pouca atenuação ao longo
dos atrasos. De forma análoga o correlalograma de uma série com sazonalidade
apresenta periodicidade que reproduz seu padrão temporal, como ilustra a
figura~\ref{fig:corr_season}, correlalograma da série da
figura~\ref{fig:seasonality}. O primeiro correlalograma é informativo até
certo ponto: informa simplesmente que a série apresenta tendência. Para
analisar tais séries de forma mais produtiva, a fim de elaborar um possível
modelo, por exemplo, é importante que a série seja estacionária
(seção~\ref{sec:stationarity}). É inclusive afirmado em alguns textos da
literatura estatística\cite{chatfield}, que um correlalograma só faz que
sentido se a série associada for estacionária, observação mais geral e rigorosa
das restrições de uso da equação~\ref{eq:autocorr}.

Na engenharia a análise do correlalograma de sinais não estacionários é
utilizada para investigação da adequação de tempo de amostragem, onde uma
autocovariância com valores lentamente decrescentes e um mínimo local indica que
o sinal pode estar superamostrado, propriedade indesejável que pode resultar em
problemas computacionais além de desperdício de memória.

TODO: add sampling analysis of silica series? later maybe

\section{Estacionariedade}\label{sec:stationarity}

Uma série temporal $\mathbf{x}_t$ gerada por um processo $\mathbf{X}(t)$ é
considerada estacionária se atender às seguintes três condições:

\begin{enumerate}
    \item $E(\mathbf{X}(t)) = \mu$
    \item $Var(\mathbf{X}(t)) = \sigma^2$
    \item $Cov[\mathbf{X}(t), \mathbf{X}(t+\tau)] = \gamma(\tau)$
\end{enumerate}\vspace{.5cm}

Que podem ser interpretadas da seguinte forma

\begin{enumerate}
    \item A média da série $\mathbf{x}_t$ é constante ao longo do tempo
    \item A variância da série $\mathbf{x}_t$ é constante ao longo do tempo
    \item A autocorrelação de $\mathbf{x}_t$ depende apenas do atraso
\end{enumerate}\vspace{.5cm}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{figures/stationarity_examples.png}
    \caption{Conjunto de séries demonstrando diferentes níveis de
    estacionariedade.}
    \label{fig:stationarity}
\end{figure}

Um bom exemplo de graus de estacionariedade em séries temporais dado por
Athanasopoulos e Hyndman~\cite{athana} é ilustrado pela
figura~\ref{fig:stationarity}. As séries das figures~\ref{fig:stationarity}.a,
~\ref{fig:stationarity}.e e~\ref{fig:stationarity}.i demonstram clara
tendência, sendo portanto não estacionárias. As séries das figures
~\ref{fig:stationarity}.d, ~\ref{fig:stationarity}.h, ~\ref{fig:stationarity}.i
são igualmente não estacionárias por apresentarem clara sazonalidade, enquanto
a série da figura~\ref{fig:stationarity}.g aparenta ser sazonal mas apresenta
picos aperiódicos de intensidade muito distinta, sendo considerada estacionária
pelos autores. O caso da série da figura~\ref{fig:stationarity}.g é um
complicado por aparentar também violar o requisito de variância constante. Para
concluir assim como os autores que a série é estacionária é necessário mais do
que uma investigação visual; o texto que discute a série leva em conta seu
processo gerador~\cite{athana}.

As séries das figures~\ref{fig:stationarity}.c e ~\ref{fig:stationarity}.f são
aparentemente não estacionárias no intervalo observado por demonstrarem
variação em sua média móvel mas podem ser um caso de raízes unitárias.

Por eliminação temos que apenas as séries ilustradas pelas
figures~\ref{fig:stationarity}.b e ~\ref{fig:stationarity}.g são estacionárias,
o caso de ~\ref{fig:stationarity}.b contendo uma clara anomalia.

O exemplo de análise de estacionariedade da figura~\ref{fig:stationarity}
demonstra a imprecisão da abordagem visual para essa tarefa. Torna-se
necessário o estabelecimento de procedimentos mais objetivos para detecção de
estacionariedade (seção ~\ref{ssec:stationarity_tests}).

\subsection{Importância de Estacionariedade}

Estacionariedade é uma propriedade desejável de se observar em uma série
temporal para fins analíticos e de modelagem.

Há algumas formas de compreender como estacionariedade colabora para a
modelagem bem sucedida de uma série temporal.

Podemos pensar na propriedade de estacionariedade como um tipo de estrutura de
dependência. Se as amostras $X_1, X_2, \dots, X_N$ de um conjunto $\mathbf{X}$
forem independentes entre si temos formas interessantes de modelar a função
geradora de $\mathbf{X}$ como o teorema do limite central, lei dos grandes
números, etc. Há apenas uma forma de um conjunto amostral ser independente mas
muitas formas de ser dependente, tornando difícil o estabelecimento de recursos
eficientes para modelagem geral de processes dependentes. Séries temporais
sendo naturalmente observações de processos dependentes
(seção~\ref{ssec:definition}) é interessante definir estruturas de dependência
que permitam o uso de tais recursos. Estacionariedade é uma estrutura de
dependência que permite aplicar propriedades úteis de independência em séries
temporais. Abordando a mesma ideia mais intuitivamente podemos pensar no
seguinte exemplo: se um processo possui valor esperado e variância constante e
autocorrelação invariante ao tempo podemos por meio da lei dos grandes números
estimar seu valor esperado e variância com cada vez mais confiança a partir da
média e variância amostral, respectivamente. O mesmo argumento intuitivo se
estende
analogamente para o caso de aprendizado de máquina, no qual por meio de teoria
de aprendizado estatístico é possível argumentar que uma série estacionária é
``mais fácil'' de aprender.

De forma mais quantitativa o teorema de decomposição de Wold~\cite{chatfield}
conclui que qualquer série temporal estacionária pode ser representada pela
seguinte combinação linear

$$\mathbf{y}_t = \sum_{j=0}^\infty b_j Z_{t-j} + \eta_t$$

No qual $\eta$ representa uma série determinística e $Z_t$ um processo
puramente aleatório (seção~\ref{sec:white_noise}). O leitor reconhecerá parte da expressão
acima como um processo $MA(\infty)$ (seção~\ref{ssec:MA(p)}). Esse resultado tem
como consequência a importante conclusão que qualquer série estacionária é
possivelmente aproximável por um modelo MA e portanto, via invertibilidade,
modelos AR e ARMA (seção~\ref{ssec:stability_invertibility}).

Por fim estacionariedade permite o uso de uma série de modelos que serão
discutidos na seção sobre modelos estacionários. Esses métodos são bem
compreendidos e implementados, facilitando sua interpretação, uso e
sustentação.

\subsection{Categorias Básicas de Não Estacionariedade}\label{ssec:taxonomy}

Como extensão do argumento sobre estruturas de dependência na seção anterior
podemos afirmar que, sendo estacionariedade um padrão de dependência, temos
infinitas formas de não estacionariedade, retornando ao caso de dependência
generalizada. É interessante identificar nesse universo de dependência padrões
de séries não estacionárias que são facilmente transformadas em séries
estacionárias.

Uma série temporal com presença de tendência determinística, como ilustrada na
figura~\ref{fig:trend}, pode ser representada pela seguinte expressão:

$$  y_t = e_t + f(t) + \varepsilon_t  \hspace{1cm}\text{onde} \hspace{.4cm}\varepsilon_t \sim \hspace{.2cm}\text{i.i.d.} \hspace{.2cm}\mathcal{N}(0, \sigma^2)$$

Na qual $e_t$ representa uma série estacionária, $f(t)$ uma função
determinística do tempo e $\varepsilon_t$ ruído
branco(seção~\ref{sec:white_noise}). Nota-se que $f(t)$ é uma função
monotônica arbitrária tal que $y_t$ seja uma série não estacionária. No caso da
figura~\ref{fig:trend} temos $f(t)$ linear e na figura ~\ref{fig:log_trend}
logarítmica. Uma série temporal demonstrando esse tipo de não estacionariedade
é considerada \textbf{tendência-estacionária}, uma vez que simplesmente
removendo a tendência $f(t)$ temos estacionariedade. Isso pode ser feito de
várias formas, talvez com maior simplicidade diferenciando a série. Métodos
mais sofisticados incluem decomposição ETS (seção~\ref{sec:decomposition}) e
regressão com finalidade de modelar $f(t)$ de forma que o resíduo represente
uma a série estacionária $e_t + \varepsilon_t$.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{figures/random_walk.png}
    \caption{Visualização de caminhada aleatória com $y_0=5$}
    \label{fig:random_walk}
\end{figure}

Uma série com presença de tendência estocástica pode ser classificada de
maneira semelhante. O exemplo mais simples de tal série é gerada por um
passeio aleatório, definido pelo seguinte processo, visualizado pela figura
~\ref{fig:random_walk}:

$$  y_t = y_{t-1} + \varepsilon_t  \hspace{1cm}\text{onde} \hspace{.4cm}\varepsilon_t \sim \hspace{.2cm}\text{i.i.d.} \hspace{.2cm}\mathcal{N}(0, \sigma^2)$$

Por meio de um desenvolvimento recursivo do processo podemos escrever:

$$ y_t = (y_{t-2} + \varepsilon_{t-1}) + \varepsilon_{t} $$
$$ y_t = ((y_{t-3} + \varepsilon_{t-2}) + \varepsilon_{t-1}) + \varepsilon_{t} $$
$$ \vdots $$
$$ y_t = \sum_{j=0}^{N-1} \varepsilon_{t-j} + y_0$$
\vspace{1cm}

Resultado a partir do qual a não estacionariedade de $y_t$ se torna evidente,
uma vez que

$$ var(y_t) = \sigma^2 t $$

Além da covariância ser dependente do tempo.

Uma forma simples de tornar a série estacionária é diferenciá-la em primeira
ordem, isso é, aplicar o operador de diferença primeira:

$$ \nabla y_t = y_t - y_{t-1} $$
$$ y_t - y_{t-1} = \varepsilon_t$$
$$ \nabla y_t =  \varepsilon_t$$

Sabemos pela seção~\ref{sec:white_noise} que ruído branco é um processo
estacionário.

A caminhada aleatória é denominada uma série \textbf{diferença-estacionária}
pelo fato da operação de diferença introduzir estacionariedade. Essa é uma
forma tão comum de não estacionariedade que a ideia de ``diferenciar uma
série antes de fazer qualquer coisa'' é proeminente entre profissionais de
dados, apesar de que geralmente necessita-se apenas de estacionariedade
Essa prática é parcialmente justificada considerando que a maior
parte das séries temporais ``reais'' são não estacionárias e frequentemente
diferencialmente estacionárias.

É igualmente possível que uma série diferença-estacionária seja estacionária em
sua $n$-ésima diferença, tal que estacionariedade seja observada por uma
operação de diferenças de ordem $n$. A ideia de tirar sucessivas diferenças
até atingir estacionariedade é fundamental no método de Box-Jenkins, por
exemplo.

Séries diferença-estacionárias apresentam raízes unitárias e os dois termos são
frequentemente usados nos mesmos contextos.

Podemos resumir as definições das categorias de não estacionariedade abordadas
nessa seção assim como suas implicações como segue:

\begin{enumerate}
    \item \textbf{Estacionariedade em Tendência}: Uma série é considerada
        tendência-estacionária se apresentar uma tendência determinística. No
        caso de anomalias ou eventos de perturbação séries com esse tipo de
        tendência retornam ao valor da tendência ao longo do tempo,
        ``esquecendo'' o evento perturbador. Esse tipo de série se torna
        estacionária pela remoção da tendência determinística, processo
        realizado por meio de regressão da tendência, por diferenciação, por
        decomposição, etc.
    \item \textbf{Estacionariedade Diferenciável}: Uma série é considerada
        diferença-estacionária se apresentar uma tendência estocástica. No
        caso de anomalias ou eventos de perturbação séries com esse tipo de
        tendência são irreversivelmente afetadas,
        ``lembrando'' do evento perturbador. Esse tipo de série se torna
        estacionária por diferenciação em ordem $n$. Possui raízes unitárias
        e é frequentemente discutida nessa linguagem.

\end{enumerate}

\section{Ruído Branco}\label{sec:white_noise}

Uma série temporal $\mathbf{x}_t$ gerada por um processo $\mathbf{X}(t)$ é
considerada ruído branco ou um processo puramente aleatório se atender às
seguintes três condições:

\begin{enumerate}
    \item $E(\mathbf{X}(t)) = 0$
    \item $Var(\mathbf{X}(t)) = \sigma^2$
    \item $Cov[\mathbf{X}(t), \mathbf{X}(t+\tau)] = 0$
\end{enumerate}\vspace{.5cm}

Que podem ser interpretadas da seguinte forma

\begin{enumerate}
    \item A média da série $\mathbf{x}_t$ é nula ao longo do tempo
    \item A variância da série $\mathbf{x}_t$ é constante ao longo do tempo
    \item Não há correlação entre as amostras de $\mathbf{x}_t$
\end{enumerate}\vspace{.5cm}

Observa-se que ruído branco é um caso específico de estacionariedade, se
diferenciando pela especificação do valor esperado e autocorrelação entre
quaisquer amostras em zero. A compreensão da definição e capacidade de
identificação de ruído branco é importante para análise de resíduos, detalhado
na seção~\ref{sec:residual_analysis}.

\chapter{Teoria Univariada}
\label{chap:univariate_theory}

\chapter{Análise Clássica de Séries Temporais Univariadas}

\section*{Introdução}

O seguinte capítulo tem por objetivo introduzir uma parte pequena mas
representativa do corpo clássico de análise de séries temporais univariadas
usando uma base de vocabulário e terminologia desenvolvida no capítulo 1.

\section{Decomposição de Séries Temporais}
\label{sec:decomposition}

Decomposição em séries temporais tipicamente descreve o processo de
representação de uma série por uma combinação linear de três componentes:
tendência, sazonalidade e resíduos. A decomposição de uma série $y_t$ pode ser
aditiva ou multiplicativa como expressada pelas equações~\ref{eq:add_decomp} e
~\ref{eq:mult_decomp} respectivamente.

\begin{equation}\label{eq:add_decomp}
    y_t = S_t + T_t + R_t
\end{equation}

\begin{equation}\label{eq:mult_decomp}
    y_t = S_t \cdot T_t \cdot R_t
\end{equation}

A escolha de decomposição aditiva ou multiplicativa deve ser feita de acordo
com o tipo de sazonalidade (seção~\ref{sec:seasonality}).

TODO: ver se isso procede ou não!

Como herança de econometria grande parte dos algoritmos clássicos de
decomposição (X11~\cite{x11}, SEATS~\cite[capítulo~5.2]{SEATS} e derivados) são
baseados em período de sazonalidade anuais, semestrais, trimestrais e
mensais~\cite{athana}. A incapacidade desses algoritmos de processar dados com
período menor os torna pouco utilizáveis no contexto de sinais elétricos, por
exemplo, cujas séries tipicamente são de período inferior a um dia,
apresentando padrões sazonais com período semelhante.

Os algoritmos abordados nas seguintes subseções são teoricamente capazes de
modelar padrões sazonais de período arbitrário mas na prática suas
implementações frequentemente operam com períodos limitantes de um dia ou
mais. Cada um dos algoritmos apresentados é portanto implementado em
\verb+python+.

Antes de prosseguir é interessante comentar sobre os principais objetivos de
decomposição:

\begin{enumerate}

    \item Análise: O processo de pensar sobre, elaborar, ajustar e observar
    resultados de decomposição são altamente informativos da natureza da
    série sob análise. A operação é frequentemente usada para fins de análise
    exploratória.

    \item Indução de estacionariedade: Como tendência e sazonalidade são
    padrões cuja presença qualifica não estacionariedade~\ref{sec:stationarity}
    sua remoção pode tornar uma série ``mais estacionária" ou pelo menos mais
    apropriada para modelagem por meio de modelos não estacionários.

    \item Detecção de Anomalias: Eventos anômalos são tipicamente dissociados
    de e ocultados por estruturas de tendência e sazonalidade. A remoção
    dessas estruturas tende a expor anomalias de forma mais detectável.

    \item Modelagem e Previsão: A representação de uma série por meio de três
    componentes distintas é interessante para problemas de previsão pela
    possibilidade de desenvolver modelos e previsões para cada componente de
    acordo com suas propriedades. Os modelos independentes tendem a ser menos
    complexos, mais generalizáveis e mais robustos do que um modelo adequado
    para a série original, se existir.

\end{enumerate}

A imagem~\ref{fig:add_decomp} ilustra o resultado do processo de
decomposição aditiva de uma serie temporal sintética da forma que é tipicamente
apresentado: quatro linhas contendo a série original, componente de tendência ,
componente sazonal e componente residual em aparição decrescente. A operação
descrita pela equação~\ref{eq:add_decomp} equivale à constatação que a
imagem da primeira linha corresponde à soma das imagens das três linhas
inferiores.

TODO put add figure here

% \begin{figure}[H]
%     \centering
%     \includegraphics[scale=0.6]{figures/add_decomposition.png}
%     \caption{Exemplo de decomposição de série sintética}
%     \label{fig:add_decomp}
% \end{figure}

\subsection{Modelagem de Tendência}

Uma operação essencial em decomposição de séries temporais é a determinação
de uma tendência subjacente dos dados. Os principais procedimentos para tal,
abordados a seguir, são simples e bem estabelecidos.

\subsubsection{Média Móvel}

Um filtro de média móvel de ordem $m$ é tipicamente expressado como:

\begin{equation}\label{eq:ma}
    MA_{t_{m}} = \frac{1}{m} \sum_{i=-k}^{i=k} y_{t+i}
\end{equation}

A operação é visualizada pela figura~\ref{fig:MA}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/moving_average.png}
    \caption{Visualização de aplicação de filtro média móvel com m=15}
    \label{fig:MA}
\end{figure}

TODO: talk (and research about !) the use of kalman filter for moving average

\subsubsection{Regressão Localizada/Filtro de Savitzky-Golay}

Uma forma mais robusta e granular de modelagem de tendência é o algoritmo
LOESS, baseado em regressão localizada e conhecido em linguagem de processamento
de sinais como o filtro de Savitzky-Golay. Apesar do algoritmo ser simples
seu desenvolvimento será omitido por ser matematicamente verboso. É importante
que o leitor conheça a existência do método LOESS, sabendo que ele oferece uma
alternativa mais precisa para modelagem de tendência do que média móvel,
apesar de ser potencialmente mais computacionalmente complexo. Uma breve
intuição sobre seu funcionamento é apresentada a seguir.

A ideia fundamental do algoritmo é repartir a série temporal em grupos de
pontos menores, calculando uma regressão (tipicamente mas não necessariamente
linear) nesses grupos menores de forma a construir uma curva linear por partes
que aproxima a tendência da curva original. O tamanho dos grupos é arbitrário
e tipicamente informado como uma fração do tamanho da série completa e os grupos
em si são determinados pelos $n$ pontos mais próximos a um elemento chamado de
``ponto focal'' que é incrementado a cada iteração. O valor dos incrementos dos
pontos focais também é um parâmetro.

A figura~\ref{fig:loess} ilustra o resultado do algoritmo LOESS para a mesma
série analisada pela figura~\ref{fig:MA}. A figura ilustra curvas de tendência
modeladas via LOESS para diferentes tamanhos de grupos. Os tamanhos são
informados como porcentagem do comprimento total do sinal. Observa-se que o uso
de uma porcentagem menor do sinal aumenta a localidade da regressão tornando o
ajuste da tendência mais sinuosa.

TODO: fiz loess figure here !
% \begin{figure}[H]
%     \centering
%     \includegraphics[scale=0.6]{figures/loess.png}
%     \caption{Visualização de algoritmo LOESS}
%     \label{fig:loess}
% \end{figure}

\subsection{Modelagem de Sazonalidade(Ajuste de Sazonalidade)}

TODO: remover isso aqui? se pah

A operação fundamental de decomposição é a modelagem ou ajuste de sazonalidade.
Esse processo se preocupa em identificar estruturas fortemente periódicas em
uma série temporal. Consideravelmente mais complicada que identificação de
tendência identificação de padrões de sazonalidade tende a ser realizada de
três formas:

\begin{itemize}
    \item Métodos de regressão que modelagem o padrão sazonal como uma função
    do tempo
    \item Modelos ARIMA(seção ~\ref{ssec:ARIMA})
    \item Modelos baseados em média móvel ou filtros lineares
\end{itemize}

As abordagens apresentadas nesse documento abrangem apenas o primeiro conjunto
de métodos.

TODO: pesquisar mais ! transpor modelagem de sazonalidade de algoritmo clássico
para aqui, se estiver correto. enumerar métodos de modelagem

\subsection{Decomposição Clássica (Aditiva)}

TODO: ver se isso entra mesmo

O algoritmo de decomposição clássica é simples e capaz de identificar períodos
arbitrários de sazonalidade. O procedimento é apresentado passo a passo
acompanhando a decomposição de uma série temporal de corrente elétrica. O
código acompanhando cada passo é registrado no \verb+jupyter notebook+
associado intitulado \verb+decomposicao_classica.ipnb+.

TODO: decidir se incluir decomposição em jupyter, nessa seção mesmo ou em uma
seção encerrando decomposição

\subsubsection{Passo 1}

\textbf{Identificar o período de sazonalidade que se deseja modelar.}

O período $m$ de sazonalidade corresponde à duração de um ciclo de um padrão
periódico observável nos dados. A determinação da duração desse ciclo não é
necessariamente trivial, especialmente tratando de séries de alta frequência,
sendo interessante inclusive usar métodos automáticos para determinação da
distância entre amostras iniciais dos períodos de sazonalidade.

\subsubsection{Passo 2}

\textbf{Modelar tendência }$\mathbf{T_t}$\textbf{ por meio de média móvel}

A tendência $T_t$ é modelada calculando a média móvel de ordem $m$ da
série.

\subsubsection{Passo 3}

\textbf{Remover tendência da série}

A componente sem tendência é dada por $y_t - T_t$. A série resultante deve
possuir média aproximadamente nula.

\subsubsection{Passo 4}

\textbf{Estimar componente sazonal}

A estimativa da componente sazonal é realizada calculando o ``padrão sazonal
médio'' da série sem tendência. A série de tamanho $n$ sem tendência é divida
em $s$ segmentos de $m$ amostras, no qual $s = \frac{n}{m}$. Usando livremente
a vírgula como símbolo para concatenação podemos enumerar os segmentos como
$$\chi_1, \chi_2, ... \chi_s$$ onde um segmento arbitrário $$\chi_i =
\chi_{i_{1}}, \chi_{i_{1}} ... \chi_{i_{m}}$$

O padrão sazonal modelado $S'_t$ corresponde ao segmento médio, isso é,

$$ S'_t = \left(\frac{1}{s} \sum_{k=1}^{k=s} \chi_{1_{k}}\right), \left(\frac{1}{s} \sum_{k=1}^{k=s} \chi_{2_{k}}\right), \left(\frac{1}{s} \sum_{k=1}^{k=s} \chi_{3_{k}}\right) \hdots \left(\frac{1}{s} \sum_{k=1}^{k=s} \chi_{m_{k}}\right)$$

A componente sazonal $S_t$ é então dada pela concatenação de $m$ cópias de
$S'_t$

$$ S_t = \overbrace{S'_t, S'_t, \hdots, S'_t}^{s\text{ vezes}} $$

e posterior remoção de sua tendência.

\subsubsection{Passo 5}

\textbf{Calcular a componente residual}

Por fim calcula-se a componente residual $R_t$ como

$$ R_t = y_t - T_t - S_t $$

A série está enfim descomposta

$$ y_t = R_t + T_t + S_t $$

\subsection{Decomposição STL}

TODO: estudar e incluir algoritmo

\subsection{Quantificação de tendência e sazonalidade}

\subsubsection{Força}

A decomposição de uma série temporal em componentes isolados de tendência,
sazonalidade e resíduos permite que a intensidade de tendência e sazonalidade
sejam quantificáveis de forma elegante. Essa intensidade de tendência ou
sazonalidade é tipicamente chamada de força.

Para séries com forte tendência é esperado que a componente de tendência
contenha considerável variância. Uma forma interessante de quantificar a força
da tendência de uma série, proposta por Hyndman et. al.~\cite{hyndman}, parte
da observação da variância adicional introduzida pela adição do termo de
tendência ao residual:

$$ \frac{Var(R_t)}{Var(T_t + R_t)} $$

Espera-se que essa razão seja pequena para séries com alta tendência, isso é,
haja introdução de considerável variância pela adição da componente de
tendência no denominador. Podemos então definir a força $F_T$ da tendência de
uma série como

$$ F_T = max\left(0, 1 - \frac{Var(R_t)}{Var(T_t + R_t)}\right) $$

TODO: Formalize $F_T$ é um número real limitado entre 0 e 1.
TODO: Observe que $F_T \in real$ limitado entre 0 e 1.

De forma identicamente análoga podemos definir a força da sazonalidade de uma
série como

$$ F_S = max\left(0, 1 - \frac{Var(R_t)}{Var(S_t + R_t)}\right) $$

Essas medidas são úteis ao oferecer uma interface quantitativa aos atributos
tipicamente qualitativos de tendência e sazonalidade. É importante deixar
claro que essas medidas necessitam que a série seja decomposta em suas
componentes de tendência e sazonalidade, possivelmente dificultando sua
aplicação.

TODO: verificar última frase acima

\subsection{Definição de Problema}

TODO: definir problema da gerdau aqui

% \begin{figure}[H]
%     \centering
%     \includegraphics[scale=0.6]{figures/current.png}
%     \caption{Visualização de corrente elétrica do motor 17 das 14 horas às
%     14:30 no domínio do tempo}
%     \label{fig:raw_current}
% \end{figure}

Observa-se na figura~\ref{fig:raw_current} que há aparente padrão de
sazonalidade.

\subsection{Exemplo de Decomposição}

TODO: ver como enquadrar problema da gerdau. talvez passar para parte de
aplicações. rever tudo isso de decomposição, tá ficando meio volumoso demais





\section{Modelos Estacionários}

\subsection{Modelo Linear Generalizado}
\label{sec:glm}

\subsection{Modelo Média Móvel}
\label{ssec:MA(p)}

Um processo $\mathbf{Y}(t)$ é considerado de média móvel de ordem $q$ se ele
for definido pela equação~\ref{eq:ma(q)}, na qual $\varepsilon_t$ representa um
processo puramente aleatório. O processo é tipicamente chamado de $MA(q)$.

\begin{equation}\label{eq:ma(q)}
    y_t = \sum_{i=0}^{i=q} \beta_i \varepsilon_{t-i}
\end{equation}

Um processo de média móvel, como sugerido pelo nome, é uma média ponderada de
de observações anteriores de série temporal de ruído branco.

A seguinte expressão exemplifica um processo $MA(2)$.

$$ y_t = \beta_0 \varepsilon_t + \beta_1 \varepsilon_{t-1} + \beta_2 \varepsilon_{t-2} $$

Como no caso do processo autoregressivo podemos escrever o processo acima por
meio do operador de atraso, resultando na seguinte forma.

$$ y_t = Z_t (\beta_0 + L\beta_1 + L^2\beta_2) $$

Podemos escrever a equação~\ref{eq:ma(q)} usando o operador de atraso por meio
da equação~\ref{eq:ma(q)_L}.

\begin{equation}\label{eq:ma(q)_L}
    y_t = Z_t \sum_{i=0}^{i=q} \beta_{t-i} L^{i} = Z_t \phi(L)
\end{equation}

A figura~\ref{fig:ma_time} ilustra o comportamento temporal de modelos $MA$ para
diferentes ordens.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/ma_time.png}
    \caption{Visualização no tempo de processos média móvel de ordens
    diferentes.}
    \label{fig:ma_time}
\end{figure}

Como no caso dos modelos autoregressivos não temos clara identidade visual dos
processos média móvel.

\subsubsection{Condições para Estacionariedade}

Não há restrições de estacionariedade para um processo de média móvel de ordem
finito. Na prática qualquer processo $MA(p)$, $p < \infty$, é estacionário.

\subsubsection{Autocorrelação}

A função de autocorrelação de um processo média móvel exibe a interessante
propriedade de ``cortar'' após o atraso $q$, isso é, demonstrar autocorrelação
igual a zero após um atraso de número correspondente à ordem do processo. A
autocorrelação amostral de uma série temporal gerada por um processo de média
móvel tende a apresentar a mesma propriedade, apesar de ser perfeitamente
possível e provável da autocorrelação de um processo $MA(q)$ cair para zero
\emph{antes} do lag $q$.

A imagem~\ref{fig:ma_autocorr} demonstra a visualização dos correlalogramas
correspondentes aos modelos ilustrados no tempo pela figura~\ref{fig:ma_time}.
Observe que as autocorrelações são distintas de zero apenas para atrasos
iguais ou inferiores à ordem $q$ do processo.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/ma_corr.png}
    \caption{Visualização do correlalograma de processos média móvel de ordens
    diferentes.}
    \label{fig:ma_corr}
\end{figure}

Essa propriedade da função de autocorrelação possui consequências diretas
para modelagem de séries temporais: é possível que uma série temporal
estacionária com autocorrelações iguais a zero após atraso $k$ seja
parcialmente aproximada por um modelo $MA(k)$. Não há garantia nenhuma que
esse seja o caso, no entanto. Retomando uma discussão presente no capítulo 1
relembramos que a autocorrelação amostral de uma série temporal pode ser
gerada por infinitas funções, tal que a observação de uma autocorrelação
amostral característica de processos $MA(q)$ não garante que o processo
gerador da série temporal seja de fato $MA(q)$ - isso é apenas insinuado.
Nesses casos modelos $MA(q)$ são excelentes primeiras tentativas.


\subsection{Modelo Autoregressivo}
\label{ssec:AR(p)}

Um processo $\mathbf{Y}(t)$ é considerado autoregressivo de ordem $p$ se ele
for definido pela equação~\ref{eq:ar(p)}, onde $\varepsilon_t$ representa um
processo puramente aleatório. O processo é frequentemente chamado de $AR(p)$.

\begin{equation}\label{eq:ar(p)}
   y_t = \sum_{i=1}^{i=p} y_{t-i}\alpha_i + \varepsilon_t
\end{equation}

Como sugerido por seu nome um processo autoregressivo de ordem $p$ é
caraterizado por uma dependência entre uma amostra em um instante de tempo $t$
e as amostras de instantes de tempo $t-1$, $t-2$, ..., $t-p$. Observa-se que
uma amostra em tempo $t$ é exatamente uma média ponderada das $p$ amostras
anteriores.

Como exemplo temos que um processo autoregressivo de segunda ordem, isso é,
$AR(2)$, é definido pela expressão a seguir.

$$ y_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \varepsilon_t$$

Esse processo pode ser escrito por meio de operadores de atraso como apontado
pelo seguinte desenvolvimento.

$$ y_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \varepsilon_t$$
$$ y_t - \alpha_1 y_{t-1} - \alpha_2 y_{t-2} = \varepsilon_t$$
$$ y_t - L\alpha_1 y_{t} - L^2\alpha_2 y_{t-2} = \varepsilon_t$$
$$ (1 - L\alpha_1 - L^2\alpha_2)y_t= \varepsilon_t$$
$$ y_t= \frac{\varepsilon_t}{(1 - L\alpha_1 - L^2\alpha_2)}$$

O polinômio de operadores de atraso $1 - L\alpha_1 - L^2\alpha_2$ recebe o
nome de equação característica. Generalizando para ordem $p$ a expressão de um
processo autoregressivo em função do operador de atraso chegamos à equação
~\ref{eq:ar(p)_L}.

\begin{equation}\label{eq:ar(p)_L}
    y_t = \frac{Z_t}{1 - \sum_{i=1}^{i=p} L^i \alpha_i} \frac{1}{\theta(L)}
\end{equation}

A formulação desse processo em função de sua equação característica aproxima-se
de teoria de controle clássica inclusive no que pode ser compreendido da série
temporal a partir da equação característica. Uma análise das raízes desse
polinômio informa estabilidade, instabilidade ou estabilidade marginal, com o
caso de estabilidade marginal correspondo ao de raízes unitárias, discutido na
seção~\ref{ssec:unit_roots}. A relação entre a equação característica, suas
raízes e a dinâmica de uma série temporal é um tópico muito explorado por
Box e Jenkins em~\cite{box}, abordado na seção~\ref{ssec:multivariate}.

Uma visualização do comportamento temporal de processos autoregressivos é dada
pela figura~\ref{fig:ar_time_visualization}. Observa-se que as séries temporais
não exibem comportamento visivelmente distinguível no domínio do tempo.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/ar_time.png}
    \caption{Visualização de processos autoregressivos de diferentes ordens
    no domínio do tempo.}
    \label{fig:ar_time_visualization}
\end{figure}

\subsection{Modelo ARMA}
\label{ssec:ARMA}

\subsection{ARMA}\label{ssec:ARMA}

Processos ARMA, como sugerido pelo nome, são gerados pela sobreposição de
processos AR e MA. Um processo $\mathbf{Y}(t)$ é considerado $ARMA(p, q)$ se
for dado pela equação~\ref{eq:arma}

\begin{equation}\label{eq:arma}
    y_t = \sum_{i=0}^{i=q} \beta_i \varepsilon_{t-i} + \sum_{i=1}^{i=p} y_{t-i}\alpha_i
\end{equation}

Um processo $ARMA(2, 1)$ é portanto da seguinte forma.

$$ y_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \beta_0 Z_{t} + \beta_1 Z_{t-1}  $$

Em analogia aos casos anteriores o processo pode ser escrito por meio do
operador de atraso.

$$ y_t (1 - \alpha_1 L - \alpha_2 L^2) = Z_{t} (\beta_0 + \beta_1 L) $$

Podemos introduzir $\phi(L) = 1 - \alpha_1 L - \alpha_2 L$ e $\theta(L) =
\beta_0 + \beta_1 L$ tal que:

\begin{equation}\label{eq:arma_l}
    y_t = Z_t \frac{\phi(L)}{\theta(L)}
\end{equation}

A equação~\ref{eq:arma_1} é prontamente generalizada para corresponder à
equação~\ref{eq:arma} estabelecendo:

$$\phi(L) = 1 - \alpha_1 L - \alpha_2 L^2 \hdots - \alpha_p L^p$$

$$\theta(L) = 1 - \beta_0 - \beta_1 L^1 \hdots - \beta_q L^q$$

A forma da equação~\ref{eq:arma_l} se assemelha a uma função de transferência.
A analogia é também presente nas equações~\ref{eq:ar(p)_l} e~\ref{eq:ma(p)_2}
de forma menos clara. O paralelo entre a teoria de modelos AR, MA e ARMA e a
teoria de sistemas em torno da ideia de funções de transferência é extensa
e presente principalmente no contexto de séries multivariadas. Essa
interseção é explorada na seção correspondente.

% TODO: corrigir notação de 1, beta_0 etc! Dos modelos ARMA. Diferença relevante.

Uma propriedade interessante de modelos ARMA é que modelos estacionários
frequentemente podem ser modelados por modelos ARMA com menos parâmetros que
modelos AR ou MA.

Por fim é interessante observar que um modelo $ARMA(p, 0)$ corresponde
a um modelo $AR(p)$ e um modelo $ARMA(0, q)$ corresponde a um modelo
$MA(q)$.

\subsection{Raízes Unitárias}
\label{sec:unit_roots}

\section{Modelo ARIMA}
\label{sec:ARIMA}

TODO: rever TUDO!

Também chamados de modelos ARMA Integrados modelos ARIMA são essencialmente
modelos ARMA com tentativas de remoção de não estacionariedade. Assumindo que
a série em questão seja diferença-estacionária (ou tendência-estacionária
com tendência linear) o modelo ARIMA tenta induzir estacionariedade
introduzindo raízes unitárias em um modelo ARMA.

A distinção entre o modelo ARMA e ARIMA é a substituição de $y_t$ por $\nabla^d
y_t$.

Definindo $w_t = \nabla^d y_t$ para $d = 0, 1, 2, ...$ temos a definição de um
processo $ARIMA(p, d, q)$ dada pela equação~\ref{eq:arima}. Observe que a
equação é idêntica à equação~\ref{eq:arma} com $w_t$ ao invés de $y_t$ e que a
saída é $w_t$, demandando uma integração para previsão de $y_t$. O nome do
modelo referencia essa operação de reconstrução de $y_t$.

\begin{equation}\label{eq:arima}
    w_t = \sum_{i=0}^{i=q} \beta_i \varepsilon_{t-i} + \sum_{i=1}^{i=p} w_{t-i}\alpha_i
\end{equation}

Podemos constatar que o modelo ARIMA é simplesmente um ARMA após uma
diferenciação de ordem arbitrária da série temporal sob análise.

Podemos escrever um modelo $ARIMA(p, d, q)$ por meio do operador de atrasos
por meio da equação~\ref{eq:arima_l}, onde:

$$\phi(L) = 1 - \alpha_1 L - \alpha_2 L^2 \hdots - \alpha_p L^p$$

$$\theta(L) = 1 - \beta_0 - \beta_1 L^1 \hdots - \beta_q L^q$$

Nessa forma o paralelo entre modelos ARMA e ARIMA se torna mais claro.

\begin{equation}\label{eq:arima_l}
    y_t = Z_t \frac{\theta(L)}{\phi(L)} \frac{1}{(1-L)^d}
\end{equation}

O modelo ARIMA segue a metologia Box-Jenkins de modelagem, assumindo
diferença-estacionariedade e tentando induzir estacionariedade por meio de
sucessivas operações de diferenças. O modelo é portanto apropriado para
modelagem de séries não estacionárias, apresentando a mesma propriedade de
modelos ARMA de frequentemente demandar menos parâmetros que modelos AR ou MA.

Por fim é interessante observar que um modelo $ARIMA(p, 0, q)$ corresponde
a um modelo $ARMA(p, q)$.

\subsection{Estabilidade e Invertibilidade}
\label{ssec:stability_invertibility}

\section{Modelos de Sazonalidade}

\subsection{Diferenciação Sazonal}

\subsection{SARIMA}\label{ssec:SARIMA}

O modelo ARIMA pode ser estendido de forma natural para acomodar padrões de
sazonalidade. O modelo SARIMA (Seasonal ARIMA) parte da observação que saídas
de sinais com alta sazonalidade podem possuem alta correlação com saídas
anteriores em uma distância corresponde ao padrão de sazonalidade, como
constatado no capítulo 1.

O modelo SARIMA então estabelece, em adição à operação de diferenciação do
modelo ARIMA, uma operação de diferenciação sazonal ao definir a componente
$w_t$ da seguinte forma:

$$ w_t = \nabla^D_{s}\nabla^d  y_t $$

Como na definição de $w_t$ para o modelo ARIMA temos uma diferenciação
tradicional de ordem $d$ seguida de uma diferenciação sazonal de período $s$
e ordem $D$.

Como exemplo um $w_t$ definido em função de $d=1$, $D=1$ e $s=24$ assume a
seguinte forma.

$$ w_t =  \nabla^1_{2} \nabla^1 y_t = \nabla^1_{2} (y_t - y_{t-1}) $$
$$ w_t =  (y_t - y_{t-24}) - (y_{t-1} - y_{t-25}) $$

Em seguida componentes autoregressivos e de média móvel são introduzidos com
atrasos em múltiplos de $s$. O modelo final é descrito como $SARIMA(p, d, q)
(P, D, Q)_{s}$, com $P$ e $D$ referenciando os componentes $AR$ e $MA$
sazonais. Para ilustrar melhor esse conceito um pouco confuso podemos analisar a
expressão de um modelo específico para depois generalizar.

Um modelo $SARIMA(1, 0, 1)(2, 1, 1)_{12}$ é dado pela seguinte expressão.

$$ w_t = \varepsilon_t + \overbrace{\alpha_1 w_{t-1}}^{p = 1} + \overbrace{
\alpha_2 w_{t-12}}^{P=1} + \overbrace{\alpha_3 w_{t-24}}^{P=2} +
\overbrace{\beta_1 \varepsilon_{t-1}}^{q=1} + \overbrace{\beta_2
\varepsilon_{t-12}}^{Q=1}$$

A escrita desse modelo por meio do operador de atraso é mais clara.

$$  w_t = \varepsilon_t \frac{\beta_1 L^1 + \beta_2 L^{12}}{(1 - \alpha_1 L^1 + \alpha_2 L^{12} + \alpha_3 L^{24})}  $$

Podemos então generalizar o modelo na forma do operador de atraso por meio da
equação~\ref{eq:sarima}.

\begin{equation}\label{eq:sarima}
    w_t = \varepsilon_{t} \frac{\theta_q(L)\theta_Q(L)}{\phi_p (L) \phi_P (L^s)}
\end{equation}

Com os polinômios de atraso definidos como nos casos anteriores.

O modelo SARIMA é interessante ao herdar as propriedades e habilidades de
ARIMA com a capacidade adicional de modelar padrões sazonais com poucos
parâmetros. Os parâmetros do modelo tendem a ser difíceis de ajustar
especialmente na presença de múltiplos padrões de sazonalidade mas ferramentas
de ajuste automático de parâmetros são prontamente disponíveis.

\subsection{Variáveis de Fourier}

\chapter{Teoria de Volatilidade}
\label{sec:volatility_theory}

\section{Definições Adicionais}

\section{Modelo de Heterocedacidade Condicional Autoregressiva Generalizada}
\label{sec:garch}

\section{Modelo de Volatilidade Estocástica}

\section{Modelos Adicionais}

\chapter{Teoria Multivariada}

\section{Definições Adicionais}

Adicionar medidas de similaridade? Possivelmente em capitulo de analise
moderna

\subsection{Correlação Cruzada}

\subsection{Cointegração}

\section{Modelos em Espaço de Estados}

\section{Modelos Multivariados}

\section{Modelos de Variáveis Exógenas}

\chapter{Testes Estatísticos}

\section{Considerações sobre testes em séries temporais}

\section{Teste de \emph{Dickey-Fuller}}

Os testes de \emph{Dickey-Fuller} testam a hipótese nula de presença de raízes
unitárias no processo gerador de uma série temporal com a hipótese alternativa
de estacionariedade.

O teste mais simples de \emph{Dickey-Fuller} assume que o processo gerador da
série temporal em questão é dado pela equação~\ref{eq:ad}, na qual
$\varepsilon$ é ruído branco.

\begin{equation}\label{eq:ad}
    y_t = \phi y_{t-1} + \varepsilon_t
\end{equation}

A hipótese nula do teste é a presença de raízes unitárias em $\mathbf{y}_t$,
isso é, $\phi = 1$, e a hipótese alternativa é $\phi < 1$, correspondente à
estacionariedade.

$$
\begin{cases}
    H_0: \phi = 1, \text{não estacionariedade (raízes unitárias, possível diferença-estacionariedade)} \\
    H_1: \phi < 1, \text{estacionariedade}
\end{cases}
$$

Em seguida $Ly_t$ é subtraído de ambos os lados da equação~\ref{eq:ad},
resultando no desenvolvimento a seguir:

$$ y_t - y_{t-1} = \phi y_{t-1} - y_{t-1} + \varepsilon $$
$$ \nabla y_t = (\phi - 1) y_{t-1}  + \varepsilon $$
$$ \nabla y_t = \delta y_t  + \varepsilon $$

As hipóteses do teste são agora reformuladas para as seguintes

$$
\begin{cases}
    H_0: \delta = 0, \text{não estacionariedade (raízes unitárias, diferença-estacionariedade)} \\
    H_1: \delta < 0, \text{estacionariedade}
\end{cases}
$$

TODO: definir estatística de teste

A estatística de teste é computada e comparada com um valor crítico proveniente
da distribuição de \emph{Dickey-Fuller} (geralmente sob $p=0.5$) para rejeição
ou não da hipótese nula.

O modelo da série temporal, dado explicitamente pela equação~\ref{eq:ad}, pode
ser alterado para testar raízes unitárias com constante e com constante e
tendência determinística no tempo por meio das equações~\ref{eq:ad_constant} e
~\ref{eq:ad_constant_trend}, respectivamente. Note que ambas as equações são
apresentadas em função de $\delta$. É mais comum testar por apenas raízes
unitárias, uma vez que uma análise subjetiva visual, por correlalograma ou
decomposição juntamente de remoção de tendência ou sazonalidade é tipicamente
realizada antes de um teste estatístico de estacionariedade.

\begin{equation}\label{eq:ad_constant}
    \nabla y_t = \delta y_{t-1} + u_t + a_0
\end{equation}

\begin{equation}\label{eq:ad_constant_trend}
    \nabla y_t = \delta y_{t-1} + u_t + a_0 + a_1 t
\end{equation}

O teste aumentado de \emph{Dickey-Fuller}(ADF) modela o processo de forma mais
geral, incluindo na equação~\ref{eq:ad} termos representativos de processos
estacionários arbitrários. O teste aumentado é projetado para remover
autocorrelação do processo de validação de hipótese. De forma identicamente
análoga ao teste de DF temos expansões do ADF para incluir constantes e
tendências como nas equações~\ref{eq:ad_constant} e
~\ref{eq:ad_constant_trend} apesar de que, como no teste de DF, essas
variações são pouco usadas. A estatística de teste do ADF é negativa, isso é,
quanto menor seu valor maior a rejeição da hipótese nula de não
estacionariedade (maior certeza de estacionariedade).

Como em qualquer teste de hipótese um valor $p$ maior que $0.05$ indica falha
em rejeitar a hipótese nula, nesse caso correspondendo à impossibilidade de
constatar estacionariedade. Um valor $p$ menor ou igual a $0.05$ indica
rejeição da hipótese nula, correspondendo à conclusão que a série sob análise
é estacionária.

Na prática o teste mais usado é o ADF que é uma simples extensão mais robusta
do teste de \emph{Dickey-Fuller}. Implementações eficiente e populares existem
para \verb+R+ e \verb+Python+.

\subsection{Teste \emph{Kwiatkowski-Phillips-Schmidt-Shin} (KPSS)}

TODO: check this

O teste KPSS desempenha uma função semelhante ao ADF com a relevante
diferença de inerentemente modelar uma tendência linear no tempo por meio da
equação~\ref{eq:KPSS}. Seu desenvolvimento matemático é análogo porém mais
trabalhoso que o caso do ADF e será portanto omitido.

\begin{equation}\label{eq:KPSS}
    y_t = \phi y_{t-1} + \varepsilon_t + \beta t
\end{equation}

Com $\varepsilon_t$ representando ruído branco. O teste em seguida define as
seguintes hipóteses:

$$
\begin{cases}
    H_0: \text{a série apresenta tendência-estacionariedade} \\
    H_1: \text{a série apresenta raízes unitárias}
\end{cases}
$$

Como em qualquer teste de hipótese um valor $p$ maior que $0.05$ indica falha
em rejeitar a hipótese nula, nesse caso correspondendo à impossibilidade de
constatar que a série não apresenta tendência estacionariedade, boa evidência
de que a série é tendência-estacionária. Um valor $p$ menor ou igual a $0.05$
indica rejeição da hipótese nula, correspondendo à conclusão que a série sob
análise possui raízes unitárias e é portanto não estacionária.

Observe que há uma diferença crítica: a alternativa nula não postula não
estacionariedade, como no caso do ADF, mas sim tendência-estacionariedade
(seção ~\ref{ssec:taxonomy}) decorrente diretamente da inclusão de tendência
linear no modelo da equação~\ref{eq:KPSS}. A diferença principal da alteração
da hipótese nula é que o KPSS é usado para investigar presença de
estacionariedade sob uma tendência determinística (tendência-estacionariedade)
e o ADF (tipicamente) de estacionariedade propriamente dita. O KPSS é bem
implementado em \verb+R+ e \verb+Python+.

\section{Causalidade de Granger}

\section{Box-Pierce}

\section{Ljung-Box-Pierce}

\section{\emph{Convergence Cross Mapping}}

\section{Johansen}

\chapter{Validação de Modelos}

\section{Análise de Resíduos}
\label{sec:residual_analysis}

\section{Validação Cruzada}

\chapter{Análise Espectral}

\chapter{Modelos Não Lineares}
\label{chap:non_stationary}

\chapter{Análise de Séries Não Estacionárias}

\section{Definições Adicionais}
\subsection{Autocorrelação Instantânea}
\label{ssec:inst_autocorr}

\section{Indução de Estacionariedade}

\chapter{Tópicos Adicionais}
