\chapter{Teoria Univariada}
\label{chap:univariate_theory}

\section*{Introdução}

O seguinte capítulo tem por objetivo introduzir uma parte pequena mas
representativa do corpo clássico de análise de séries temporais univariadas
usando uma base de vocabulário e terminologia desenvolvida no capítulo 2.

\newpage

\section{Decomposição de Séries Temporais}
\label{sec:decomposition}

Decomposição em séries temporais tipicamente descreve o processo de
representação de uma série por uma combinação linear de três componentes:
tendência, sazonalidade e resíduos. A decomposição de uma série $y_t$ pode ser
aditiva ou multiplicativa como expressada pelas equações~\ref{eq:add_decomp} e
~\ref{eq:mult_decomp} respectivamente.

\begin{equation}\label{eq:add_decomp}
    y_t = S_t + T_t + R_t
\end{equation}

\begin{equation}\label{eq:mult_decomp}
    y_t = S_t \cdot T_t \cdot R_t
\end{equation}

A escolha de decomposição aditiva ou multiplicativa deve ser feita de acordo
com o tipo de sazonalidade (seção~\ref{sec:seasonality}).

Como herança de econometria grande parte dos algoritmos clássicos de
decomposição (X11~\cite{x11}, SEATS~\cite[capítulo~5.2]{SEATS} e derivados) são
baseados em período de sazonalidade anuais, semestrais, trimestrais e
mensais~\cite{athana}. A incapacidade desses algoritmos de processar dados com
período menor os torna pouco utilizáveis no contexto de sinais elétricos, por
exemplo, cujas séries tipicamente são de período inferior a um dia,
apresentando padrões sazonais com período semelhante.

Antes de prosseguir é interessante comentar sobre os principais objetivos de
decomposição:

\begin{enumerate}

    \item Análise: O processo de pensar sobre, elaborar, ajustar e observar
    resultados de decomposição são altamente informativos da natureza da
    série sob análise. A operação é frequentemente usada para fins de análise
    exploratória.

    \item Indução de estacionariedade: Como tendência sempre é e sazonalidade pode ser
    um padrão cuja presença qualifica não estacionariedade~\ref{sec:stationarity}
    sua remoção pode tornar uma série ``mais estacionária" ou pelo menos mais
    apropriada para modelagem por meio de modelos não estacionários.

    \item Detecção de Anomalias: Eventos anômalos são tipicamente dissociados
    de e ocultados por estruturas de tendência e sazonalidade. A remoção
    dessas estruturas tende a expor anomalias de forma mais detectável.

    \item Modelagem e Previsão: A representação de uma série por meio de três
    componentes distintas é interessante para problemas de previsão pela
    possibilidade de desenvolver modelos e previsões para cada componente de
    acordo com suas propriedades. Os modelos independentes tendem a ser menos
    complexos, mais generalizáveis e mais robustos do que um modelo adequado
    para a série original, se existir.

\end{enumerate}

A imagem~\ref{fig:add_decomp} ilustra o resultado do processo de
decomposição aditiva de uma serie temporal sintética da forma que é tipicamente
apresentado: quatro linhas contendo a série original, componente de tendência ,
componente sazonal e componente residual em aparição decrescente. A operação
descrita pela equação~\ref{eq:add_decomp} equivale à constatação que a
imagem da primeira linha corresponde à soma das imagens das três linhas
inferiores.

TODO put add figure here

% \begin{figure}[H]
%     \centering
%     \includegraphics[scale=0.6]{figures/add_decomposition.png}
%     \caption{Exemplo de decomposição de série sintética}
%     \label{fig:add_decomp}
% \end{figure}

\subsection{Modelagem de Tendência}

Uma operação essencial em decomposição de séries temporais é a determinação
de uma tendência subjacente dos dados. Os principais procedimentos para tal
são abordados a seguir.

\subsubsection{Média Móvel}

Um filtro de média móvel de ordem $m$ é tipicamente expressado como:

\begin{equation}\label{eq:ma}
    MA_{t_{m}} = \frac{1}{m} \sum_{i=-k}^{i=k} y_{t+i}
\end{equation}

A operação é visualizada pela figura~\ref{fig:MA}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/moving_average.png}
    \caption{Visualização de aplicação de filtro média móvel com m=15}
    \label{fig:MA}
\end{figure}

TODO: talk (and research about !) the use of kalman filter for moving average

\subsubsection{Regressão Localizada/Filtro de Savitzky-Golay}

Uma forma mais robusta e granular de modelagem de tendência é o algoritmo
LOESS, baseado em regressão localizada e conhecido em linguagem de processamento
de sinais como o filtro de Savitzky-Golay. Apesar do algoritmo ser simples
seu desenvolvimento será omitido por ser matematicamente verboso. É importante
que o leitor conheça a existência do método LOESS, sabendo que ele oferece uma
alternativa mais precisa para modelagem de tendência do que média móvel,
apesar de ser potencialmente mais computacionalmente complexo. Uma breve
intuição sobre seu funcionamento é apresentada a seguir.

A ideia fundamental do algoritmo é repartir a série temporal em grupos de
pontos menores, calculando uma regressão (tipicamente mas não necessariamente
linear) nesses grupos menores de forma a construir uma curva linear por partes
que aproxima a tendência da curva original. O tamanho dos grupos é arbitrário
e tipicamente informado como uma fração do tamanho da série completa e os grupos
em si são determinados pelos $n$ pontos mais próximos a um elemento chamado de
``ponto focal'' que é incrementado a cada iteração. O valor dos incrementos dos
pontos focais também é um parâmetro.

A figura~\ref{fig:loess} ilustra o resultado do algoritmo LOESS para a mesma
série analisada pela figura~\ref{fig:MA}. A figura ilustra curvas de tendência
modeladas via LOESS para diferentes tamanhos de grupos. Os tamanhos são
informados como porcentagem do comprimento total do sinal. Observa-se que o uso
de uma porcentagem menor do sinal aumenta a localidade da regressão tornando o
ajuste da tendência mais sinuosa.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/loess.png}
    \caption{Visualização de algoritmo LOESS}
    \label{fig:loess}
\end{figure}

\subsection{Modelagem de Sazonalidade(Ajuste de Sazonalidade)}

A operação fundamental de decomposição é a modelagem ou ajuste de sazonalidade.
Esse processo se preocupa em identificar estruturas fortemente periódicas em
uma série temporal.

TODO: como fazer isso?

\subsection{Decomposição Clássica (Aditiva)}

TODO: ver se isso entra mesmo

O algoritmo de decomposição clássica é simples e capaz de identificar períodos
arbitrários de sazonalidade. O procedimento é apresentado passo a passo
acompanhando a decomposição de uma série temporal de corrente elétrica.

TODO: colocar a decomposição da corrente elétrica aqui ou na seção de aplicações

\subsubsection{Passo 1}

\textbf{Identificar o período de sazonalidade que se deseja modelar.}

O período $m$ de sazonalidade corresponde à duração de um ciclo de um padrão
periódico observável nos dados. A determinação da duração desse ciclo não é
necessariamente trivial, especialmente tratando de séries de alta frequência,
sendo interessante inclusive usar métodos automáticos para determinação da
distância entre amostras iniciais dos períodos de sazonalidade.

TODO: incluir algum método de detecção de período? seria bom!

\subsubsection{Passo 2}

\textbf{Modelar tendência }$\mathbf{T_t}$\textbf{ por meio de média móvel}

A tendência $T_t$ é modelada calculando a média móvel de ordem $m$ da série.

\subsubsection{Passo 3}

\textbf{Remover tendência da série}

A componente sem tendência é dada por $y_t - T_t$. A série resultante deve
possuir média aproximadamente nula.

\subsubsection{Passo 4}

\textbf{Estimar componente sazonal}

A estimativa da componente sazonal é realizada calculando o ``padrão sazonal
médio'' da série sem tendência. A série de tamanho $n$ sem tendência é divida
em $s$ segmentos de $m$ amostras, no qual $s = \frac{n}{m}$. Usando livremente
a vírgula como símbolo para concatenação podemos enumerar os segmentos como
$$\chi_1, \chi_2, ... \chi_s$$ onde um segmento arbitrário $$\chi_i =
\chi_{i_{1}}, \chi_{i_{1}} ... \chi_{i_{m}}$$

O padrão sazonal modelado $S'_t$ corresponde ao segmento médio, isso é,

$$ S'_t = \left(\frac{1}{s} \sum_{k=1}^{k=capcas} \chi_{1_{k}}\right), \left(\frac{1}{s} \sum_{k=1}^{k=s} \chi_{2_{k}}\right), \left(\frac{1}{s} \sum_{k=1}^{k=s} \chi_{3_{k}}\right) \hdots \left(\frac{1}{s} \sum_{k=1}^{k=s} \chi_{m_{k}}\right)$$

A componente sazonal $S_t$ é então dada pela concatenação de $m$ cópias de
$S'_t$

$$ S_t = \overbrace{S'_t, S'_t, \hdots, S'_t}^{s\text{ vezes}} $$

e posterior remoção de sua tendência.

\subsubsection{Passo 5}

\textbf{Calcular a componente residual}

Por fim calcula-se a componente residual $R_t$ como

$$ R_t = y_t - T_t - S_t $$

A série está enfim descomposta

$$ y_t = R_t + T_t + S_t $$

\subsection{Decomposição STL}

TODO: será que isso entra?

\subsection{Qualificação de tendência e sazonalidade}

A presenta de tendência ou sazonalidade é tipicamente visível imediatamente.
Dúvida sobre a presença ou não dessas propriedades pode surgir se o sinal
estiver imerso em ruído. Se houver presença de raízes unitárias curtos
intervalos de tempo podem ser ambíguos quanto ao determinismo de sua tendência,
isso é, uma tendência estocástica pode ser confundida com tendência
determinística. A figura~\ref{fig:ambiguous_trend} exemplifica um sinal com
esse tipo de ambiguidade.

Nesses cenários é interessante detectar a presença desses componentes.
TODO: como? testes estatisticos, análise de correlalograma

\subsection{Quantificação de tendência e sazonalidade}

\subsubsection{Força}

A decomposição de uma série temporal em componentes isolados de tendência,
sazonalidade e resíduos permite que a intensidade de tendência e sazonalidade
sejam quantificáveis de forma elegante. Essa intensidade de tendência ou
sazonalidade é tipicamente chamada de força.

Para séries com forte tendência é esperado que a componente de tendência
contenha considerável variância. Uma forma interessante de quantificar a força
da tendência de uma série, proposta por Hyndman et. al.~\cite{athana}, parte
da observação da variância adicional introduzida pela adição do termo de
tendência ao residual:

$$ \frac{Var(R_t)}{Var(T_t + R_t)} $$

Espera-se que essa razão seja pequena para séries com alta tendência, isso é,
haja introdução de considerável variância pela adição da componente de
tendência no denominador. Podemos então definir a força $F_T$ da tendência de
uma série como

$$ F_T = max\left(0, 1 - \frac{Var(R_t)}{Var(T_t + R_t)}\right) $$

TODO: Formalize $F_T$ é um número real limitado entre 0 e 1.
TODO: Observe que $F_T \in real$ limitado entre 0 e 1.

De forma identicamente análoga podemos definir a força da sazonalidade de uma
série como

$$ F_S = max\left(0, 1 - \frac{Var(R_t)}{Var(S_t + R_t)}\right) $$

Essas medidas são úteis ao oferecer uma interface quantitativa aos atributos
tipicamente qualitativos de tendência e sazonalidade. É importante deixar
claro que essas medidas necessitam que a série seja decomposta em suas
componentes de tendência e sazonalidade, possivelmente dificultando sua
aplicação.

\section{Modelos Estacionários}

\subsection{O Modelo Estacionário como um Filtro Linear}

Segundo Box e Jenkins~\cite[capítulo~1.2.1]{box} os modelos estacionários que
serão abordados nas seguinte seções foram idealizados por (YULE, 1927) como
filtros lineares que modelam a série temporal de interesse por meio do
processamento de ruído branco. A modelagem apropriada do sinal em questão se
resume então ao apropriado ajuste da função de transferência $\psi$ desse
filtro a partir dos dados observados, isso é, o apropriado posicionamento de
zeros e/ou polos.

A função de transferência $\psi$ é definida no domínio do tempo por meio do
operador de atraso $B$ ao invés de $z^{-1}$. Como no contexto de filtros
digitais podemos pensar em um plano $B$, em analogia com o plano $Z$, e chegar às
exatas mesmas conclusões sobre o efeito do posicionamento de polos e zeros na
estabilidade (inclusive marginal), invertibilidade e resposta em frequência do
filtro. A diferença relevante é que o operador de atraso no domínio $z$,
$z^{-1}$ é definido de forma inversa à referência do plano. Essa inversão não
acontece no plano $B$, levando a uma inversão das propriedades conhecidas.

É possível também descrever certos tipos de modelos-filtros como \emph{Finite
Impulse Response}(FIR) ou \emph{Infinite Impulse Response}(IIR) dependendo
a equação de recorrência, levando às propriedades conhecidas desses tipos de
filtros.

A figura TODO torna clara a interpretação traçada. Para tornar o paralelo mais
claro podemos inicialmente pensar em um sinal de entrada $\varepsilon[n]$ (
ruído branco) transformado por uma resposta ao impulso $h[n]$, causal de
tamanho $m$, no sinal $y[n]$ pela seguinte soma de convolução

$$ y[n] = \sum^{m}_{k=0} h[n-k]\varepsilon[k] =  \sum^{m}_{k=0} \varepsilon[n-k]h[k]$$

Introduzindo o operador de atraso $B$ no somatório

$$ y[n] = \sum^{m}_{k=0} B^{k}h[k] \varepsilon[n]$$

observamos que temos um polinômio em $B$ cujos coeficientes correspondem aos
da resposta ao impulso do filtro. Podemos então, chaveando para notação de
séries temporais, reescrever a convolução como

$$ \sum^{m}_{k=0} \psi(k)B^k\varepsilon_{t} = \sum^{m}_{k=0} \psi(k)\varepsilon_{t-k}$$

Expandindo as somas temos

$$ y_t = \varepsilon_t + \psi_1 \varepsilon_{t-1} + \psi_2 \varepsilon_{t-2} + ... +  \psi_m \varepsilon_{t-m}$$
$$ y_t = \varepsilon_t + \psi_1 B\varepsilon_{t} + \psi_2 B^2 \varepsilon_{t} + ... +  \psi_m B^m \varepsilon_{t}$$
$$ y_t = \varepsilon_t(1 + \psi_1 B + \psi_2 B^2 + ... + \psi B^3)$$
$$ y_t = \psi(B)\varepsilon_t$$

onde $\psi(B)$ é um polinômio mônico em $B$.

Tikz Here


A interpretação de modelos estacionários como filtros tem suas limitações no
que diz respeito às possibilidades de herança de conhecimento da área de
processamento de sinais. Inicialmente podemos constatar que filtros lineares no
sentido tradicional se propõem em alterar as propriedades de um sinal
arbitrário que por si já contém informação. Isso leva à importância imediata da
função de impulso unitário como base natural de sinais em tempo discreto e a
uma enorme preocupação com a fase da resposta ao impulso do filtro devido à
facilidade de distorção de fase indesejada do sinal processado. Para o
modelo-filtro a função de impulso unitário não é tão importante porque esse
filtro é sempre excitado pelo mesmo tipo de sinal que não possui nenhum tipo de
estrutura a se preservar: ruído branco. A estrutura de fase desse sinal é menos
relevante ainda por ser aleatória. Essas diferenças tornam muita da teoria de
filtros inaplicável no contexto de séries temporais. Não há sentido de projetar
um modelo filtro de fase linear generalizada, por exemplo, se não há o que
preservar na fase do sinal de entrada e nenhuma preocupação com atraso de grupo.

Não obstante a interpretação de modelos estacionários como filtros lineares
facilita a assimilação de propriedades desses modelos por meio da linguagem de
processamento de sinais e nos leva a interessantes explorações de parte da
teoria de filtros aplicada à modelagem de séries temporais.

\subsection{Modelo Linear Generalizado}
\label{sec:glm}

\subsection{Modelo Média Móvel}
\label{ssec:MA(p)}

Um processo $\mathbf{Y}(t)$ é considerado de média móvel de ordem $q$ se ele
for definido pela equação~\ref{eq:ma(q)}, na qual $\varepsilon_t$ representa um
processo puramente aleatório. O processo é tipicamente chamado de $MA(q)$.

\begin{equation}\label{eq:ma(q)}
    y_t = \sum_{i=0}^{i=q} \beta_i \varepsilon_{t-i}
\end{equation}

Um processo de média móvel, como sugerido pelo nome, é uma média ponderada de
de observações anteriores de série temporal de ruído branco.

A seguinte expressão exemplifica um processo $MA(2)$.

$$ y_t = \beta_0 \varepsilon_t + \beta_1 \varepsilon_{t-1} + \beta_2 \varepsilon_{t-2} $$

Como no caso do processo autoregressivo podemos escrever o processo acima por
meio do operador de atraso, resultando na seguinte forma.

$$ y_t = Z_t (\beta_0 + L\beta_1 + L^2\beta_2) $$

Podemos escrever a equação~\ref{eq:ma(q)} usando o operador de atraso por meio
da equação~\ref{eq:ma(q)_L}.

\begin{equation}\label{eq:ma(q)_L}
    y_t = Z_t \sum_{i=0}^{i=q} \beta_{t-i} L^{i} = Z_t \phi(L)
\end{equation}

A figura~\ref{fig:ma_time} ilustra o comportamento temporal de modelos $MA$ para
diferentes ordens.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/ma_time.png}
    \caption{Visualização no tempo de processos média móvel de ordens
    diferentes.}
    \label{fig:ma_time}
\end{figure}

Como no caso dos modelos autoregressivos não temos clara identidade visual dos
processos média móvel.

\subsubsection{Condições para Estacionariedade}

Não há restrições de estacionariedade para um processo de média móvel de ordem
finito. Na prática qualquer processo $MA(p)$, $p < \infty$, é estacionário.

\subsubsection{Autocorrelação}

A função de autocorrelação de um processo média móvel exibe a interessante
propriedade de ``cortar'' após o atraso $q$, isso é, demonstrar autocorrelação
igual a zero após um atraso de número correspondente à ordem do processo. A
autocorrelação amostral de uma série temporal gerada por um processo de média
móvel tende a apresentar a mesma propriedade, apesar de ser perfeitamente
possível e provável da autocorrelação de um processo $MA(q)$ cair para zero
\emph{antes} do lag $q$.

A imagem~\ref{fig:ma_autocorr} demonstra a visualização dos correlalogramas
correspondentes aos modelos ilustrados no tempo pela figura~\ref{fig:ma_time}.
Observe que as autocorrelações são distintas de zero apenas para atrasos
iguais ou inferiores à ordem $q$ do processo.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/ma_corr.png}
    \caption{Visualização do correlalograma de processos média móvel de ordens
    diferentes.}
    \label{fig:ma_corr}
\end{figure}

Essa propriedade da função de autocorrelação possui consequências diretas
para modelagem de séries temporais: é possível que uma série temporal
estacionária com autocorrelações iguais a zero após atraso $k$ seja
parcialmente aproximada por um modelo $MA(k)$. Não há garantia nenhuma que
esse seja o caso, no entanto. Retomando uma discussão presente no capítulo 1
relembramos que a autocorrelação amostral de uma série temporal pode ser
gerada por infinitas funções, tal que a observação de uma autocorrelação
amostral característica de processos $MA(q)$ não garante que o processo
gerador da série temporal seja de fato $MA(q)$ - isso é apenas insinuado.
Nesses casos modelos $MA(q)$ são excelentes primeiras tentativas.


\subsection{Modelo Autoregressivo}
\label{ssec:AR(p)}

Um processo $\mathbf{Y}(t)$ é considerado autoregressivo de ordem $p$ se ele
for definido pela equação~\ref{eq:ar(p)}, onde $\varepsilon_t$ representa um
processo puramente aleatório. O processo é frequentemente chamado de $AR(p)$.

\begin{equation}\label{eq:ar(p)}
   y_t = \sum_{i=1}^{i=p} y_{t-i}\alpha_i + \varepsilon_t
\end{equation}

Como sugerido por seu nome um processo autoregressivo de ordem $p$ é
caraterizado por uma dependência entre uma amostra em um instante de tempo $t$
e as amostras de instantes de tempo $t-1$, $t-2$, ..., $t-p$. Observa-se que
uma amostra em tempo $t$ é exatamente uma média ponderada das $p$ amostras
anteriores.

Como exemplo temos que um processo autoregressivo de segunda ordem, isso é,
$AR(2)$, é definido pela expressão a seguir.

$$ y_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \varepsilon_t$$

Esse processo pode ser escrito por meio de operadores de atraso como apontado
pelo seguinte desenvolvimento.

$$ y_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \varepsilon_t$$
$$ y_t - \alpha_1 y_{t-1} - \alpha_2 y_{t-2} = \varepsilon_t$$
$$ y_t - L\alpha_1 y_{t} - L^2\alpha_2 y_{t-2} = \varepsilon_t$$
$$ (1 - L\alpha_1 - L^2\alpha_2)y_t= \varepsilon_t$$
$$ y_t= \frac{\varepsilon_t}{(1 - L\alpha_1 - L^2\alpha_2)}$$

O polinômio de operadores de atraso $1 - L\alpha_1 - L^2\alpha_2$ recebe o
nome de equação característica. Generalizando para ordem $p$ a expressão de um
processo autoregressivo em função do operador de atraso chegamos à equação
~\ref{eq:ar(p)_L}.

\begin{equation}\label{eq:ar(p)_L}
    y_t = \frac{Z_t}{1 - \sum_{i=1}^{i=p} L^i \alpha_i} \frac{1}{\theta(L)}
\end{equation}

A formulação desse processo em função de sua equação característica aproxima-se
de teoria de controle clássica inclusive no que pode ser compreendido da série
temporal a partir da equação característica. Uma análise das raízes desse
polinômio informa estabilidade, instabilidade ou estabilidade marginal, com o
caso de estabilidade marginal correspondo ao de raízes unitárias, discutido na
seção~\ref{ssec:unit_roots}. A relação entre a equação característica, suas
raízes e a dinâmica de uma série temporal é um tópico muito explorado por
Box e Jenkins em~\cite{box}, abordado na seção~\ref{ssec:multivariate}.

Uma visualização do comportamento temporal de processos autoregressivos é dada
pela figura~\ref{fig:ar_time_visualization}. Observa-se que as séries temporais
não exibem comportamento visivelmente distinguível no domínio do tempo.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/ar_time.png}
    \caption{Visualização de processos autoregressivos de diferentes ordens
    no domínio do tempo.}
    \label{fig:ar_time_visualization}
\end{figure}

\subsection{Modelo ARMA}
\label{ssec:ARMA}

\subsection{ARMA}\label{ssec:ARMA}

Processos ARMA, como sugerido pelo nome, são gerados pela sobreposição de
processos AR e MA. Um processo $\mathbf{Y}(t)$ é considerado $ARMA(p, q)$ se
for dado pela equação~\ref{eq:arma}

\begin{equation}\label{eq:arma}
    y_t = \sum_{i=0}^{i=q} \beta_i \varepsilon_{t-i} + \sum_{i=1}^{i=p} y_{t-i}\alpha_i
\end{equation}

Um processo $ARMA(2, 1)$ é portanto da seguinte forma.

$$ y_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \beta_0 Z_{t} + \beta_1 Z_{t-1}  $$

Em analogia aos casos anteriores o processo pode ser escrito por meio do
operador de atraso.

$$ y_t (1 - \alpha_1 L - \alpha_2 L^2) = Z_{t} (\beta_0 + \beta_1 L) $$

Podemos introduzir $\phi(L) = 1 - \alpha_1 L - \alpha_2 L$ e $\theta(L) =
\beta_0 + \beta_1 L$ tal que:

\begin{equation}\label{eq:arma_l}
    y_t = Z_t \frac{\phi(L)}{\theta(L)}
\end{equation}

A equação~\ref{eq:arma_1} é prontamente generalizada para corresponder à
equação~\ref{eq:arma} estabelecendo:

$$\phi(L) = 1 - \alpha_1 L - \alpha_2 L^2 \hdots - \alpha_p L^p$$

$$\theta(L) = 1 - \beta_0 - \beta_1 L^1 \hdots - \beta_q L^q$$

A forma da equação~\ref{eq:arma_l} se assemelha a uma função de transferência.
A analogia é também presente nas equações~\ref{eq:ar(p)_l} e~\ref{eq:ma(p)_2}
de forma menos clara. O paralelo entre a teoria de modelos AR, MA e ARMA e a
teoria de sistemas em torno da ideia de funções de transferência é extensa
e presente principalmente no contexto de séries multivariadas. Essa
interseção é explorada na seção correspondente.

% TODO: corrigir notação de 1, beta_0 etc! Dos modelos ARMA. Diferença relevante.

Uma propriedade interessante de modelos ARMA é que modelos estacionários
frequentemente podem ser modelados por modelos ARMA com menos parâmetros que
modelos AR ou MA.

Por fim é interessante observar que um modelo $ARMA(p, 0)$ corresponde
a um modelo $AR(p)$ e um modelo $ARMA(0, q)$ corresponde a um modelo
$MA(q)$.

\subsection{Raízes Unitárias}
\label{sec:unit_roots}

\section{Modelo ARIMA}
\label{sec:ARIMA}

TODO: rever TUDO!

Também chamados de modelos ARMA Integrados modelos ARIMA são essencialmente
modelos ARMA com tentativas de remoção de não estacionariedade. Assumindo que
a série em questão seja diferença-estacionária (ou tendência-estacionária
com tendência linear) o modelo ARIMA tenta induzir estacionariedade
introduzindo raízes unitárias em um modelo ARMA.

A distinção entre o modelo ARMA e ARIMA é a substituição de $y_t$ por $\nabla^d
y_t$.

Definindo $w_t = \nabla^d y_t$ para $d = 0, 1, 2, ...$ temos a definição de um
processo $ARIMA(p, d, q)$ dada pela equação~\ref{eq:arima}. Observe que a
equação é idêntica à equação~\ref{eq:arma} com $w_t$ ao invés de $y_t$ e que a
saída é $w_t$, demandando uma integração para previsão de $y_t$. O nome do
modelo referencia essa operação de reconstrução de $y_t$.

\begin{equation}\label{eq:arima}
    w_t = \sum_{i=0}^{i=q} \beta_i \varepsilon_{t-i} + \sum_{i=1}^{i=p} w_{t-i}\alpha_i
\end{equation}

Podemos constatar que o modelo ARIMA é simplesmente um ARMA após uma
diferenciação de ordem arbitrária da série temporal sob análise.

Podemos escrever um modelo $ARIMA(p, d, q)$ por meio do operador de atrasos
por meio da equação~\ref{eq:arima_l}, onde:

$$\phi(L) = 1 - \alpha_1 L - \alpha_2 L^2 \hdots - \alpha_p L^p$$

$$\theta(L) = 1 - \beta_0 - \beta_1 L^1 \hdots - \beta_q L^q$$

Nessa forma o paralelo entre modelos ARMA e ARIMA se torna mais claro.

\begin{equation}\label{eq:arima_l}
    y_t = Z_t \frac{\theta(L)}{\phi(L)} \frac{1}{(1-L)^d}
\end{equation}

O modelo ARIMA segue a metologia Box-Jenkins de modelagem, assumindo
diferença-estacionariedade e tentando induzir estacionariedade por meio de
sucessivas operações de diferenças. O modelo é portanto apropriado para
modelagem de séries não estacionárias, apresentando a mesma propriedade de
modelos ARMA de frequentemente demandar menos parâmetros que modelos AR ou MA.

Por fim é interessante observar que um modelo $ARIMA(p, 0, q)$ corresponde
a um modelo $ARMA(p, q)$.

\subsection{Estabilidade e Invertibilidade}
\label{ssec:stability_invertibility}

\section{Modelos de Sazonalidade}

\subsection{Diferenciação Sazonal}

\subsection{SARIMA}\label{ssec:SARIMA}

O modelo ARIMA pode ser estendido de forma natural para acomodar padrões de
sazonalidade. O modelo SARIMA (Seasonal ARIMA) parte da observação que saídas
de sinais com alta sazonalidade podem possuem alta correlação com saídas
anteriores em uma distância corresponde ao padrão de sazonalidade, como
constatado no capítulo 1.

O modelo SARIMA então estabelece, em adição à operação de diferenciação do
modelo ARIMA, uma operação de diferenciação sazonal ao definir a componente
$w_t$ da seguinte forma:

$$ w_t = \nabla^D_{s}\nabla^d  y_t $$

Como na definição de $w_t$ para o modelo ARIMA temos uma diferenciação
tradicional de ordem $d$ seguida de uma diferenciação sazonal de período $s$
e ordem $D$.

Como exemplo um $w_t$ definido em função de $d=1$, $D=1$ e $s=24$ assume a
seguinte forma.

$$ w_t =  \nabla^1_{2} \nabla^1 y_t = \nabla^1_{2} (y_t - y_{t-1}) $$
$$ w_t =  (y_t - y_{t-24}) - (y_{t-1} - y_{t-25}) $$

Em seguida componentes autoregressivos e de média móvel são introduzidos com
atrasos em múltiplos de $s$. O modelo final é descrito como $SARIMA(p, d, q)
(P, D, Q)_{s}$, com $P$ e $D$ referenciando os componentes $AR$ e $MA$
sazonais. Para ilustrar melhor esse conceito um pouco confuso podemos analisar a
expressão de um modelo específico para depois generalizar.

Um modelo $SARIMA(1, 0, 1)(2, 1, 1)_{12}$ é dado pela seguinte expressão.

$$ w_t = \varepsilon_t + \overbrace{\alpha_1 w_{t-1}}^{p = 1} + \overbrace{
\alpha_2 w_{t-12}}^{P=1} + \overbrace{\alpha_3 w_{t-24}}^{P=2} +
\overbrace{\beta_1 \varepsilon_{t-1}}^{q=1} + \overbrace{\beta_2
\varepsilon_{t-12}}^{Q=1}$$

A escrita desse modelo por meio do operador de atraso é mais clara.

$$  w_t = \varepsilon_t \frac{\beta_1 L^1 + \beta_2 L^{12}}{(1 - \alpha_1 L^1 + \alpha_2 L^{12} + \alpha_3 L^{24})}  $$

Podemos então generalizar o modelo na forma do operador de atraso por meio da
equação~\ref{eq:sarima}.

\begin{equation}\label{eq:sarima}
    w_t = \varepsilon_{t} \frac{\theta_q(L)\theta_Q(L)}{\phi_p (L) \phi_P (L^s)}
\end{equation}

Com os polinômios de atraso definidos como nos casos anteriores.

O modelo SARIMA é interessante ao herdar as propriedades e habilidades de
ARIMA com a capacidade adicional de modelar padrões sazonais com poucos
parâmetros. Os parâmetros do modelo tendem a ser difíceis de ajustar
especialmente na presença de múltiplos padrões de sazonalidade mas ferramentas
de ajuste automático de parâmetros são prontamente disponíveis.

\subsection{Variáveis de Fourier}
