\chapter{Teoria Univariada}
\label{chap:univariate_theory}

\section*{Introdução}

O seguinte capítulo tem por objetivo introduzir uma parte pequena mas
representativa do corpo clássico de análise de séries temporais univariadas
usando uma base de vocabulário e terminologia desenvolvida no capítulo 2.

\newpage

\section{Decomposição de Séries Temporais}
\label{sec:decomposition}

Decomposição em séries temporais tipicamente descreve o processo de
representação de uma série por uma combinação linear de três componentes:
tendência, sazonalidade e resíduos. A decomposição de uma série $y_t$ pode ser
aditiva ou multiplicativa como expressada pelas equações~\ref{eq:add_decomp} e
~\ref{eq:mult_decomp} respectivamente.

\begin{equation}\label{eq:add_decomp}
    y_t = S_t + T_t + R_t
\end{equation}

\begin{equation}\label{eq:mult_decomp}
    y_t = S_t \cdot T_t \cdot R_t
\end{equation}

A escolha de decomposição aditiva ou multiplicativa deve ser feita de acordo
com o tipo de sazonalidade (seção~\ref{sec:seasonality}).

Como herança de econometria grande parte dos algoritmos clássicos de
decomposição (X11~\cite{x11}, SEATS~\cite[capítulo~5.2]{SEATS} e derivados) são
baseados em período de sazonalidade anuais, semestrais, trimestrais e
mensais~\cite{athana}. A incapacidade desses algoritmos de processar dados com
período menor os torna pouco utilizáveis no contexto de sinais elétricos, por
exemplo, cujas séries tipicamente são de período inferior a um dia,
apresentando padrões sazonais com período semelhante.

Antes de prosseguir é interessante comentar sobre os principais objetivos de
decomposição:

\begin{enumerate}

    \item Análise: O processo de pensar sobre, elaborar, ajustar e observar
    resultados de decomposição são altamente informativos da natureza da
    série sob análise. A operação é frequentemente usada para fins de análise
    exploratória.

    \item Indução de estacionariedade: Como tendência sempre é e sazonalidade
    pode ser um padrão cuja presença qualifica não estacionariedade sua
    remoção pode tornar uma série ``mais estacionária" ou pelo menos mais
    apropriada para modelagem por meio de modelos não estacionários.

    \item Detecção de Anomalias: Eventos anômalos são tipicamente dissociados
    de e ocultados por estruturas de tendência e sazonalidade. A remoção
    dessas estruturas tende a expor anomalias de forma mais detectável.

    \item Modelagem e Previsão: A representação de uma série por meio de três
    componentes distintas é interessante para problemas de previsão pela
    possibilidade de desenvolver modelos e previsões para cada componente de
    acordo com suas propriedades. Os modelos independentes tendem a ser menos
    complexos, mais generalizáveis e mais robustos do que um modelo adequado
    para a série original, se existir.

\end{enumerate}

\subsection{Modelagem de Tendência}

Uma operação essencial em decomposição de séries temporais é a determinação
de uma tendência subjacente dos dados. Os principais procedimentos para tal
são abordados a seguir.

\subsubsection{Média Móvel}\label{sssec:MA(p)}

Um filtro de média móvel de ordem $m$ é tipicamente expressado como:

\begin{equation}\label{eq:ma}
    MA_{t_{m}} = \frac{1}{m} \sum_{i=-k}^{i=k} y_{t+i}
\end{equation}

A operação é visualizada pela figura~\ref{fig:MA}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/moving_average.png}
    \caption{Visualização de aplicação de filtro média móvel com m=15}
    \label{fig:MA}
\end{figure}


\subsubsection{Regressão Localizada/Filtro de Savitzky-Golay}

Uma forma mais robusta e granular de modelagem de tendência é o algoritmo
LOESS, baseado em regressão localizada e conhecido em linguagem de processamento
de sinais como o filtro de Savitzky-Golay. Apesar do algoritmo ser simples
seu desenvolvimento será omitido por ser matematicamente verboso. É importante
que o leitor conheça a existência do método LOESS, sabendo que ele oferece uma
alternativa mais precisa para modelagem de tendência do que média móvel,
apesar de ser potencialmente mais computacionalmente complexo. Uma breve
intuição sobre seu funcionamento é apresentada a seguir.

A ideia fundamental do algoritmo é repartir a série temporal em grupos de
pontos menores, calculando uma regressão (tipicamente mas não necessariamente
linear) nesses grupos menores de forma a construir uma curva linear por partes
que aproxima a tendência da curva original. O tamanho dos grupos é arbitrário
e tipicamente informado como uma fração do tamanho da série completa e os grupos
em si são determinados pelos $n$ pontos mais próximos a um elemento chamado de
``ponto focal'' que é incrementado a cada iteração. O valor dos incrementos dos
pontos focais também é um parâmetro.

A figura~\ref{fig:loess} ilustra o resultado do algoritmo LOESS para a mesma
série analisada pela figura~\ref{fig:MA}. A figura ilustra curvas de tendência
modeladas via LOESS para diferentes tamanhos de grupos. Os tamanhos são
informados como porcentagem do comprimento total do sinal. Observa-se que o uso
de uma porcentagem menor do sinal aumenta a localidade da regressão tornando o
ajuste da tendência mais sinuosa.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/loess.png}
    \caption{Visualização de algoritmo LOESS}
    \label{fig:loess}
\end{figure}

\subsection{Filtro de Kalman}

TODO: talk (and research about !) the use of kalman filter for moving average

\subsection{Modelagem de Sazonalidade(Ajuste de Sazonalidade)}

A operação fundamental de decomposição é a modelagem ou ajuste de sazonalidade.
Esse processo se preocupa em identificar estruturas periódicas em uma série
temporal. Modelagem de sazonalidade em séries temporais é de fato uma área em
si~\cite{seasonal_modelling}. A seção~\ref{sec:seasonality_models} é dedicada
à uma pequena exploração desses métodos. Abordamos a seguir sua versão mais
simples, tipicamente implementada no processo de decomposição de séries
temporais.

Uma possibilidade para estimativa da componente sazonal é determinar o ``padrão
sazonal médio'' da série sem tendência. A série de tamanho $n$ sem tendência é
divida em $s$ segmentos de $m$ amostras, no qual $s = \frac{n}{m}$. Usando
livremente a vírgula como símbolo para concatenação podemos enumerar os
segmentos como $$\chi_1, \chi_2, ... \chi_s$$ onde um segmento arbitrário
$$\chi_i = \chi_{i_{1}}, \chi_{i_{1}} ... \chi_{i_{m}}$$

O padrão sazonal modelado $S'_t$ corresponde ao segmento médio, isso é,

$$ S'_t = \left(\frac{1}{s} \sum_{k=1}^{k=s} \chi_{1_{k}}\right), \left(\frac{1}{s} \sum_{k=1}^{k=s} \chi_{2_{k}}\right), \left(\frac{1}{s} \sum_{k=1}^{k=s} \chi_{3_{k}}\right) \hdots \left(\frac{1}{s} \sum_{k=1}^{k=s} \chi_{m_{k}}\right)$$

A componente sazonal $S_t$ é então dada pela concatenação de $m$ cópias de
$S'_t$

$$ S_t = \overbrace{S'_t, S'_t, \hdots, S'_t}^{s\text{ vezes}} $$

\subsection{Decomposição Clássica (Aditiva)}\label{ssec:classical_decomposition}

O algoritmo de decomposição clássica aditiva é simples e capaz de identificar
períodos arbitrários de sazonalidade. O procedimento é apresentado passo a
passo.

\subsubsection{Passo 1}

\textbf{Identificar o período de sazonalidade que se deseja modelar.}

O período $m$ de sazonalidade corresponde à duração de um ciclo de um padrão
periódico observável nos dados. A determinação da duração desse ciclo não é
necessariamente trivial, especialmente tratando de séries de alta frequência,
sendo interessante inclusive usar métodos automáticos para determinação da
distância entre amostras iniciais dos períodos de sazonalidade. Uma
possibilidade, por exemplo,  é analisar os picos da função de autocorrelação e
acusar a distância entre os picos como um período amostral.

\subsubsection{Passo 2}

\textbf{Modelar tendência }$\mathbf{T_t}$

A tendência $T_t$ é modelada por meio de algum dos métodos mencionados, por
exemplo, um filtro média móvel com tamanho de janela $w$.

\subsubsection{Passo 3}

\textbf{Remover tendência da série}

A componente sem tendência é dada por $y_t - T_t$. A série resultante deve
possuir média aproximadamente nula.

\subsubsection{Passo 4}

\textbf{Estimar componente sazonal}

A componente sazonal de $S_t$ é estimada por meio de algum dos métodos
mencionados. Para a maioria dos métodos precisamos informar o período sazonal
$m$.

\subsubsection{Passo 5}

\textbf{Calcular a componente residual}

Por fim calcula-se a componente residual $R_t$ como

$$ R_t = y_t - T_t - S_t $$

A série está enfim descomposta

$$ y_t = R_t + T_t + S_t $$

\subsection{Qualificação de tendência e sazonalidade}

A presença de tendência ou sazonalidade é tipicamente visível imediatamente.
Dúvida sobre a presença ou não dessas propriedades pode surgir se o sinal
estiver imerso em ruído. Se houver presença de raízes unitárias curtos
intervalos de tempo podem ser ambíguos quanto ao determinismo de sua tendência,
isso é, uma tendência estocástica pode ser confundida com tendência
determinística. Nesses cenários é interessante detectar a presença desses
componentes por meio de testes estatísticos ou análise de correlalograma.

\subsection{Quantificação de tendência e sazonalidade}

\subsubsection{Força}

A decomposição de uma série temporal em componentes isolados de tendência,
sazonalidade e resíduos permite que a intensidade de tendência e sazonalidade
sejam quantificáveis de forma elegante. Essa intensidade de tendência ou
sazonalidade é tipicamente chamada de força.

Para séries com forte tendência é esperado que a componente de tendência
contenha considerável variância. Uma forma interessante de quantificar a força
da tendência de uma série, proposta por Hyndman et. al.~\cite{athana}, parte
da observação da variância adicional introduzida pela adição do termo de
tendência ao residual:

$$ \frac{Var(R_t)}{Var(T_t + R_t)} $$

Espera-se que essa razão seja pequena para séries com alta tendência, isso é,
haja introdução de considerável variância pela adição da componente de
tendência no denominador. Podemos então definir a força $F_T$ da tendência de
uma série como

$$ F_T = max\left(0, 1 - \frac{Var(R_t)}{Var(T_t + R_t)}\right) $$

Observe que $F_T \in real$ limitado entre 0 e 1.

De forma identicamente análoga podemos definir a força da sazonalidade de uma
série como

$$ F_S = max\left(0, 1 - \frac{Var(R_t)}{Var(S_t + R_t)}\right) $$

Essas medidas são úteis ao oferecer uma interface quantitativa aos atributos
tipicamente qualitativos de tendência e sazonalidade. É importante deixar
claro que essas medidas necessitam que a série seja decomposta em suas
componentes de tendência e sazonalidade, possivelmente dificultando sua
aplicação.

\section{Modelos Estacionários}

\subsection{O Modelo Estacionário como um Filtro Linear}

Segundo Box e Jenkins~\cite[capítulo~1.2.1]{box} os modelos estacionários que
serão abordados nas seguinte seções foram idealizados por (YULE, 1927) como
filtros lineares que modelam a série temporal de interesse por meio do
processamento de ruído branco. A modelagem apropriada do sinal em questão se
resume então ao apropriado ajuste da função de transferência $\psi$ desse
filtro a partir dos dados observados, isso é, o apropriado posicionamento de
zeros e/ou polos.

A função de transferência $\psi$ é definida no domínio do tempo por meio do
operador de atraso $L$ ao invés de $z^{-1}$. Como no contexto de filtros
digitais podemos pensar em um plano $L$, em analogia com o plano $Z$, e chegar às
exatas mesmas conclusões sobre o efeito do posicionamento de polos e zeros na
estabilidade (inclusive marginal), invertibilidade e resposta em frequência do
filtro. A diferença relevante é que o operador de atraso no domínio $Z$,
$z^{-1}$, é definido de forma inversa à referência do plano. Essa inversão não
acontece no plano $L$, levando a uma inversão das propriedades conhecidas.

É possível também descrever certos tipos de modelos-filtros como \emph{Finite
Impulse Response}(FIR) ou \emph{Infinite Impulse Response}(IIR), dependendo
a equação de recorrência, levando às propriedades conhecidas desses tipos de
filtros.

\begin{figure}
    \centering
    \begin{tikzpicture}[node distance=4.5cm,
        every node/.style={fill=white, font=\sffamily}, align=center]

        \node (system)      [activityStarts]              {Sistema LTI\\$\psi(t)$};
        \node (input)       [process, left of=system]     {$\varepsilon_t$};
        \node (output)      [process, right of=system]    {$y_t$};

        \draw[->]     (input) -- (system);
        \draw[->]     (system) -- (output);

    \end{tikzpicture}
    \vspace{.6cm}
    \caption{Representação de série temporal como modelo linear generalizado}
    \label{fig:white_noise_LTI}
\end{figure}


A figura~\ref{fig:white_noise_LTI} torna clara a interpretação traçada. Para
tornar o paralelo mais claro podemos inicialmente pensar em um sinal de entrada
$\varepsilon[n]$ ( ruído branco) transformado por uma resposta ao impulso
$h[n]$, causal de tamanho $m$, no sinal $y[n]$ pela seguinte soma de convolução

$$ y[n] = \sum^{m}_{k=0} h[n-k]\varepsilon[k] =  \sum^{m}_{k=0} \varepsilon[n-k]h[k]$$

Introduzindo o operador de atraso $L$ no somatório

$$ y[n] = \sum^{m}_{k=0} L^{k}h[k] \varepsilon[n]$$

observamos que temos um polinômio em $L$ cujos coeficientes correspondem aos
da resposta ao impulso do filtro. Podemos então, chaveando para notação de
séries temporais, reescrever a convolução como

$$ \sum^{m}_{k=0} \psi(k)L^k\varepsilon_{t} = \sum^{m}_{k=0} \psi(k)\varepsilon_{t-k}$$

Expandindo as somas temos

$$ y_t = \varepsilon_t + \psi_1 \varepsilon_{t-1} + \psi_2 \varepsilon_{t-2} + ... +  \psi_m \varepsilon_{t-m}$$
$$ y_t = \varepsilon_t + \psi_1 L\varepsilon_{t} + \psi_2 L^2 \varepsilon_{t} + ... +  \psi_m L^m \varepsilon_{t}$$
$$ y_t = \varepsilon_t(1 + \psi_1 L + \psi_2 L^2 + ... + \psi L^m)$$

\begin{equation}\label{eq:fir}
    y_t = \psi(L)\varepsilon_t
\end{equation}

onde $\psi(L)$ é um polinômio mônico em $L$.

Tikz Here


A interpretação de modelos estacionários como filtros tem suas limitações no
que diz respeito às possibilidades de herança de conhecimento da área de
processamento de sinais. Inicialmente podemos constatar que filtros lineares no
sentido tradicional se propõem em alterar as propriedades de um sinal
arbitrário que por si já contém informação. Isso leva à importância imediata da
função de impulso unitário como base natural de sinais em tempo discreto e a
uma enorme preocupação com a fase da resposta ao impulso do filtro devido à
facilidade de distorção de fase indesejada do sinal processado. Para o
modelo-filtro a função de impulso unitário não é tão importante porque esse
filtro é sempre excitado pelo mesmo tipo de sinal que não possui nenhum tipo de
estrutura a se preservar: ruído branco. A estrutura de fase desse sinal é menos
relevante ainda por ser aleatória. Essas diferenças tornam muita da teoria de
filtros inaplicável no contexto de séries temporais. Não há sentido de projetar
um modelo filtro de fase linear generalizada, por exemplo, se não há o que
preservar na fase do sinal de entrada e nenhuma preocupação com atraso de grupo.

Não obstante a interpretação de modelos estacionários como filtros lineares
facilita a assimilação de propriedades desses modelos por meio da linguagem de
processamento de sinais e nos leva a interessantes explorações de parte da
teoria de filtros aplicada à modelagem de séries temporais.

Por fim podemos notar que nesse contexto o filtro tradicional representa uma
função linear do processo estocástico de ruído branco ${\epsilon(t)}$ para a
saída ${Y(t)}$, qualificando a saída em si como um processo estocástico.
Conforme discutido na seção~\ref{sec:process} abordaremos as propriedades de
realizações específicas do processo ${Y(t)}$.



\subsection{Modelo Linear Generalizado}
\label{sec:glm}

É interessante expandir a discussão anterior para a definição de um modelo
linear generalizado (GLM).

Tomando a equação~\ref{eq:fir} com $m \rightarrow \infty$ temos um modelo linear
generalizado que corresponde ao processamento de ruído branco por um filtro
linear com resposta ao impulso de duração infinita:

\begin{equation}\label{eq:glm}
    y_t = \psi(L)\varepsilon_t = \sum^{\infty}_{0} \psi_m \varepsilon_t
\end{equation}

onde $\psi(L) = 1 + L + L^2 + L^3 ...$

A literatura estatística frequentemente se refere ao vetor de ruído branco
$\varepsilon_t$ como ``choques'' ou ``inovações''. O teorema de Wold
~\cite{wold} estabelece que qualquer série estacionária tem uma representação
dada pela equação~\ref{eq:glm} tal que $\sum^{\infty}_{0} \psi_k^2 < \infty$. Esse
resultado é equivalente à constatação que qualquer sinal estacionário pode ser
representado pelo processamento de ruído branco por um filtro com resposta ao
impulso quadrado somável. Outro ponto de vista do teorema do Wold pode ser
obtido analisando a variância do GLM.

$$\sigma^2_{GLM} = E\left[\left(\sum^{\infty}_{k=0} \psi_k \varepsilon_{t-k}\right)^2\right] = E\left[\sum^{\infty}_{k=0} \psi_k^2 \varepsilon_{t-k}^2\right]$$
$$\sigma^2_{GLM} = \sum^{\infty}_{k=0} \psi_k^2 E[\varepsilon_{t-k}^2]$$
$$\sigma^2_{GLM} = \sigma^2_{\varepsilon_t} \sum^{\infty}_{k=0} \psi_k^2 $$

Que implica a necessidade de finitude de $\sigma^2_{GLM}$. Construindo um
vetor $\mathbf{\psi}$ formado pelos coeficientes temos a variância representada
pelo quadrado da norma desse vetor $\norm{\psi}^2 = \sigma^2_{GLM}$.

\subsubsection{Autocorrelação}

A fim de definir a função de $\rho_{GLM}(\tau)$ de autocorrelação de um modelo
linear generalizado definimos inicialmente sua covariância

$$\gamma_{GLM}(\tau) = E\left[\left(\sum^{\infty}_{k=0} \psi_k \varepsilon_{t-k}\right)\left(\sum^{\infty}_{i=0} \psi_i \varepsilon_{t-i-\tau}\right)  \right] $$
$$\gamma_{GLM}(\tau) = E\left[\sum^{\infty}_{k=0} \sum^{\infty}_{i=0} \psi_k \varepsilon_{t-k} \psi_i \varepsilon_{t-i-\tau}\right] $$

onde temos que a esperança entre quaisquer $\varepsilon_t-a$ e $\varepsilon_t-b$
nula exceto para $a = b$, já que por definição $\varepsilon_t$ é composto de
variáveis aleatórias independentes. Estamos então interessados nos casos em que
$t-k = t-i-\tau$ tal que $i = k - \tau$. Substituindo as variáveis temos

$$\gamma_{GLM}(\tau) = E\left[\sum^{\infty}_{k=0} \psi_{k} \varepsilon_{t-k} \psi_{k-\tau} \varepsilon_{t-k}\right] $$
$$\gamma_{GLM}(\tau) = E\left[\sum^{\infty}_{k=0} \psi_{k}\psi_{k-\tau} \varepsilon_{t-k}^2\right] = \sigma^2_{\varepsilon_t}\sum^{\infty}_{k=0}\psi_{k}\psi_{k-\tau} $$

A autocorrelação é então dada por

$$ \rho_{GLM}(\tau) = \sum^{\infty}_{k=0}\psi_{k}\psi_{k-\tau} $$

\subsection{Modelo Média Móvel}

Um processo ${\mathbf{Y}(t, q)}$ é considerado de média móvel de ordem $q$ se uma
realização $y_t$ for definida pela equação~\ref{eq:ma(q)}, na qual
$\varepsilon_t$ representa a realização de um processo puramente aleatório. O
processo é tipicamente chamado de $MA(q)$.

\begin{equation}\label{eq:ma(q)}
    y_t = \varepsilon_t + \sum_{i=1}^{i=q} \beta_i \varepsilon_{t-i}
\end{equation}

Pela definição acima observamos que o modelo média móvel corresponde a um
filtro FIR excitado por ruído branco.

Um processo de média móvel, como sugerido pelo nome, é análogo a uma média
móvel de de observações anteriores de uma série temporal de ruído branco. Não
é de fato uma média móvel porque os coeficientes não necessariamente se somam a
um.

Podemos escrever a equação~\ref{eq:ma(q)} usando o operador de atraso por meio
da equação~\ref{eq:ma(q)_L}. Nessa equação observamos claramente como o modelo
$MA(q)$ é um caso particular do $GLM$.

\begin{equation}\label{eq:ma(q)_L}
    y_t = \varepsilon_t(1 + \sum_{i=1}^{i=q} \beta_{t-i} L^{i}) = \varepsilon_t \phi(L)
\end{equation}

A figura~\ref{fig:ma_time} ilustra o comportamento temporal de modelos $MA$ para
diferentes ordens.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/ma_time.png}
    \caption{Visualização no tempo de processos média móvel de ordens
    diferentes.}
    \label{fig:ma_time}
\end{figure}

Observamos que não temos clara identidade visual dos processos de média móvel
representados devido à natureza estocástica do sinal.

\subsubsection{Autocorrelação}

Definimos inicialmente a função de autocorrelação para o processo média móvel

$$\gamma_{MA(q)}(\tau) = E\left[\left(\varepsilon_t + \sum^{m}_{k=1} \beta_k \varepsilon_{t-k}\right)\left( \varepsilon_{t-\tau} + \sum^{m}_{i=1} \beta_i \varepsilon_{t-i-\tau}\right) \right] $$

observando que para $\tau > q$ temos $\gamma_{MA(q)}(\tau) = 0$, concluímos de
forma análoga à dedução da autocorrelação do processo linear generalizado a
seguinte forma

TODO: make case

$$\gamma_{MA(q)}(\tau) = \sigma_{\varepsilon_t}^2 \sum^{m}_{k=1} \beta_k\beta_{k-\tau} $$

A variância é dada por $\gamma(0)$ tal que

$$\sigma^{2}_{MA(q)} = \sigma_{\varepsilon_t}^2 \sum^{m}_{k=1} \beta{k}^2 $$

Se definirmos o vetor $\mathbf{\theta}$ a partir dos coeficientes do polinômio
$\theta(L)$ temos $\sigma^{2}_{MA(q)} = \sigma_{\varepsilon_t}^2\norm{\mathbf{\theta}}^2$.

Temos autocorrelação definida então como

$$\rho_{MA(q)}(\tau) = \frac{\sum^{m}_{k=1} \beta_k\beta_{k-\tau}}{\norm{\mathbf{\theta}}}$$

A função de autocorrelação de um processo média móvel exibe a interessante
propriedade de ``cortar'' após o atraso $q$, isso é, demonstrar autocorrelação
igual a zero após um atraso de número correspondente à ordem do processo. A
autocorrelação amostral de uma série temporal gerada por um processo de média
móvel tende a apresentar a mesma propriedade, apesar de ser perfeitamente
possível da autocorrelação amostral de um processo $MA(q)$ cair para zero
\emph{antes} do lag $q$~\cite{chatfield}.

A imagem~\ref{fig:ma_corr} demonstra a visualização dos correlalogramas
correspondentes aos modelos ilustrados no tempo pela figura~\ref{fig:ma_time}.
Observe que as autocorrelações são distintas de zero apenas para atrasos
iguais ou inferiores à ordem $q$ do processo.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/ma_corr.png}
    \caption{Visualização do correlalograma de processos média móvel de ordens
    diferentes.}
    \label{fig:ma_corr}
\end{figure}

Essa propriedade da função de autocorrelação possui consequências diretas para
modelagem de séries temporais: é possível que uma série temporal estacionária
com autocorrelações iguais a zero após atraso $k$ seja satisfatoriamente
aproximada por um modelo $MA(k)$. A função de autocorrelação amostral é usada
então como ferramenta de identificação da aplicabilidade e ordem de modelos
$MA(q)$.

\subsection{Modelo Autoregressivo}
\label{ssec:AR(p)}

Um processo ${\mathbf{Y}(t, p)}$ é considerado autoregressivo de ordem $p$ se
uma realização $y_t$ ele for definido pela equação~\ref{eq:ar(p)}, onde
$\varepsilon_t$ representa um processo puramente aleatório. O processo é
frequentemente chamado de $AR(p)$.

\begin{equation}\label{eq:ar(p)}
   y_t = \sum^{i=p}_{i=1} y_{t-i} \alpha_i + \varepsilon_t
\end{equation}

Um processo autoregressivo de ordem $p$ é caracterizado por uma dependência
entre uma amostra de instante de tempo $t$ e as amostras de instantes de tempo
$t-1$, $t-2$, ..., $t-p$. Como sugerido pelo nome a equação de diferenças
estabelece uma relação de regressão entre uma série temporal e suas versões
atrasadas no tempo.

Podemos reescrever a equação~\ref{eq:ar(p)} por meio do operador de atraso
resultando na equação.~\ref{eq:ar(p)_L}. O polinômio $\alpha(L)$ é chamado de
polinômio autoregressivo ou equação característica.

\begin{equation}\label{eq:ar(p)_L}
    y_t = \frac{\varepsilon_t}{1 - \sum_{i=1}^{i=p} L^i \alpha_i}
\end{equation}

Como exemplo temos que um processo autoregressivo de segunda ordem, isso é,
$AR(1)$, é definido pela expressão a seguir.

\begin{equation}\label{eq:ar_1}
    y_t = \frac{\varepsilon_t}{(1 - L\alpha_1)}
\end{equation}

O polinômio de operadores de atraso $1 - L\alpha_1$ recebe o nome de equação
característica. Ao expandir essa equação por divisão polinomial encontramos o
conhecido desenvolvimento de um filtro IIR.

\begin{equation}\label{eq:iir}
    y_t = \sum^{\infty}_{k=0} \alpha^k \varepsilon_{t-k}
\end{equation}

Observamos primeiramente que o parâmetro $\alpha$ deve ser tal que a soma do
lado direito da equação~\ref{eq:iir} seja convergente. Sabemos que essa
condição corresponde à estabilidade do filtro e veremos que corresponde também
à estacionariedade do sinal produzido pela filtragem. Concluímos que um sinal
produzido por um processo autoregressivo estacionário pode ser modelado pelo
processamento de ruído branco por um filtro IIR estável. Essa forma da
equação autoregressiva é claramente um caso específico do GLM.

A divisão polinomial entre a equação~\ref{eq:ar_1} e~\ref{eq:iir} é generalizada
como uma inversão do polinômio $\alpha(L)$, de forma que

$$ y_t\alpha(L) = \varepsilon_t = y_t = \alpha^{-1}(L)\varepsilon_t$$

$\alpha^{-1}(L)$ é bem definido sobre certas condições que serão discutidas na
seção~\ref{ssec:stability_invertibility}.

Uma visualização do comportamento temporal de processos autoregressivos é dada
pela figura~\ref{fig:ar_time_visualization}. Observa-se que as séries temporais
não exibem comportamento visivelmente distinguível no domínio do tempo, como
no caso das séries $MA(q)$.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.65]{figures/ar_time.png}
    \caption{Visualização de processos autoregressivos de diferentes ordens
    no domínio do tempo.}
    \label{fig:ar_time_visualization}
\end{figure}

\subsubsection{Autocorrelação}

Assumindo estacionariedade e multiplicando ambos os lados da
equação~\ref{eq:ar(p)} por $y_{t-\tau}$ podemos então calcular a esperança da
expressão para obter a autocovariância do processo para $\tau \neq 0$

$$ \gamma_{AR(p)}(\tau) = E\left[\left(\sum^{i=p}_{k=1} y_{t-k}y_{t-\tau}\alpha_k\right) + (y_{t-\tau}\varepsilon_t)\right] $$
$$ \gamma_{AR(p)}(\tau) = E\left[\left(\sum^{i=p}_{k=1} y_{t-k}y_{t-\tau}\alpha_k\right)\right] $$
$$ \gamma_{AR(p)}(\tau) = \sum^{i=p}_{k=1} \alpha_k \gamma_{AR(p)}(\tau - k) $$

Equação que igualmente satisfaz a autocorrelação $\rho(\tau)$ pela divisão de
cada um dos termos acima por $\gamma_{AR(P)}(0) = \sigma_{AR(p)}^2$.

\begin{equation}\label{eq:ar_autocorr}
    \rho_{AR(p)}(\tau) = \sum^{i=p}_{k=1} \alpha_k \rho_{AR(p)}(\tau - k), \tau > 0
\end{equation}

Obtemos então uma autoregressão de ordem $p$ nas autocovariâncas também. Para
calcular a variância do processo multiplicamos os termos da equação~\ref{eq:ar(p)}
por $y_{t}$

$$ \sigma_{AR(p)}^2 = \sigma_{\varepsilon_t}^2 + \sum^{i=p}_{k=1} \gamma_{AR(p)}(k)$$
$$ \sigma_{AR(p)}^2 = \sigma_{\varepsilon_t}^2 + \sigma_{AR(p)}^2\sum^{i=p}_{k=1} \rho_{AR(p)}(k)$$
$$ \sigma_{AR(p)}^2 - \left(\sigma_{AR(p)}^2\sum^{i=p}_{k=1} \rho_{AR(p)}(k)\right) = \sigma_{\varepsilon_t}^2 $$
$$ \sigma_{AR(p)}^2 = \frac{\sigma_{\varepsilon_t}^2}{1 - \sum^{i=p}_{k=1} \rho_{AR(p)}(k)}$$

A variância é também uma função autoregressiva da autocorrelação.

Podemos reescrever a equação~\ref{eq:ar_autocorr} em função do polinômio de
atraso (operando agora sobre $\tau$ ao invés de $t$) como

\begin{equation}\label{eq:ar_rho}
    \alpha(L)\rho_{AR(p)}(\tau) = 0
\end{equation}


Expressando o polinômio em função de suas raízes $G_i$ temos que

$$\alpha(L) = \prod_{k=1}^{p} (1 - G_i L)$$

É demonstrado em~\cite[capítulo~4]{box} que a solução geral para a equação de
~\ref{eq:ar_rho} é dada por

\begin{equation}\label{eq:rho_solution}
    \rho_{AR(p)}(\tau) = \sum_{k=1}^{p} A_k G^{\tau}_k
\end{equation}

Essa é a expressão conclusiva da autocorrelação de um processo autoregressivo
de ordem $p$.

Denotemos as raízes reais de $\alpha(L)$ por $R_h$ e as complexas conjugadas
por $C_j, C_k$. Observamos que sob a forma~\ref{eq:rho_solution} as raízes
reais $R_h$, assumindo que $|R_h| < 1$, contribuem para a autocorrelação como
exponenciais amortecidas $A_h^{\tau} R_h$. Raízes complexas conjugadas $C_j,
C_k$ contribuem com UM SENOIDE AMORTECIDO. DEDUZIR COMO!
TODO

$$ A_j^{\tau}C_j + A_k^{\tau}C_k $$

Concluímos então que a autocorrelação de um processo $AR(p)$ é uma combinação
de $N$ exponenciais e senóides amortecidos tal que $N \leq p$.

Para o caso específico de um processo estacionário $AR(1)$ temos pela
equação~\ref{eq:rho_solution} a seguinte relação de autocorrelação

$$ \rho_{AR(1)}(\tau) = \alpha \rho_{AR(1)}(\tau - 1)$$

$$ \rho_{AR(1)}(\tau) = \alpha^{\tau}$$

TODO: add AR figures here !!

\subsubsection{Equações de Yule Walker}

É possível estimar os coeficientes autoregressivos $\alpha$ desenvolvendo as
equações~\ref{eq:ar_rho}. Com $\rho(-k) = \rho(k)$ e $\rho(0) = 1$ temos

$$\rho(1) = \alpha_{1} + \alpha_{2}\rho(1) + \alpha_{3}\rho(2) ... + \alpha{p}\rho(p-1)$$
$$\rho(2) = \alpha_{1}\rho(1) + \alpha_{1}\rho(1) + \alpha_{2}\rho(2) ... + \alpha{p}\rho(p)$$
$$\vdots$$
$$\rho(p) = \alpha_{1}\rho(p-1) + \alpha_{1}\rho(-1) + \alpha_{2}\rho(-2) ... + \alpha{p}$$

Em forma matricial

\begin{gather}\label{eq:yule_walker}
  \begin{bmatrix} \rho(1) \\ \rho(2) \\ \vdots \\ \rho(p) \end{bmatrix}
  =
  \begin{bmatrix}
      1 & \rho(1) & ... & \rho(p-1) \\
      \rho(1) & 1 & ... & \rho(p) \\
      \vdots & \vdots & \ddots & \vdots \\
      \rho(p-1) & \rho(p-2) & ... & 1 \\
  \end{bmatrix}
  \begin{bmatrix} \alpha_{1} \\ \alpha_{2} \\ \vdots \\ \alpha_{p} \end{bmatrix}
\end{gather}

As equações~\ref{eq:yule_walker} são conhecidas como equações de Yule Walker,
que permitem uma estimativa de coeficientes autoregressivos por meio de uma
estimativa de coeficientes de autocorrelação. A forma mais intuitiva de estimar
o vetor $\mathbf{\alpha}$ é por meio da inversão da matriz de autocorrelações
defasadas, mas outros algoritmos como o de Durbin-Levinson podem ser utilizados.

\subsubsection{Autocorrelação Parcial}

Como mencionado na seção~\ref{ssec:partial_acorr} autocorrelação parcial
informa a correlação restante entre $y_t$ e $y_{t-\tau}$ após levar em
conta a contribuição resultante dos termos intermediários $y_{t-1}, y_{t-2}
... y_{t-\tau+1}$.

Autocorrelação parcial é formalmente definida para um processo autoregressivo
partindo da equação~\ref{eq:ar_rho}, onde tomando $p=1$ para um processo
autoregressivo de ordem 1 temos que

$$ \rho(\tau) = \alpha_1 \rho(\tau - 1) $$

Tomando $\tau = p = 1$ e adicionando um subscrito adicional em $\alpha$ para
indicar a ordem do processo autoregressivo temos

$$ \alpha_{1_{1}} = \rho(1) $$

Para $p=2$ temos

$$ \rho(\tau) = \alpha_1 \rho(\tau - 1) + \alpha_2 \rho(\tau - 2) $$

Novamente tomando $\tau = p = 2$ obtemos

$$ \rho(2) = \alpha_{1_{2}} \rho(1) + \alpha_{2_{2}} \rho(0) $$

Estamos como no caso de $p=1$ interessados no valor de $\alpha_{2_{2}}$, isso é,
$\alpha_{\tau_{\tau}}$ com $\tau=2$. Uma solução para esse valor em função
das autocorrelações $\rho$, obtida por meio das equações de Yule Walker, é

$$ \alpha_{2_{2}} = \frac{\rho(2) - \rho^2(1)}{1 - \rho^2(1)} $$

As autocorrelações parciais em função de $\tau$ são então dadas por sucessivos
$\alpha_{\tau_{\tau}}$ para $\tau = 1, 2, 3 ...$, isso é, o último coeficiente
autoregressivo $\alpha_p$ de processos autoregressivos de ordem $p$ crescentes.

A solução desses valores em função das autocorrelações de cada processo de
ordem crescente, dada por~\cite{morettin}, é generalizada como

$$ \alpha_{\tau_{\tau}} = \frac{\norm{\mathbf{P}^*_{\tau}}}{\norm{\mathbf{P}_{\tau}}} $$

Onde a matriz $\mathbf{\mathbf{P}}$ é a matriz de autocorrelações de ordem $p =
\tau$ como definida na equação~\ref{eq:yule_walker} e a matriz
$\mathbf{P}^*$ é obtida pela substituição da última coluna de
$\mathbf{P}$ pelo vetor de autocorrelações $\mathbf{\rho}$.

Essa definição de autocorrelação parcial é bem definida para processos
autoregressivos e para esse tipo de processo seu valor claramente se torna
igual a zero a partir do atraso $\tau = p$.

A autocorrelação parcial amostral é calculada da mesma forma partindo da
autocorrelação amostral como definida pela equação~\ref{eq:autocorr}. A
autocorrelação parcial amostral por sua vez é definida para séries temporais
arbitrárias. Nesse caso sucessivos modelos autoregressivos de ordem $p = \tau =
1, 2, 3..$ são ajustados à série em questão e o último coeficiente
de cada regressão é armazenado como a autocorrelação parcial amostral para o
atraso $\tau$.

Como estamos interessados em trabalhar com séries temporais a definição acima
de autocorrelação parcial amostral é o suficiente para generalizar o conceito
definido sobre processos autoregressivos para realizações únicas (séries
temporais) de processos estocásticos arbitrários.

\subsection{ARMA}
\label{ssec:ARMA}

Processos ARMA, como sugerido pelo nome, são gerados pela sobreposição de
processos AR e MA. Um processo ${\mathbf{Y}}(t)$ é considerado $ARMA(p, q)$ se
for dado pela equação~\ref{eq:arma}

\begin{equation}\label{eq:arma}
    y_t = \varepsilon_t + \sum_{i=1}^{i=q} \beta_i \varepsilon_{t-i} + \sum_{i=1}^{i=p} y_{t-i}\alpha_i
\end{equation}

Um processo $ARMA(2, 1)$ é portanto dado pela seguinte equação de recorrência.

$$ y_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \varepsilon_{t} + \beta_1 \varepsilon_{t-1} $$

Em analogia aos casos anteriores o processo pode ser escrito por meio do
operador de atraso.

$$ y_t (1 - \alpha_1 L - \alpha_2 L^2) = \varepsilon_{t} (\beta_0 + \beta_1 L) $$

Podemos introduzir $\phi(L) = 1 - \alpha_1 L - \alpha_2 L$ e $\theta(L) =
\beta_0 + \beta_1 L$ tal que:

\begin{equation}\label{eq:arma_l}
    y_t = \varepsilon_t \frac{\phi(L)}{\theta(L)}
\end{equation}

A equação~\ref{eq:arma_l} é prontamente generalizada para corresponder à
equação~\ref{eq:arma} estabelecendo:

$$\phi(L) = 1 - \alpha_1 L - \alpha_2 L^2 \hdots - \alpha_p L^p$$

$$\theta(L) = 1 - \beta_1 L^1 \hdots - \beta_q L^q$$


Observamos que a forma da equação~\ref{eq:arma_l} é idêntica à de uma função de
transferência, sendo definida no domínio do tempo sobre polinômios de $L$ ao
invés de no domínio $Z$ sobre polinômios em $z$. As implicações do
posicionamento das raízes dos polinômios numerador e denominador decorrem de
maneira análoga.

Uma propriedade interessante de modelos ARMA é que processos estacionários
frequentemente podem ser modelados por modelos ARMA com menos parâmetros que
modelos AR ou MA.

\subsection{Estabilidade e Invertibilidade}
\label{ssec:stability_invertibility}

\section{Raízes Unitárias}
\label{sec:unit_roots}

Como um processo ARMA é essencialmente resultante do processamento de ruído
branco por um filtro linear com função de transferência
$\frac{\theta(L)}{\phi(L)}$ sabemos que a posição das raízes dos polinômios
$\theta(L)$ e $\phi(L)$ determina suas propriedades.

O posicionamento de raízes no círculo unitário é um caso interessante de
analisar. Se algum dos polos função de transferência de um processo ARMA
estiverem posicionados no círculo unitário diz-se que esse processo possui
raízes unitárias. Esse termo tipicamente descreve o posicionamento dos polos e
não dos zeros pelo maior efeito dessas raízes na dinâmica do sistema, mas essa
seção inclui uma breve discussão sobre o efeito de zeros unitários também.

Antes de prosseguir um processo $y_t$ $ARMA(2, 1)$ será estabelecido para
exemplificar as seguintes discussões.

Diante da recorrência

$$ y_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \varepsilon_t + \beta_1 \varepsilon_{t-1}$$

temos

$$ y_t = \frac{1 + \beta_1L}{1 - \alpha_1 L - \alpha_2 L^2} \varepsilon_t $$

Uma realização desse processo para os parâmetros

$$\alpha_1 = -0.5, \alpha_2 = 0.25, \beta_1 = 0.8$$

é ilustrada pela figura~\ref{fig:ARMA2-1} e seu diagrama de polos e zeros
no plano $L$ pela figura~\ref{fig:ARMA2-1pzp}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/arma_21.png}
    \caption{Visualização de realização de processo ARMA(2, 1) no tempo}
    \label{fig:ARMA2-1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/arma_21_pzp.png}
    \caption{Visualização de posicionamento de polos e zeros de processo
    ARMA(2, 1) em relação ao
    círculo unitário no plano L}
    \label{fig:ARMA2-1-pzp}
\end{figure}

\subsection{Raízes Unitárias no Polinômio Autoregressivo}

Inspecionemos qualitativamente a inserção presença de uma raiz unitária no
polinômio autoregressivo do processo ARMA(2, 1) apresentado. Tomando $z_t$
como o processo

$$ z_t = \frac{y_t}{(1 - L)} = \frac{1 + 0.8L}{(1 + 0.5 L - 0.25 L^2)(1 - L)} \varepsilon_t $$

Temos a seguinte relação entre $z_t$ e $y_t$

$$ (1 - L)z_t = y_t $$
$$ z_t - z_{t-1} = y_t $$
$$ \nabla z_t = y_t $$

Tal que $z_t$ represente uma integração de $y_t$.

$$ z_t = \nabla^{-1} y_t $$

Dizemos que nesse caso o processo $z_t$ é integrado de ordem um, ou $I(1)$, já
que o diferenciando uma vez temos em um processo estacionário. Observamos
imediatamente que o processo $z_t$ não é estacionário.

Podemos definir um processo integrado de ordem $d$, $I(d)$, como um processo
cuja diferenciação em $d$ vezes resulta em estacionariedade. Tal processo seria
gerado a partir de $y_t$ por meio de $d$ integrações. Essa definição implica
corretamente que um processo $I(d)$, $d > 0$, não é estacionário, já que a
condição de estacionariadade para processos ARMA é pontualmente violada.

Analisando uma realização do processo $z_t$, por meio da
figura~\ref{fig:ARMA-2-1-integrated}, observamos um típico processo de raiz
unitária, cuja não estacionariedade não é tão óbvia quanto nos casos de um
filtro ARMA instável ou presença de tendência determinística. Processos com
raíz unitária são diferença estacionários e possuem tendência determinística,
como discutido na seção~\ref{ssec:taxonomy}. Como esperado o diagrama de
polos e zeros desse modelo inclui um polo adicional correspondente à raiz
unitária.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/arma_21_integrated.png}
    \caption{Visualização de realização de processo ARMA(2, 1) com introdução
    de raiz unitária no tempo}
    \label{fig:ARMA2-1-integrated}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/arma_21_integrated_pzp.png}
    \caption{Visualização de posicionamento de polos e zeros de processo
    ARMA(2, 1) com introdução de raiz unitária em relação ao
    círculo unitário no plano L}
    \label{fig:ARMA2-1-integrated-pzp}
\end{figure}

Processos de raízes unitárias são especialmente interessantes por sua
capacidade de representar fenômenos do mundo real de forma eficiente. Logo
serão reconhecimentos como processos da classe ARIMA.

No processamento de sinais determinísticos sistemas lineares com raiz unitária
levam ao caso de estabilidade marginal, onde a resposta ao impulso do sistema é
um sinal de potência ao invés de um sinal de energia. A ideia de uma resposta
ao impulso de energia infinita sob presença de polos integradores manifesta-se
no contexto de sinais aleatórios quando é dito que processos de tendência
estocástica (de estacionariedade diferenciável, como discutido na seção
~\ref{ssec:taxonomy}) são afetados irreversivelmente por eventos perturbadores.
A ``lembrança'' desses eventos é justamente a resposta de duração infinita do
evento perturbador, apropriadamente modelado por funções impulso ou degrau.
Essa interpretação é importante no campo de análise de intervenção, que procura
incorporar efeitos de eventos perturbadores reais em modelos de séries
temporais.

\subsection{Raízes Unitárias no Polinômio Média Móvel}\label{ssec:ma_roots}

A presença de uma raiz unitária no polinômio de média móvel de um filtro ARMA
gera efeitos menos dramáticos em sua dinâmica. Antes de tudo nota-se que a
introdução de raízes unitárias de médias móveis tornam um sistema não
inversível, já que sua inversão  tornaria essa raiz um polo.

As figuras~\ref{fig:ARMA2-1-diff} e~\ref{fig:ARMA2-1-diff-pzp} ilustram uma
realização do processo $y_t$ com adição de raiz unitária de média móvel e o
diagrama de polos e zeros resultante, respectivamente.

TODO: check this @ later time !!

Observamos que a série com raiz de média móvel adicional aparenta ter uma
distribuição de potência mais enviesada para altas frequências, de forma
recíproca ao de raiz unitária autoregressiva, em que há introdução de
componentes de baixa frequência. Isso será discutido apropriadamente no
capítulo ~\ref{chap:spectral_analysis}.


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/arma_21_diff.png}
    \caption{Visualização de realização de processo ARMA(2, 1) derivado
    no tempo}
    \label{fig:ARMA2-1-diff}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/arma_21_diff_pzp.png}
    \caption{Visualização de posicionamento de polos e zeros de processo
    ARMA(2, 1) derivado em relação ao círculo unitário no plano L}
    \label{fig:ARMA2-1-diff-pzp}
\end{figure}

Tomando o processo $w_t$ como

$$ w_t = (1 - L)y_t = \frac{1 - 0.2L + L^2}{1 + 0.5L - 0.25L^2}\varepsilon_t$$

Temos $w_t = y_t - y_{t-1} = \nabla y_t$ tal que $w_t$ seja a derivada de
$y_t$. Vemos que ato de diferenciar uma série, em uma tentativa de introduzir
estacionariedade por exemplo, corresponde à introdução de uma raiz unitária no
polinômio de média móvel. Conseguimos dessa forma compreender a indução de
estacionariedade em uma série não estacionária com raiz unitária autoregressiva
por meio de sua derivação como a sobreposição de $d$ zeros aos $d$ polos
integradores do processo original. Isso é ilustrado por uma comparação das
figuras~\ref{fig:ARMA2-1-integrated-pzp} e~\ref{fig:ARMA2-1-diff-pzp}, onde
vemos que a diferenciação da série $z_t$ introduziria um zero unitário de forma
a ``cancelar'' seu polo unitário.

A presença de raízes unitárias de média móvel podem indicar que a série foi
diferenciada mais vezes que necessário ou que diferenciação é a operação
inadequada para indução de estacionariedade, isso é, a série não é diferença
estacionária.

Um exemplo é a série tendência estacionária $m_t$ a seguir:

$$ m_t = \mu + \eta t + \varepsilon_t $$

$$ \nabla m_t = \varepsilon_t - \varepsilon_{t-1} + \eta $$
$$ \nabla m_t = (1 - L)\varepsilon_t + \eta $$

Em $\nabla m_t$ temos uma raiz unitária no polinômio autoregressivo, resultando
em uma série não inversível. A tendência determinística poderia ter sido
removida por regressão resultando em um processo estacionário e inversível.


\section{Modelo ARIMA}
\label{sec:ARIMA}

Também chamados de modelos ARMA Integrados modelos ARIMA são essencialmente
modelos ARMA com tentativas de remoção de não estacionariedade. Assumindo que a
série em questão seja diferença-estacionária  o modelo ARIMA induz
estacionariedade ajustando um modelo ARMA com raízes unitárias adicionais. A
expectativa é que o processo original contenha $d$ polos unitários que serão
então sobrepostos por $d$ zeros unitários, resultando em processo estacionário.

A distinção entre o modelo ARMA e ARIMA é a substituição de $y_t$ por $\nabla^d
y_t$.

Definindo $w_t = \nabla^d y_t$ para $d = 0, 1, 2, ...$ temos a definição de um
processo $ARIMA(p, d, q)$ dada pela equação~\ref{eq:arima}. Observe que a
equação é idêntica à equação~\ref{eq:arma} com $w_t$ ao invés de $y_t$ e que a
saída é $w_t$, demandando uma integração para previsão de $y_t$. O nome do
modelo referencia essa operação de reconstrução de $y_t$.

\begin{equation}\label{eq:arima}
    w_t = \varepsilon_t  + \sum_{i=1}^{i=q} \beta_i \varepsilon_{t-i} + \sum_{i=1}^{i=p} w_{t-i}\alpha_i
\end{equation}

Podemos escrever um modelo $ARIMA(p, d, q)$ por meio do operador de atrasos
por meio da equação~\ref{eq:arima_l}, onde:

$$\phi(L) = 1 - \alpha_1 L - \alpha_2 L^2 \hdots - \alpha_p L^p$$

$$\theta(L) = 1 - \beta_0 - \beta_1 L^1 \hdots - \beta_q L^q$$

Nessa forma o paralelo entre modelos ARMA e ARIMA se torna mais claro.

\begin{equation}\label{eq:arima_l}
    y_t = \varepsilon_t \frac{\theta(L)}{\phi(L)} \frac{1}{(1-L)^d}
\end{equation}

O modelo ARIMA segue a metologia Box-Jenkins de modelagem, assumindo
diferença-estacionariedade e tentando induzir estacionariedade por meio de
sucessivas operações de diferenças. Pela discussão acima é claro que um modelo
$ARIMA(p, 0, q)$ corresponde a um modelo $ARMA(p, q)$.

O leitar agora reconhece o sistema da figura~\ref{fig:ARMA2-1-integrated-pzp}
como ARIMA(2, 1, 1).

\section{Modelos de Sazonalidade}\label{sec:seasonality_models}

\subsection{Diferenciação Sazonal}

O operador de diferença sazonal $\nabla_s$ estende o operador de diferenças
para subtração entre elementos não adjacentes e age sob uma série temporal
$\mathbf{y}_t$ da seguinte forma:

$$ \nabla \mathbf{y}_t = (1 - L^s)\mathbf{y}_t = \mathbf{y}_t - L^s\mathbf{y}_t = \mathbf{y}_t - \mathbf{y}_{t-s} $$

O operador mantém as propriedades da diferença simples e pode ser empregado em
ordens elevadas da mesma forma:

$$\nabla_4^2 y_t = \nabla_4 \nabla_4 y_t = \nabla_4 (y_t - y_{t-4}) = y_t - 2y_{t-4} + y_{t-8} $$

\subsection{SARIMA}\label{ssec:SARIMA}

O modelo ARIMA pode ser estendido de forma natural para acomodar padrões de
sazonalidade. O modelo SARIMA (Seasonal ARIMA) parte da observação que saídas
de sinais com alta sazonalidade podem possuem alta correlação com saídas
anteriores em uma distância corresponde ao padrão de sazonalidade, como
constatado no capítulo 1.

O modelo SARIMA então estabelece, em adição à operação de diferenciação do
modelo ARIMA, uma operação de diferenciação sazonal ao definir a componente
$w_t$ da seguinte forma:

$$ w_t = \nabla^D_{s}\nabla^d  y_t $$

Como na definição de $w_t$ para o modelo ARIMA temos uma diferenciação
tradicional de ordem $d$ seguida de uma diferenciação sazonal de período $s$
e ordem $D$.

Como exemplo um $w_t$ definido em função de $d=1$, $D=1$ e $s=24$ assume a
seguinte forma.

$$ w_t =  \nabla^1_{12} \nabla^1 y_t = \nabla^1_{2} (y_t - y_{t-1}) $$
$$ w_t =  (y_t - y_{t-24}) - (y_{t-1} - y_{t-25}) $$

Em seguida componentes autoregressivos e de média móvel são introduzidos com
atrasos em múltiplos de $s$. O modelo final é descrito como $SARIMA(p, d, q)
(P, D, Q)_{s}$, com $P$ e $D$ referenciando os componentes $AR$ e $MA$
sazonais. Para ilustrar melhor essa notação podemos analisar a expressão de um
modelo específico para depois generalizar.

Um modelo $SARIMA(1, 0, 1)(2, 1, 1)_{12}$ é dado pela seguinte expressão.

$$ w_t = \varepsilon_t + \overbrace{\alpha_1 w_{t-1}}^{p = 1} + \overbrace{
\alpha_2 w_{t-12}}^{P=1} + \overbrace{\alpha_3 w_{t-24}}^{P=2} +
\overbrace{\beta_1 \varepsilon_{t-1}}^{q=1} + \overbrace{\beta_2
\varepsilon_{t-12}}^{Q=1}$$

A escrita desse modelo por meio do operador de atraso é mais clara.

$$  w_t = \varepsilon_t \frac{\beta_1 L^1 + \beta_2 L^{12}}{(1 - \alpha_1 L^1 + \alpha_2 L^{12} + \alpha_3 L^{24})}  $$

Podemos então generalizar o modelo na forma do operador de atraso por meio da
equação~\ref{eq:sarima}.

\begin{equation}\label{eq:sarima}
    w_t = \varepsilon_{t} \frac{\theta_q(L)\theta_Q(L)}{\phi_p (L) \phi_P (L^s)}
\end{equation}

Com os polinômios de atraso definidos como nos casos anteriores.

A diferenciação sazonal inicial do modelo SARIMA é um dos exemplos mais simples
de um passo essencial no processamento de séries com padrões sazonais: a
modelagem do padrão de sazonalidade em si, que de fato é um campo próprio com
livros como~\cite{x11}. Diferenciação sazonal é limitada em sua capacidade de
expressar múltiplos padrões de sazonalidade. Outra forma mais flexível de
modelagem de sazonalidade será abordada na subseção seguinte.

\subsection{Variáveis de Fourier}

Versões sazonais de modelos ARIMA, como SARIMA, são pouco eficientes para
modelar padrões sazonais com as seguintes características:

\begin{itemize}
    \item Múltiplas periodicidades~\cite{athana}
    \item Períodos múltiplos fracionários do tempo de amostragem~\cite{hyndman_weekly}.
    \item Curto tempo de amostragem (de um dia ou inferior)~\cite{athana}
    \item Períodos maiores que algumas centenas do tempo de
    amostragem~\cite{hyndman_long_season}.
\end{itemize}

Sinais elétricos quase sempre possuem as duas últimas propriedades, tornando a
exploração de métodos alternativos indispensável para este trabalho.

A inclusão de variáveis de Fourier como regressores é capaz de modelar
padrões sazonais como esses de forma mais flexível. Essas variáveis exógenas
aos ao modelo são somas de senos e cossenos, ambos reais, que oscilam em
múltiplos de uma frequência fundamental definida como a frequência do padrão
sazonal que se deseja modelar.

Para modelar um padrão de período $m$ tomamos as variáveis dadas pela série
$F_t$

$$ F_t = \sum^K_{k=1} \left(\alpha_k \frac{sen(2\pi k t)}{m} + \beta_k\frac{cos(2\pi k t)}{m}\right) $$

onde harmônicos do período sazonal $m$ são incluídos pelo incremento de $k$.
Para incluir múltiplos períodos sazonais as variáveis são generalizadas para
diferentes valores de $m$ como a seguir:

$$ F_t = \sum^M_{i=1}\sum^K_{k=1} \left(\alpha_k \frac{sen(2\pi k t)}{m_i} + \beta_k\frac{cos(2\pi k t)}{m_i}\right) $$

Temos alguns parâmetros não regressores: $K$, a ordem dos harmônicos, $m_i$, os
períodos dos padrões que se deseja modelar. $M$ é simplesmente o número de
padrões. A determinação de $m_i$ pode ser realizada por conhecimento a priori
do fenômeno em questão ou métodos quantitativos como análise espectral dos
dados amostrais. Já $K$ é melhor determinado por métodos de seleção de modelo
como AIC e BIC, discutidos na seção~\ref{sec:validacao}.

A série exógena pode então ser incluída como informação auxiliar em um modelo
ARIMA resultando em um modelo sazonal $y_t$ em função de $(p, d, q, m_i, k)$:

$$ y_t = \varepsilon_t \frac{\theta(L)}{\phi(L)} \frac{1}{(1-L)^d} + F_t $$

$$ y_t = \varepsilon_t \frac{\theta(L)}{\phi(L)} \frac{1}{(1-L)^d} + \sum^M_{i=1}\sum^K_{k=1} \left(\alpha_k \frac{sen(2\pi k t)}{m_i} + \beta_k\frac{cos(2\pi k t)}{m_i}\right) $$
