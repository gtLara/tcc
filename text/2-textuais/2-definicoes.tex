\chapter*{Introdução}
\label{sec:teorica_classica_series_temporais_introducao}

\chapter{Definições e Propriedades}
\label{chap:teorica_classica_series_temporais_definicoes}

\section{Processo Estocástico}\label{sec:process}

Dado um conjunto arbitrário $\mathcal{T}$ um processo estocástico é uma família
${X(t,\omega)}$, $ t\in \mathcal{T}$ e $\omega \in \Omega$ de forma que para
cada $t \in T$, $\omega \in \Omega$ $X(t, \Omega)$ é uma variável aleatória. As
variáveis aleatórias podem ser reais ou complexas. Esse trabalho aborda apenas
processos estocásticos reais exceto quando explicitamente mencionado.

Supõe-se que a família de variáveis aleatórias seja definida em um mesmo
espaço de probabilidades $(\Omega, \mathcal{A}, P)$ com $\Omega$ representando
um espaço amostral, $\mathcal{A}$ uma $\sigma$-álgebra e $P$ uma medida de
probabilidade. Para propósitos desse trabalho podemos tomar o conjunto
$\mathcal{T}$ como $\mathbb{R}$, resultando em processos de tempo contínuo, e
$\mathbb{Z}$, resultando em processos de tempo discreto.

Para cada $t \in \mathcal{T}$ temos uma função de densidade de probabilidade
associada à variável aleatória $X(t_k, \omega)$ (assumindo que essa função exista).
Na prática um processo aleatório no mundo real é observado ao longo de $t$, tal que
$\omega$ seja fixado ao universo em que a observação ocorre. Sob essa condição
$X(t, \omega_k)$ é considerada uma realização do processo estocástico. Realizações também
são chamadas de \emph{sample record} em alguns livros de engenharia
e séries temporais na literatura estatística.

Para ilustrar os conceitos acima podemos pensar em um exemplo proposto por
~\cite{random_data} em que um gerador de ruído térmico é construído e sua tensão
ao longo de um intervalo do tempo é medida. Se um outro gerador fosse
construído sob condições e com propriedades idênticas sua tensão medida no
mesmo intervalo de tempo não seria idêntica, assim como a tensão medida sob
qualquer outro gerador idêntico. De fato cada registro de tensão é um exemplo
de infinitos registros que poderiam ter ocorrido. Nessa situação os registros
ou séries temporais de tensão são as realizações de um processo estocástico
representativo de todas as possíveis realizações.

O adequado estudo de séries temporais é consequência de um primeiro adequado
estudo sobre processos estocásticos, geradores dessas séries temporais. Essa
não é a intenção desse trabalho. Como em grande parte da teoria de séries
temporais estamos preocupados com o que podemos compreender ou inferir sobre
o processo estocástico gerador de uma realização a partir apenas de seu único
registro. Essa abordagem é essencial e de fato mais aplicável que um estudo
que se preocupa excessivamente com os processos geradores devido ao fato de dados
do mundo real frequentemente representarem realizações únicas. Não é possível
realizar novamente o índice Ibovespa entre 1970 e 2020 e muito menos ter acesso
à realização desses índices em universos paralelos.

Ao longo desse trabalho a distinção e referência ao processo gerador de uma série
temporal será feita quando necessário.


\section{Série Temporal}\label{ssec:definition}

Uma série temporal é um conjunto de observações realizadas sequencialmente no
tempo, indexadas de acordo com o momento em que foram observadas. As
observações representam a realização de um processo estocástico. Em alguns
contextos, como análise de processos industriai, a natureza do processo
subjacente é relevante para análise e modelagem de qualquer série temporal. Em
outros, como análise de séries financeiras, o sistema gerador das séries é tão
complexo que dificilmente conhecimento sobre sua dinâmica seja útil.

Assume-me, na linguagem de~~\cite{hamilton}, que um conjunto de amostras
$\mathbf{y}_t = (y_1, y_2, y_3 \dots y_T)$ pode ser interpretado como um
segmento finito de uma sequência duplamente infinita:

$${\mathbf{y}}_{t=-\infty}^{\infty} = ({\dots, y_{-1},y_0, \overbrace{y_1, y_2, y_3, \dots, y_T}^{\text{Série Observada}}, y_{T+1}, y_{T+2}}\dots)$$

\vspace{1cm}

Apesar de parecer pouco tangível, de fato qualquer série observada é
satisfatoriamente descrita dessa forma. Em um contexto industrial, por exemplo,
o valor de uma variável de processo é zero até a planta ser construída e entrar
em operação, assume valores representativos ou não da dinâmica de interesse
(assumindo valores irrelevantes quando a planta não está em operação plena), e
tendendo ao infinito retorna a zero quando a planta for desativada.

Essa interpretação de uma série temporal é importante ao implicitamente
insinuar que o processo existe em um intervalo temporal mais abrangente do que
o observado. É portanto necessário se questionar sobre quanto os
dados representam o processo analisado e em quais intervalos de tempo.

Séries temporais são inerentemente diferentes de dados tabulares por
representarem amostras de um mesmo processo estocástico ao invés de amostras
aleatórias de uma população. Não faz sequer sentido discutir uma população no
contexto de séries temporais uma vez que estamos restritos à realizações
observadas de um processo estocástico. Essa relevante diferença impede
propriedades estatísticas agradáveis consequentes da independência amostral
associada à dados tabulares apropriadamente amostrados como a lei do grandes
números e o teorema do limite central.

A forma mais natural de analisar uma série temporal é visualizar seus valores
no tempo, como ilustra a imagem~\ref{fig:example}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/white_noise.png}
    \caption{Visualização no tempo de ruído branco}
    \label{fig:example}
\end{figure}


\section{Operador de Atraso(\emph{Lag})}

É importante introduzir o operador de atraso ou \emph{lag}.

Dadas as séries $\mathbf{y}_t = (y_1, y_2, y_3 \dots y_T)$ e
$\mathbf{x}_t = (x_0, x_1, x_2 \dots x_{T-1})$ tal que

$$ \mathbf{y}_t = \mathbf{x}_{t-1}$$

isso é,

$$ y_1 = x_0 $$
$$ y_2 = x_1 $$
$$ \vdots $$
$$ y_T = x_{T-1} $$

Podemos definir $\mathbf{x}_t$ em função de $\mathbf{y}_t$ como:

$$ \mathbf{x}_t = L\mathbf{y}_t $$

tal que

$$\mathbf{y}_{t-1} = L\mathbf{y}_t$$

Observamos que o operador de atraso atrasa uma série temporal em uma unidade de
tempo. Uma breve divagação matemática~~\cite{hamilton} permite definir o
operador com propriedades muito semelhantes às de multiplicação dos números
reais, como associatividade, comutatividade e distribuição. Para atrasar
múltiplas unidades de tempo temos que:

\vspace{1cm}

$$L(L(\mathbf{y_t})) = L(\mathbf{y}_{t-1}) = \mathbf{y}_{t-2} = L^2 \mathbf{y}_{t}$$

\vspace{1cm}

de forma que

$$ L^n {\mathbf{y}} =  \mathbf{y}_{t-n}$$

\vspace{1cm}

Um uso importante do operador, decorrente de suas propriedades algébricas, é
exemplificado na seguinte expansão

$$ (aL^2 + bL^3) \mathbf{y}_{t} =  a\mathbf{y}_{t-2} + b\mathbf{y}_{t-3}$$

Conhecimento do operador de atraso é importante para compreender a literatura
de séries temporais e facilita comunicação objetiva de análises cotidianas.
O presente trabalho usa do operador para descrever uma série de modelos.

Como nota final é importante mencionar que alguns livros~~\cite{chatfield}
~~\cite{stoffer} usam a letra $B$ para denotar o operador de atraso e que na
maior parte dos recursos \emph{online} o operador é referido por seu nome em
inglês, \emph{lag}.

A analogia entre o operador de atraso e a variável complexa $e^{-j\omega} = z^{-1}$
é clara, com a relevante diferença que $z^{-1}$ atrasa um sinal em uma unidade
de tempo se a operação for realizada no domínio $z$ enquanto o operador de
atraso atua diretamente no domínio do tempo. Essa característica permite que
filtros com equações de recorrência complexas sejam representados de forma
compacta no domínio do tempo por meio de polinômios de atraso.

Uma propriedade interessante decorrente da equivalência entre $z^{-1}$ e $L$ é
que pode se pensar em um plano $L$ cuja análise é igualmente informativa à do
plano z, notando que o espaço é de certa forma invertido. Uma análise da
posição dos polos de um sistema representado por meio de um polinômio em $L$
conclui que o sistema é instável se tais polos estiverem fora do círculo unitário,
contrário do que conhecemos do plano z.

Por fim vale mencionar que alguns autores como~\cite{aguirre} usam a notação $q^{-1}$
para esse operador.

\section{Operador de Diferença}\label{sec:diff}

O operador de diferenças $\nabla$ ou  $\Delta$ é o equivalente discreto da
operação contínua de derivação e opera sob uma série temporal
$\mathbf{y}_t$ da seguinte forma:

$$ \nabla \mathbf{y}_t = (1 - L)\mathbf{y}_t = \mathbf{y}_t - L\mathbf{y}_t = \mathbf{y}_t - \mathbf{y}_{t-1} $$

O operador possui propriedades de associatividade e distribuição, tal que

$$ \nabla^2 \mathbf{y}_t = \nabla(\nabla(\mathbf{y}_t)) = \nabla(\mathbf{y}_t - \mathbf{y}_{t-1}) = \nabla \mathbf{y}_t - \nabla \mathbf{y}_{t-1} = \mathbf{y}_t - 2 \mathbf{y}_{t-1} + \mathbf{y}_{t-2} $$

\section{Tendência(\emph{Trend})}

A variação do valor esperado de um processo estocástico é denominado tendência.
A partir de uma série temporal definimos tendência como a variação de sua média
amostral. A imagem ~\ref{fig:trend} ilustra uma série com tendência linear.
Observa-se que ao longo do tempo a média das observações cresce linearmente.
Tendências de séries reais frequentemente seguem um perfil
logarítmico~~\cite{chatfield}, como na figura~\ref{fig:log_trend}. Nesse caso
uma transformação exponencial da série, isso é, a aplicação de uma função
exponencial a cada observação, tornaria a tendência linear.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/trend.png}
    \caption{Série temporal com tendência linear}
    \label{fig:trend}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/log_trend.png}
    \caption{Série temporal com tendência logarítmica}
    \label{fig:log_trend}
\end{figure}

\section{Sazonalidade}\label{sec:seasonality}

A variação periódica de média móvel das observações de uma série temporal é
denominada sazonalidade. Em séries no contexto de finanças sazonalidade
frequentemente segue ciclos de calendário como anual, mensal, semestral, etc.
No contexto mais amplo de séries temporais sazonalidade apresenta período
arbitrário, apesar da linguagem em torno dessa propriedade estar muito
associada aos períodos anteriormente mencionados.

Um exemplo de série temporal com sazonalidade é ilustrado na
figura~\ref{fig:seasonality}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/seasonality.png}
    \caption{Série temporal com sazonalidade senoidal de período arbitrário}
    \label{fig:seasonality}
\end{figure}

Na presença de tendência sazonalidade pode ser considerada aditiva, se sua
variação for constante em torno da tendência, e multiplicativa, se sua variação
depender o valor da tendência. Exemplos de sazonalidade aditiva e
multiplicativa são dados pelas figuras~\ref{fig:add_seasonality} e
~\ref{fig:mult_seasonality}, respectivamente. Discernir entre os dois tipos de
sazonalidade é importante para modelagem.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/add_seasonality.png}
    \caption{Série temporal com sazonalidade aditiva}
    \label{fig:add_seasonality}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/mult_seasonality.png}
    \caption{Série temporal com sazonalidade multiplicativa}
    \label{fig:mult_seasonality}
\end{figure}


\section{Autocorrelação}

TODO: later analogy with convolution would be cool
TODO: list properties of autocorrelation function !

A função de autocorrelação é definida para processos estocásticos como a
correlação de Pearson entre valores do processo em instantes de tempo
diferentes. A função de autocovariância entre os instantes de tempo $t_1$ e
$t_2$ é dada pela seguinte equação

\begin{equation}\label{eq:raw_autocorr}
    gamma_{xx}(t_1, t_2) = E[(X_{t_1} - \mu_{t_1})(X_{t_2} -\mu_{t_2})]
\end{equation}

Normalizando a autocovariância obtemos a autocorrelação

$$\rho_{xx}(t_1, t_2) =\frac{K_{xx}(t_1, t_2)}{\sigma_{t_1}\sigma_{t_2}}$$

Para processos estacionários (seção~\ref{sec:stationarity}) a autocovariância,
e consequentemente a autocorrelação, é função apenas do atraso $\tau = |t_1 -
t_2|$. Temos então que

$$\rho_{xx}(\tau) =\frac{K_{xx}(\tau)}{\sigma_{t_1}\sigma_{t_2}}$$

Para uma série temporal, isso é, uma única  realização de um processo
estocástico, a função de autocorrelação estacionária (tipicamente chamada
apenas de função de autocorrelação) pode ser definida diretamente a partir da
definição de correlação amostral sob as seguintes premissas

\begin{enumerate}
    \item O processo estocástico gerador da série temporal é estacionário
    \item O número de observações  $N$ de $\mathbf{y}_t$ é suficientemente
    grande ($N \approx 100$)
\end{enumerate}

resultando na equação ~\ref{eq:autocorr}, onde o subscrito duplo é omitido.
Note que o atraso é discreto, indicado por $k$.

\vspace{1cm}

\begin{equation}\label{eq:autocorr}
    \rho_y(k) = \frac{\sum_{t=1}^{N-k}(y_t - \bar{y})(y_{t+k}-\bar{y})}{\sum_{t=1}^{N}(y_t - \bar{y})^2}  , \hspace{1cm} k = 0, 1, 2, \dots
\end{equation}

Alguns comentários sobre a nomenclatura da função são apropriados. A literatura
de engenharia tende a usar os termos autocovariância e autocorrelação de forma
intercambiável para designar a definição de autocovariância apresentada. A
literatura estatística assume as definições abordadas acima, que serão usada no
restante desse trabalho.

Além disso, a função de autocorrelação como apresentada pela
equação~\ref{eq:raw_autocorr} é definida para um processo estocástico não
necessariamente estacionário, apesar do termo ser usado para descrever a
equação~\ref{eq:autocorr}. A aplicação da equação que assume estacionariedade
em uma série não estacionária resulta em correlações informativas praticamente
apenas disso. Uma modificação estratégica da função dada
por~\ref{eq:raw_autocorr} resulta na chamada função de autocorrelação
instantânea (seção~\ref{ssec:inst_autocorr}), que é usada para representar
séries temporais não estacionárias.

\vspace{1cm}

\subsection{Autocorrelação Parcial}\label{ssec:partial_acorr}

É interessante mencionar a existência de autocorrelação parcial nessa seção
juntamente de uma descrição em alto nível do que esse valor representa. Sua
definição formal será apresentada na seção~\ref{ssec:AR(p)}.

Autocorrelação parcial foi introduzida por Box e Jenkins em~\cite{box} como uma
ferramenta auxiliar na identificação de modelos. O valor $\phi_{kk}$ é
definido como a correlação parcial entre $\mathbf{y}_t$ e $\mathbf{y}_{t - k}$,
isso é, a correlação restante entre $\mathbf{y}_t$ e $\mathbf{y}_{t - k}$ após
levar em consideração a contribuição de $\mathbf{y}_t$, $\mathbf{y}_{t - 1}$
$...$ $\mathbf{y}_{t - k + 1}$.

\subsection{Correlalograma}\label{ssec:correlalogram}

Um correlalograma é um gráfico de barras representativo da autocorrelação ou
autocovariância em $k$ amostras de uma série temporal ($k=0, 1, 2, \dots$), de
forma que a primeira barra represente a autocorrelação entre $y_t$ e si mesmo
(sempre igual à 1), a segunda entre $y_t$ e $y_{t-1}$, a terceira entre $y_t$ e
$y_{t-2}$, e assim por diante. O correlalograma da série visualizada pela
figura~\ref{fig:trend} é ilustrado na figura~\ref{fig:correlalogram}.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/corr_trend.png}
    \caption{Visualização de correlalograma de série com tendência linear.
    Observe que as autocorrelações decaem lentamente ao decorrer dos atrasos,
    comportamento típico de tendências determinísticas.}
    \label{fig:correlalogram}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/corr_seasonality.png}
    \caption{Visualização de correlalograma de série com sazonalidade.
    Observe que o padrão senoidal da série é reproduzido nas autocorrelações.}
    \label{fig:corr_season}
\end{figure}

A figura~\ref{fig:correlalogram} informa um intervalo de relevância dado por um
sombreamento vermelho. Qualquer valor de autocorrelação dentro desse intervalo
é estatisticamente insignificante e pode ser considerado igual a zero.

O correlalograma é uma ferramenta indispensável em análise de séries temporais
para tarefas como detecção de estacionariedade, identificação de sazonalidade,
análise de resíduo, engenharia de características, escolha de modelo e
identificação de ruído branco(seção~\ref{sec:white_noise}), entre outros. No
contexto de identificação de sistemas autocovariância e autocorrelação e
portanto o correlalograma desempenham um importante papel na identificação de
propriedades de sinais e sistemas imersos em ruído devido à robustez ao ruído
da operação de correlação cruzada~\cite{aguirre}. A figura~\ref{fig:noisy_sine}
apresenta um sinal imerso em ruído cuja natureza periódica subjacente se torna
mais visível por meio de seu correlalograma.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/noisy_seasonal_signal.png}
    \caption{Sinal periódico imerso em ruído e sua correspondente autocorrelação.}
    \label{fig:noisy_sine}
\end{figure}

É importante mencionar que o correlalograma de uma série com tendência
determinística, como a da figura~\ref{fig:correlalogram}, apresenta o
comportamento observado de autocorrelações altas com pouca atenuação ao longo
dos atrasos. De forma análoga o correlalograma de uma série com sazonalidade
apresenta periodicidade que reproduz seu padrão temporal, como ilustra a
figura~\ref{fig:corr_season}, correlalograma da série da
figura~\ref{fig:seasonality}. O primeiro correlalograma é informativo até
certo ponto: informa simplesmente que a série apresenta tendência. Para
analisar tais séries de forma mais produtiva, a fim de elaborar um possível
modelo, por exemplo, é importante que a série seja estacionária
(seção~\ref{sec:stationarity}). É inclusive afirmado em alguns textos da
literatura estatística~\cite{chatfield}, que um correlalograma só faz que
sentido se a série associada for estacionária, observação mais geral e rigorosa
das restrições de uso da equação~\ref{eq:autocorr}.

Na engenharia a análise do correlalograma de sinais não estacionários é
utilizada para investigação da adequação de tempo de amostragem, onde uma
autocovariância com valores lentamente decrescentes e um mínimo local indica que
o sinal pode estar superamostrado, propriedade indesejável que pode resultar em
problemas computacionais além de desperdício de memória.

TODO: add sampling analysis of silica series? later maybe

\section{Estacionariedade}\label{sec:stationarity}

Um processo estocástico $\mathbf{X}(t)$ é considerado estacionário no
sentido amplo se atender às seguintes três condições:

\begin{enumerate}
    \item $E(\mathbf{X}(t)) = \mu$
    \item $Var(\mathbf{X}(t)) = \sigma^2$
    \item $Cov[\mathbf{X}(t), \mathbf{X}(t+\tau)] = \gamma(\tau)$
\end{enumerate}\vspace{.5cm}

e estacionário no sentido restrito se sua distribuição de probabilidade $P(x)$
for idêntica para todos os instantes de tempo, isso é, $P(x)_{t_{i}} = P(x)_{t_{j}}$
$\forall i, j \in \mathcal{T}$. Essa restrição é frequentemente comunicada como
a necessidade de todos os momentos da distribuição $P(x)$ serem idênticos e
invariantes ao tempo. Nesse trabalho, assim como em grande parte da literatura
estatística e de engenharia, o termo estacionário se refere a estacionariedade
no sentido amplo. Isso é parcialmente justificado pelo fato de verificação
de estacionariedade no sentido amplo tipicamente ser condição suficiente para
assumir estacionariedade no sentido restrito, segundo~\cite{random_data}.

A definição de estacionariedade apresentada descreve a família de sinais
representada por um processo estocástico. Uma definição de estacionariedade
para realizações únicas de um processo estocástico, isso é, séries temporais,
demanda o estabelecimento de resquisitos amostrais. Podemos dizer que uma
série temporal é estacionária se suas propriedades amostrais de esperança,
variância e covariância sejam apropriadamente invariantes de forma que
flutuações em seu valor sob diferentes janelas de tempo sejam explicáveis por
variações resultantes de amostragem. Sob essa perspectiva podemos traduzir
os requisitos de estacionariedade para uma série $\mathbf{x}_t$ como a seguir

\begin{enumerate}
    \item A média da série $\mathbf{x}_t$ é constante ao longo do tempo
    \item A variância da série $\mathbf{x}_t$ é constante ao longo do tempo
    \item A autocorrelação de $\mathbf{x}_t$ depende apenas do atraso
\end{enumerate}\vspace{.5cm}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{figures/stationarity_examples.png}
    \caption{Conjunto de séries demonstrando diferentes níveis de
    estacionariedade.}
    \label{fig:stationarity}
\end{figure}

Um bom exemplo de graus de estacionariedade em séries temporais dado por
Athanasopoulos e Hyndman~~\cite{athana} é ilustrado pela
figura~\ref{fig:stationarity}. As séries das figures~\ref{fig:stationarity}.a,
~\ref{fig:stationarity}.e e~\ref{fig:stationarity}.i demonstram clara
tendência, sendo portanto não estacionárias. As séries das figures
~\ref{fig:stationarity}.d, ~\ref{fig:stationarity}.h, ~\ref{fig:stationarity}.i
são igualmente não estacionárias por apresentarem clara sazonalidade, enquanto
a série da figura~\ref{fig:stationarity}.g aparenta ser sazonal mas apresenta
picos aperiódicos de intensidade muito distinta, sendo considerada estacionária
pelos autores. O caso da série da figura~\ref{fig:stationarity}.g é um
complicado por aparentar também violar o requisito de variância constante. Para
concluir assim como os autores que a série é estacionária é necessário mais do
que uma investigação visual; o texto que discute a série leva em conta seu
processo gerador~~\cite{athana}.

As séries das figures~\ref{fig:stationarity}.c e ~\ref{fig:stationarity}.f são
aparentemente não estacionárias no intervalo observado por demonstrarem
variação em sua média móvel mas podem ser um caso de raízes unitárias.

Por eliminação temos que apenas as séries ilustradas pelas
figures~\ref{fig:stationarity}.b e ~\ref{fig:stationarity}.g são estacionárias,
o caso de ~\ref{fig:stationarity}.b contendo uma clara anomalia.

O exemplo de análise de estacionariedade da figura~\ref{fig:stationarity}
demonstra a imprecisão da abordagem visual para essa tarefa. Torna-se
necessário o estabelecimento de procedimentos mais objetivos para detecção de
estacionariedade.

\subsection{Importância de Estacionariedade}

Estacionariedade é uma propriedade desejável de se observar em uma série
temporal para fins analíticos e de modelagem.

Há algumas formas de compreender como estacionariedade colabora para a
modelagem bem sucedida de uma série temporal.

Podemos pensar na propriedade de estacionariedade como um tipo de estrutura de
dependência. Se as amostras $X_1, X_2, \dots, X_N$ de um conjunto $\mathbf{X}$
forem independentes entre si temos formas interessantes de modelar a função
geradora de $\mathbf{X}$ como o teorema do limite central, lei dos grandes
números, etc. Há apenas uma forma de um conjunto amostral ser independente mas
muitas formas de ser dependente, tornando difícil o estabelecimento de recursos
eficientes para modelagem geral de processes dependentes. Séries temporais
sendo naturalmente observações de processos dependentes
(seção~\ref{ssec:definition}) é interessante definir estruturas de dependência
que permitam o uso de tais recursos. Estacionariedade é uma estrutura de
dependência que permite aplicar propriedades úteis de independência em séries
temporais. Abordando a mesma ideia mais intuitivamente podemos pensar no
seguinte exemplo: se um processo possui valor esperado, variância constante e
autocorrelação invariante ao tempo podemos por meio da lei dos grandes números
estimar seu valor esperado e variância com cada vez mais confiança a partir da
média e variância amostral, respectivamente. O mesmo argumento intuitivo se
estende
analogamente para o caso de aprendizado de máquina, no qual por meio de teoria
de aprendizado estatístico é possível argumentar que uma série estacionária é
``mais fácil'' de aprender.

De forma mais quantitativa o teorema de decomposição de Wold~~\cite{chatfield}
conclui que qualquer série temporal estacionária pode ser representada pela
seguinte combinação linear

$$\mathbf{y}_t = \sum_{j=0}^\infty b_j Z_{t-j} + \eta_t$$

No qual $\eta$ representa uma série determinística e $Z_t$ um processo
puramente aleatório (seção~\ref{sec:white_noise}). O leitor reconhecerá parte
da expressão acima como um processo $MA(\infty)$ (seção~\ref{sssec:MA(p)}).
Esse resultado tem como consequência a importante conclusão que qualquer série
estacionária é possivelmente aproximável por um modelo MA e portanto, via
invertibilidade, modelos AR e ARMA (seção~\ref{ssec:stability_invertibility}).

Por fim estacionariedade permite o uso de uma série de modelos que serão
discutidos na seção sobre modelos estacionários. Esses métodos são bem
compreendidos e implementados, facilitando sua interpretação, uso e
sustentação.

\subsection{Categorias Básicas de Não Estacionariedade}\label{ssec:taxonomy}

Como extensão do argumento sobre estruturas de dependência na seção anterior
podemos afirmar que, sendo estacionariedade um padrão de dependência, temos
infinitas formas de não estacionariedade, retornando ao caso de dependência
generalizada. É interessante identificar nesse universo de dependência padrões
de séries não estacionárias que são facilmente transformadas em séries
estacionárias.

Uma série temporal com presença de tendência determinística, como ilustrada na
figura~\ref{fig:trend}, pode ser representada pela seguinte expressão:

$$  y_t = e_t + f(t) + \varepsilon_t  \hspace{1cm}\text{onde} \hspace{.4cm}\varepsilon_t \sim \hspace{.2cm}\text{i.i.d.} \hspace{.2cm}\mathcal{N}(0, \sigma^2)$$

Na qual $e_t$ representa uma série estacionária, $f(t)$ uma função
determinística do tempo e $\varepsilon_t$ ruído
branco(seção~\ref{sec:white_noise}). Nota-se que $f(t)$ é uma função
monotônica arbitrária tal que $y_t$ seja uma série não estacionária. No caso da
figura~\ref{fig:trend} temos $f(t)$ linear e na figura ~\ref{fig:log_trend}
logarítmica. Uma série temporal demonstrando esse tipo de não estacionariedade
é considerada \textbf{tendência-estacionária}, uma vez que simplesmente
removendo a tendência $f(t)$ temos estacionariedade. Isso pode ser feito de
várias formas, talvez com maior simplicidade diferenciando a série. Métodos
mais sofisticados incluem decomposição ETS (seção~\ref{sec:decomposition}) e
regressão com finalidade de modelar $f(t)$ de forma que o resíduo represente
uma a série estacionária $e_t + \varepsilon_t$.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{figures/random_walk.png}
    \caption{Visualização de caminhada aleatória com $y_0=5$}
    \label{fig:random_walk}
\end{figure}

Uma série com presença de tendência estocástica pode ser classificada de
maneira semelhante. O exemplo mais simples de tal série é gerada por um
passeio aleatório, definido pelo seguinte processo, visualizado pela figura
~\ref{fig:random_walk}:

$$  y_t = y_{t-1} + \varepsilon_t  \hspace{1cm}\text{onde} \hspace{.4cm}\varepsilon_t \sim \hspace{.2cm}\text{i.i.d.} \hspace{.2cm}\mathcal{N}(0, \sigma^2)$$

Por meio de um desenvolvimento recursivo do processo podemos escrever:

$$ y_t = (y_{t-2} + \varepsilon_{t-1}) + \varepsilon_{t} $$
$$ y_t = ((y_{t-3} + \varepsilon_{t-2}) + \varepsilon_{t-1}) + \varepsilon_{t} $$
$$ \vdots $$
$$ y_t = \sum_{j=0}^{N-1} \varepsilon_{t-j} + y_0$$
\vspace{1cm}

Resultado a partir do qual a não estacionariedade de $y_t$ se torna evidente,
uma vez que

$$ var(y_t) = \sigma^2 t $$

Além da covariância ser dependente do tempo.

Uma forma simples de tornar a série estacionária é diferenciá-la em primeira
ordem, isso é, aplicar o operador de diferença primeira:

$$ \nabla y_t = y_t - y_{t-1} $$
$$ y_t - y_{t-1} = \varepsilon_t$$
$$ \nabla y_t =  \varepsilon_t$$

Sabemos pela seção~\ref{sec:white_noise} que ruído branco é um processo
estacionário.

A caminhada aleatória é denominada uma série \textbf{diferença-estacionária}
pelo fato da operação de diferença introduzir estacionariedade. Essa é uma
forma tão comum de não estacionariedade que a ideia de ``diferenciar uma
série antes de fazer qualquer coisa'' é proeminente entre profissionais de
dados, apesar de que geralmente necessita-se apenas de estacionariedade
Essa prática é parcialmente justificada considerando que a maior
parte das séries temporais ``reais'' são não estacionárias e frequentemente
diferencialmente estacionárias.

É igualmente possível que uma série diferença-estacionária seja estacionária em
sua $n$-ésima diferença, tal que estacionariedade seja observada por uma
operação de diferenças de ordem $n$. A ideia de tirar sucessivas diferenças
até atingir estacionariedade é fundamental no método de Box-Jenkins, por
exemplo.

Séries diferença-estacionárias apresentam raízes unitárias e os dois termos são
frequentemente usados nos mesmos contextos.

Podemos resumir as definições das categorias de não estacionariedade abordadas
nessa seção assim como suas implicações como segue:

\begin{enumerate}
    \item \textbf{Estacionariedade em Tendência}: Uma série é considerada
        tendência-estacionária se apresentar uma tendência determinística. No
        caso de anomalias ou eventos de perturbação séries com esse tipo de
        tendência retornam ao valor da tendência ao longo do tempo,
        ``esquecendo'' o evento perturbador. Esse tipo de série se torna
        estacionária pela remoção da tendência determinística, processo
        realizado por meio de regressão da tendência, por diferenciação, por
        decomposição, etc.
    \item \textbf{Estacionariedade Diferenciável}: Uma série é considerada
        diferença-estacionária se apresentar uma tendência estocástica. No
        caso de anomalias ou eventos de perturbação séries com esse tipo de
        tendência são irreversivelmente afetadas,
        ``lembrando'' do evento perturbador. Esse tipo de série se torna
        estacionária por diferenciação em ordem $n$. Possui raízes unitárias
        e é frequentemente discutida nessa linguagem.

\end{enumerate}

\section{Ruído Branco}\label{sec:white_noise}

TODO: inserir propriedades de ruído branco

Uma série temporal $\mathbf{x}_t$ gerada por um processo $\mathbf{X}(t)$ é
considerada ruído branco ou um processo puramente aleatório se atender às
seguintes três condições:

\begin{enumerate}
    \item $E(\mathbf{X}(t)) = 0$
    \item $Var(\mathbf{X}(t)) = \sigma^2$
    \item $Cov[\mathbf{X}(t), \mathbf{X}(t+\tau)] = 0$
\end{enumerate}\vspace{.5cm}

Que podem ser interpretadas da seguinte forma

\begin{enumerate}
    \item A média da série $\mathbf{x}_t$ é nula ao longo do tempo
    \item A variância da série $\mathbf{x}_t$ é constante ao longo do tempo
    \item Não há correlação entre as amostras de $\mathbf{x}_t$
\end{enumerate}\vspace{.5cm}

Observa-se que ruído branco é um caso específico de estacionariedade, se
diferenciando pela especificação do valor esperado e autocorrelação entre
quaisquer amostras em zero. A compreensão da definição e capacidade de
identificação de ruído branco é importante para análise de resíduos.
